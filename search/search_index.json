{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"General","text":"<p>This is the documentation site of the eis_toolkit python package. Here you can find documentation for each module. The documentation is automatically generated from docstrings.</p> <p>Development of eis_toolkit is related to EIS Horizon EU project.</p>"},{"location":"dependency_licenses/","title":"Dependency licenses","text":"Name Version License protobuf 3.19.4 3-Clause BSD License tensorboard-plugin-wit 1.8.1 Apache 2.0 absl-py 1.2.0 Apache Software License flatbuffers 1.12 Apache Software License ghp-import 2.1.0 Apache Software License google-auth 2.11.0 Apache Software License google-auth-oauthlib 0.4.6 Apache Software License google-pasta 0.2.0 Apache Software License grpcio 1.48.1 Apache Software License importlib-metadata 4.12.0 Apache Software License keras 2.9.0 Apache Software License libclang 14.0.6 Apache Software License requests 2.28.1 Apache Software License rsa 4.9 Apache Software License tenacity 8.2.2 Apache Software License tensorboard 2.9.1 Apache Software License tensorboard-data-server 0.6.1 Apache Software License tensorflow 2.9.2 Apache Software License tensorflow-estimator 2.9.0 Apache Software License tensorflow-io-gcs-filesystem 0.26.0 Apache Software License watchdog 2.1.9 Apache Software License packaging 21.3 Apache Software License; BSD License python-dateutil 2.8.2 Apache Software License; BSD License affine 2.3.1 BSD cligj 0.7.2 BSD geopandas 0.11.1 BSD Fiona 1.8.21 BSD License Jinja2 3.1.2 BSD License Markdown 3.3.7 BSD License MarkupSafe 2.1.1 BSD License Pygments 2.13.0 BSD License Shapely 1.8.4 BSD License Werkzeug 2.2.2 BSD License astunparse 1.6.3 BSD License click 8.1.3 BSD License click-plugins 1.1.1 BSD License cycler 0.11.0 BSD License gast 0.4.0 BSD License h5py 3.7.0 BSD License idna 3.3 BSD License joblib 1.1.0 BSD License kiwisolver 1.4.4 BSD License mkdocs 1.3.1 BSD License numpy 1.23.2 BSD License oauthlib 3.2.0 BSD License pandas 1.4.4 BSD License patsy 0.5.2 BSD License pyasn1 0.4.8 BSD License pyasn1-modules 0.2.8 BSD License rasterio 1.3.2 BSD License requests-oauthlib 1.3.1 BSD License scikit-learn 1.1.2 BSD License scipy 1.9.1 BSD License statsmodels 0.13.2 BSD License threadpoolctl 3.1.0 BSD License wrapt 1.14.1 BSD License eis-toolkit 0.1.0 European Union Public Licence 1.2 (EUPL 1.2) Pillow 9.2.0 Historical Permission Notice and Disclaimer (HPND) opt-einsum 3.3.0 MIT snuggs 1.4.7 MIT GDAL 3.4.3 MIT License Keras-Preprocessing 1.1.2 MIT License PyYAML 6.0 MIT License attrs 22.1.0 MIT License cachetools 5.2.0 MIT License charset-normalizer 2.1.1 MIT License fonttools 4.37.1 MIT License mergedeep 1.3.4 MIT License mkdocs-material 8.4.2 MIT License mkdocs-material-extensions 1.0.3 MIT License munch 2.5.0 MIT License plotly 5.14.0 MIT License pymdown-extensions 9.5 MIT License pyparsing 3.0.9 MIT License pyproj 3.3.1 MIT License pytz 2022.2.1 MIT License pyyaml_env_tag 0.1 MIT License setuptools-scm 6.4.2 MIT License six 1.16.0 MIT License termcolor 1.1.0 MIT License tomli 2.0.1 MIT License urllib3 1.26.12 MIT License zipp 3.8.1 MIT License certifi 2022.6.15 Mozilla Public License 2.0 (MPL 2.0) matplotlib 3.5.3 Python Software Foundation License typing_extensions 4.3.0 Python Software Foundation License"},{"location":"conversions/csv_to_geodataframe/","title":"Convert csv to geodataframe","text":""},{"location":"conversions/csv_to_geodataframe/#eis_toolkit.conversions.csv_to_geodataframe.csv_to_geodataframe","title":"<code>csv_to_geodataframe(csv, indexes, target_crs)</code>","text":"<p>Read CSV file to a GeoDataFrame.</p> <p>Usage of single index expects valid WKT geometry. Usage of two indexes expects POINT feature(s) X-coordinate as the first index and Y-coordinate as the second index.</p> <p>Parameters:</p> Name Type Description Default <code>csv</code> <code>Path</code> <p>Path to the .csv file to be read.</p> required <code>indexes</code> <code>Sequence[int]</code> <p>Index(es) of the geometry column(s).</p> required <code>target_crs</code> <code>int</code> <p>Target CRS as an EPSG code.</p> required <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>CSV file read to a GeoDataFrame.</p> Source code in <code>eis_toolkit/conversions/csv_to_geodataframe.py</code> <pre><code>@beartype\ndef csv_to_geodataframe(\n    csv: Path,\n    indexes: Sequence[int],\n    target_crs: int,\n) -&gt; geopandas.GeoDataFrame:\n    \"\"\"\n    Read CSV file to a GeoDataFrame.\n\n    Usage of single index expects valid WKT geometry.\n    Usage of two indexes expects POINT feature(s) X-coordinate as the first index and Y-coordinate as the second index.\n\n    Args:\n        csv: Path to the .csv file to be read.\n        indexes: Index(es) of the geometry column(s).\n        target_crs: Target CRS as an EPSG code.\n\n    Returns:\n        CSV file read to a GeoDataFrame.\n    \"\"\"\n\n    data_frame = _csv_to_geodataframe(\n        csv=csv,\n        indexes=indexes,\n        target_crs=target_crs,\n    )\n    return data_frame\n</code></pre>"},{"location":"conversions/raster_to_dataframe/","title":"Convert raster to dataframe","text":""},{"location":"conversions/raster_to_dataframe/#eis_toolkit.conversions.raster_to_dataframe.raster_to_dataframe","title":"<code>raster_to_dataframe(raster, bands=None, add_coordinates=False)</code>","text":"<p>Convert raster to Pandas DataFrame.</p> <p>If bands are not given, all bands are used for conversion. Selected bands are named based on their index e.g., band_1, band_2,...,band_n. If wanted, image coordinates (row, col) for each pixel can be written to dataframe by setting add_coordinates to True.</p> <p>Parameters:</p> Name Type Description Default <code>raster</code> <code>DatasetReader</code> <p>Raster to be converted.</p> required <code>bands</code> <code>Optional[Sequence[int]]</code> <p>Selected bands from multiband raster. Indexing begins from one. Defaults to None.</p> <code>None</code> <code>add_coordinates</code> <code>bool</code> <p>Determines if pixel coordinates are written into dataframe. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Raster converted to a DataFrame.</p> Source code in <code>eis_toolkit/conversions/raster_to_dataframe.py</code> <pre><code>@beartype\ndef raster_to_dataframe(\n    raster: rasterio.io.DatasetReader,\n    bands: Optional[Sequence[int]] = None,\n    add_coordinates: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"Convert raster to Pandas DataFrame.\n\n    If bands are not given, all bands are used for conversion. Selected bands are named based on their index e.g.,\n    band_1, band_2,...,band_n. If wanted, image coordinates (row, col) for each pixel can be written to\n    dataframe by setting add_coordinates to True.\n\n    Args:\n        raster: Raster to be converted.\n        bands: Selected bands from multiband raster. Indexing begins from one. Defaults to None.\n        add_coordinates: Determines if pixel coordinates are written into dataframe. Defaults to False.\n\n    Returns:\n        Raster converted to a DataFrame.\n    \"\"\"\n\n    data_frame = _raster_to_dataframe(\n        raster=raster,\n        bands=bands,\n        add_coordinates=add_coordinates,\n    )\n    return data_frame\n</code></pre>"},{"location":"data_tools/class_balancing/","title":"Class balancing","text":""},{"location":"data_tools/class_balancing/#eis_toolkit.data_tools.class_balancing.balance_SMOTETomek","title":"<code>balance_SMOTETomek(X, y, sampling_strategy='auto', random_state=None)</code>","text":"<p>Balances the classes of input dataset using SMOTETomek resampling method.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Union[DataFrame, ndarray]</code> <p>The feature matrix (input data as a DataFrame).</p> required <code>y</code> <code>Union[Series, ndarray]</code> <p>The target labels corresponding to the feature matrix.</p> required <code>sampling_strategy</code> <code>Union[float, str, dict]</code> <p>Parameter controlling how to perform the resampling. If float, specifies the ratio of samples in minority class to samples of majority class, if str, specifies classes to be resampled (\"minority\", \"not minority\", \"not majority\", \"all\", \"auto\"), if dict, the keys should be targeted classes and values the desired number of samples for the class. Defaults to \"auto\", which will resample all classes except the majority class.</p> <code>'auto'</code> <code>random_state</code> <code>Optional[int]</code> <p>Parameter controlling randomization of the algorithm. Can be given a seed (number). Defaults to None, which randomizes the seed.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[Union[DataFrame, ndarray], Union[Series, ndarray]]</code> <p>Resampled feature matrix and target labels.</p> <p>Raises:</p> Type Description <code>NonMatchingParameterLengthsException</code> <p>If X and y have different length.</p> Source code in <code>eis_toolkit/data_tools/class_balancing.py</code> <pre><code>@beartype\ndef balance_SMOTETomek(\n    X: Union[pd.DataFrame, np.ndarray],\n    y: Union[pd.Series, np.ndarray],\n    sampling_strategy: Union[float, str, dict] = \"auto\",\n    random_state: Optional[int] = None,\n) -&gt; tuple[Union[pd.DataFrame, np.ndarray], Union[pd.Series, np.ndarray]]:\n    \"\"\"Balances the classes of input dataset using SMOTETomek resampling method.\n\n    Args:\n        X: The feature matrix (input data as a DataFrame).\n        y: The target labels corresponding to the feature matrix.\n        sampling_strategy: Parameter controlling how to perform the resampling.\n            If float, specifies the ratio of samples in minority class to samples of majority class,\n            if str, specifies classes to be resampled (\"minority\", \"not minority\", \"not majority\", \"all\", \"auto\"),\n            if dict, the keys should be targeted classes and values the desired number of samples for the class.\n            Defaults to \"auto\", which will resample all classes except the majority class.\n        random_state: Parameter controlling randomization of the algorithm. Can be given a seed (number).\n            Defaults to None, which randomizes the seed.\n\n    Returns:\n        Resampled feature matrix and target labels.\n\n    Raises:\n        NonMatchingParameterLengthsException: If X and y have different length.\n    \"\"\"\n\n    if len(X) != len(y):\n        raise exceptions.NonMatchingParameterLengthsException(\n            \"Feature matrix X and target labels y must have the same length.\"\n        )\n\n    X_res, y_res = SMOTETomek(sampling_strategy=sampling_strategy, random_state=random_state).fit_resample(X, y)\n    return X_res, y_res\n</code></pre>"},{"location":"exploratory_analyses/dbscan/","title":"DBSCAN","text":""},{"location":"exploratory_analyses/dbscan/#eis_toolkit.exploratory_analyses.dbscan.dbscan","title":"<code>dbscan(data, max_distance=0.5, min_samples=5)</code>","text":"<p>Perform DBSCAN clustering on the input data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>GeoDataFrame</code> <p>GeoDataFrame containing the input data.</p> required <code>max_distance</code> <code>float</code> <p>The maximum distance between two samples for one to be considered as in the neighborhood of the other. Defaults to 0.5.</p> <code>0.5</code> <code>min_samples</code> <code>int</code> <p>The number of samples in a neighborhood for a point to be considered as a core point. Defaults to 5.</p> <code>5</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>GeoDataFrame containing two new columns: one with assigned cluster labels and one indicating whether a point is a core point (1) or not (0).</p> <p>Raises:</p> Type Description <code>EmptyDataFrameException</code> <p>The input GeoDataFrame is empty.</p> <code>InvalidParameterException</code> <p>The maximum distance between two samples in a neighborhood is not greater than zero or the number of samples in a neighborhood is not greater than one.</p> Source code in <code>eis_toolkit/exploratory_analyses/dbscan.py</code> <pre><code>@beartype\ndef dbscan(data: gdp.GeoDataFrame, max_distance: float = 0.5, min_samples: int = 5) -&gt; gdp.GeoDataFrame:\n    \"\"\"\n    Perform DBSCAN clustering on the input data.\n\n    Args:\n        data: GeoDataFrame containing the input data.\n        max_distance: The maximum distance between two samples for one to be considered as in the neighborhood of\n            the other. Defaults to 0.5.\n        min_samples: The number of samples in a neighborhood for a point to be considered as a core point.\n            Defaults to 5.\n\n    Returns:\n        GeoDataFrame containing two new columns: one with assigned cluster labels and one indicating whether a\n            point is a core point (1) or not (0).\n\n    Raises:\n        EmptyDataFrameException: The input GeoDataFrame is empty.\n        InvalidParameterException: The maximum distance between two samples in a neighborhood is not greater\n            than zero or the number of samples in a neighborhood is not greater than one.\n    \"\"\"\n\n    if data.empty:\n        raise EmptyDataFrameException(\"The input GeoDataFrame is empty.\")\n\n    if max_distance &lt;= 0:\n        raise InvalidParameterValueException(\n            \"The input value for the maximum distance between two samples in a neighborhood must be greater than zero.\"\n        )\n\n    if min_samples &lt;= 1:\n        raise InvalidParameterValueException(\n            \"The input value for the minimum number of samples in a neighborhood must be greater than one.\"\n        )\n\n    dbscan_gdf = _dbscan(data, max_distance, min_samples)\n\n    return dbscan_gdf\n</code></pre>"},{"location":"exploratory_analyses/k_means_cluster/","title":"K-means clustering","text":""},{"location":"exploratory_analyses/k_means_cluster/#eis_toolkit.exploratory_analyses.k_means_cluster.k_means_clustering","title":"<code>k_means_clustering(data, number_of_clusters=None, random_state=None)</code>","text":"<p>Perform k-means clustering on the input data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>GeoDataFrame</code> <p>A GeoDataFrame containing the input data.</p> required <code>number_of_clusters</code> <code>Optional[int]</code> <p>The number of clusters (&gt;= 1) to form. Optional parameter. If not provided, optimal number of clusters is computed using the elbow method.</p> <code>None</code> <code>random_state</code> <code>Optional[int]</code> <p>A random number generation for centroid initialization to make the randomness deterministic. Optional parameter.</p> <code>None</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>GeoDataFrame containing assigned cluster labels.</p> <p>Raises:</p> Type Description <code>EmptyDataFrameException</code> <p>The input GeoDataFrame is empty.</p> <code>InvalidParameterException</code> <p>The number of clusters is less than one.</p> Source code in <code>eis_toolkit/exploratory_analyses/k_means_cluster.py</code> <pre><code>@beartype\ndef k_means_clustering(\n    data: gdp.GeoDataFrame, number_of_clusters: Optional[int] = None, random_state: Optional[int] = None\n) -&gt; gdp.GeoDataFrame:\n    \"\"\"\n    Perform k-means clustering on the input data.\n\n    Args:\n        data: A GeoDataFrame containing the input data.\n        number_of_clusters: The number of clusters (&gt;= 1) to form. Optional parameter. If not provided,\n            optimal number of clusters is computed using the elbow method.\n        random_state: A random number generation for centroid initialization to make\n            the randomness deterministic. Optional parameter.\n\n    Returns:\n        GeoDataFrame containing assigned cluster labels.\n\n    Raises:\n        EmptyDataFrameException: The input GeoDataFrame is empty.\n        InvalidParameterException: The number of clusters is less than one.\n    \"\"\"\n\n    if data.empty:\n        raise EmptyDataFrameException(\"The input GeoDataFrame is empty.\")\n\n    if number_of_clusters is not None and number_of_clusters &lt; 1:\n        raise InvalidParameterValueException(\"The input value for number of clusters must be at least one.\")\n\n    k_means_gdf = _k_means_clustering(data, number_of_clusters, random_state)\n\n    return k_means_gdf\n</code></pre>"},{"location":"exploratory_analyses/parallel_coordinates/","title":"Plot parallel coordinates","text":""},{"location":"exploratory_analyses/parallel_coordinates/#eis_toolkit.exploratory_analyses.parallel_coordinates.plot_parallel_coordinates","title":"<code>plot_parallel_coordinates(df, color_column_name, plot_title=None, palette_name=None, curved_lines=True)</code>","text":"<p>Plot a parallel coordinates plot.</p> <p>Automatically removes all rows containing null/nan values. Tries to convert columns to numeric to be able to plot them. If more than 8 columns are present (after numeric filtering), keeps only the first 8 to plot.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The DataFrame to plot.</p> required <code>color_column_name</code> <code>str</code> <p>The name of the column in df to use for color encoding.</p> required <code>plot_title</code> <code>Optional[str]</code> <p>The title for the plot. Default is None.</p> <code>None</code> <code>palette_name</code> <code>Optional[str]</code> <p>The name of the color palette to use. Default is None.</p> <code>None</code> <code>curved_lines</code> <code>bool</code> <p>If True, the plot will have curved instead of straight lines. Default is True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Figure</code> <p>A matplotlib figure containing the parallel coordinates plot.</p> <p>Raises:</p> Type Description <code>EmptyDataFrameException</code> <p>Raised when the DataFrame is empty.</p> <code>InvalidColumnException</code> <p>Raised when the color column is not found in the DataFrame.</p> <code>InconsistentDataTypesException</code> <p>Raised when the color column has multiple data types.</p> Source code in <code>eis_toolkit/exploratory_analyses/parallel_coordinates.py</code> <pre><code>@beartype\ndef plot_parallel_coordinates(\n    df: pd.DataFrame,\n    color_column_name: str,\n    plot_title: Optional[str] = None,\n    palette_name: Optional[str] = None,\n    curved_lines: bool = True,\n) -&gt; matplotlib.figure.Figure:\n    \"\"\"Plot a parallel coordinates plot.\n\n    Automatically removes all rows containing null/nan values. Tries to convert columns to numeric\n    to be able to plot them. If more than 8 columns are present (after numeric filtering), keeps only\n    the first 8 to plot.\n\n    Args:\n        df: The DataFrame to plot.\n        color_column_name: The name of the column in df to use for color encoding.\n        plot_title: The title for the plot. Default is None.\n        palette_name: The name of the color palette to use. Default is None.\n        curved_lines: If True, the plot will have curved instead of straight lines. Default is True.\n\n    Returns:\n        A matplotlib figure containing the parallel coordinates plot.\n\n    Raises:\n        EmptyDataFrameException: Raised when the DataFrame is empty.\n        InvalidColumnException: Raised when the color column is not found in the DataFrame.\n        InconsistentDataTypesException: Raised when the color column has multiple data types.\n    \"\"\"\n\n    if df.empty:\n        raise exceptions.EmptyDataFrameException(\"The input DataFrame is empty.\")\n\n    if color_column_name not in df.columns:\n        raise exceptions.InvalidColumnException(\n            f\"The provided color column {color_column_name} is not found in the DataFrame.\"\n        )\n\n    df = df.convert_dtypes()\n    df = df.apply(pd.to_numeric, errors=\"ignore\")\n\n    color_data = df[color_column_name].to_numpy()\n    if len(set([type(elem) for elem in color_data])) != 1:\n        raise exceptions.InconsistentDataTypesException(\n            \"The color column should have a consistent datatype. Multiple data types detected in the color column.\"\n        )\n\n    df = df.select_dtypes(include=np.number)\n\n    # Drop non-numeric columns and the column used for coloring\n    columns_to_drop = [color_column_name]\n    for column in df.columns.values:\n        if df[column].isnull().all():\n            columns_to_drop.append(column)\n    df = df.loc[:, ~df.columns.isin(columns_to_drop)]\n\n    # Keep only first 8 columns if more are still present\n    if len(df.columns.values) &gt; 8:\n        df = df.iloc[:, :8]\n\n    data_labels = df.columns.values\n    data = df.to_numpy()\n\n    fig = _plot_parallel_coordinates(\n        data=data,\n        data_labels=data_labels,\n        color_data=color_data,\n        color_column_name=color_column_name,\n        plot_title=plot_title,\n        palette_name=palette_name,\n        curved_lines=curved_lines,\n    )\n    return fig\n</code></pre>"},{"location":"exploratory_analyses/pca/","title":"PCA","text":""},{"location":"exploratory_analyses/pca/#eis_toolkit.exploratory_analyses.pca.compute_pca","title":"<code>compute_pca(data, number_of_components)</code>","text":"<p>Compute the principal components for the input data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>A DataFrame containing the input data.</p> required <code>number_of_components</code> <code>int</code> <p>The number of principal components to compute.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame containing the computed principal components.</p> <code>ndarray</code> <p>The explained variance ratios for each component.</p> <p>Raises:</p> Type Description <code>EmptyDataFrameException</code> <p>The input DataFrame is empty.</p> <code>InvalidNumberOfPrincipalComponents</code> <p>The number of principal components is less than 2 or more than number of columns in the DataFrame.</p> Source code in <code>eis_toolkit/exploratory_analyses/pca.py</code> <pre><code>@beartype\ndef compute_pca(data: pd.DataFrame, number_of_components: int) -&gt; Tuple[pd.DataFrame, np.ndarray]:\n    \"\"\"\n    Compute the principal components for the input data.\n\n    Args:\n        data: A DataFrame containing the input data.\n        number_of_components: The number of principal components to compute.\n\n    Returns:\n        DataFrame containing the computed principal components.\n        The explained variance ratios for each component.\n\n    Raises:\n        EmptyDataFrameException: The input DataFrame is empty.\n        InvalidNumberOfPrincipalComponents: The number of principal components is less than 2 or more than\n            number of columns in the DataFrame.\n    \"\"\"\n\n    if data.empty:\n        raise EmptyDataFrameException(\"The input DataFrame is empty.\")\n\n    if number_of_components &lt; 2 or number_of_components &gt; len(data.columns):\n        raise InvalidParameterValueException(\n            \"The number of principal components should be &gt;= 2 and at most the number of columns in the DataFrame.\"\n        )\n\n    principal_component_df, explained_variances = _compute_pca(data, number_of_components)\n    return principal_component_df, explained_variances\n</code></pre>"},{"location":"prediction/fuzzy_overlay/","title":"Fuzzy overlay","text":""},{"location":"prediction/fuzzy_overlay/#eis_toolkit.prediction.fuzzy_overlay.and_overlay","title":"<code>and_overlay(data)</code>","text":"<p>Compute an 'and' overlay operation with fuzzy logic.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>The input data as a 3D Numpy array. Each 2D array represents a raster band. Data points should be in the range [0, 1].</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>2D Numpy array with the result of the 'and' overlay operation. Values are in range [0, 1].</p> <p>Raises:</p> Type Description <code>InvalidParameterValueException</code> <p>If data values are not in range [0, 1].</p> Source code in <code>eis_toolkit/prediction/fuzzy_overlay.py</code> <pre><code>@beartype\ndef and_overlay(data: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Compute an 'and' overlay operation with fuzzy logic.\n\n    Args:\n        data: The input data as a 3D Numpy array. Each 2D array represents a raster band.\n            Data points should be in the range [0, 1].\n\n    Returns:\n        2D Numpy array with the result of the 'and' overlay operation. Values are in range [0, 1].\n\n    Raises:\n        InvalidParameterValueException: If data values are not in range [0, 1].\n    \"\"\"\n    _check_input_data(data=data)\n\n    return data.min(axis=0)\n</code></pre>"},{"location":"prediction/fuzzy_overlay/#eis_toolkit.prediction.fuzzy_overlay.gamma_overlay","title":"<code>gamma_overlay(data, gamma)</code>","text":"<p>Compute a 'gamma' overlay operation with fuzzy logic.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>The input data as a 3D Numpy array. Each 2D array represents a raster band. Data points should be in the range [0, 1].</p> required <code>gamma</code> <code>float</code> <p>The gamma parameter. With gamma value 0, result will be same as 'product'overlay. When gamma is closer to 1, the weight of 'sum' overlay is increased. Value must be in the range [0, 1].</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>2D Numpy array with the result of the 'gamma' overlay operation. Values are in range [0, 1].</p> <p>Raises:</p> Type Description <code>InvalidParameterValueException</code> <p>If data values or gamma are not in range [0, 1].</p> Source code in <code>eis_toolkit/prediction/fuzzy_overlay.py</code> <pre><code>@beartype\ndef gamma_overlay(data: np.ndarray, gamma: float) -&gt; np.ndarray:\n    \"\"\"Compute a 'gamma' overlay operation with fuzzy logic.\n\n    Args:\n        data: The input data as a 3D Numpy array. Each 2D array represents a raster band.\n            Data points should be in the range [0, 1].\n        gamma: The gamma parameter. With gamma value 0, result will be same as 'product'overlay.\n            When gamma is closer to 1, the weight of 'sum' overlay is increased.\n            Value must be in the range [0, 1].\n\n    Returns:\n        2D Numpy array with the result of the 'gamma' overlay operation. Values are in range [0, 1].\n\n    Raises:\n        InvalidParameterValueException: If data values or gamma are not in range [0, 1].\n    \"\"\"\n    if gamma &lt; 0 or gamma &gt; 1:\n        raise exceptions.InvalidParameterValueException(\"The gamma parameter must be in range [0, 1]\")\n\n    sum = sum_overlay(data=data)\n    product = product_overlay(data=data)\n    return product ** (1 - gamma) * sum**gamma\n</code></pre>"},{"location":"prediction/fuzzy_overlay/#eis_toolkit.prediction.fuzzy_overlay.or_overlay","title":"<code>or_overlay(data)</code>","text":"<p>Compute an 'or' overlay operation with fuzzy logic.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>The input data as a 3D Numpy array. Each 2D array represents a raster band. Data points should be in the range [0, 1].</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>2D Numpy array with the result of the 'or' overlay operation. Values are in range [0, 1].</p> <p>Raises:</p> Type Description <code>InvalidParameterValueException</code> <p>If data values are not in range [0, 1].</p> Source code in <code>eis_toolkit/prediction/fuzzy_overlay.py</code> <pre><code>@beartype\ndef or_overlay(data: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Compute an 'or' overlay operation with fuzzy logic.\n\n    Args:\n        data: The input data as a 3D Numpy array. Each 2D array represents a raster band.\n            Data points should be in the range [0, 1].\n\n    Returns:\n        2D Numpy array with the result of the 'or' overlay operation. Values are in range [0, 1].\n\n    Raises:\n        InvalidParameterValueException: If data values are not in range [0, 1].\n    \"\"\"\n    _check_input_data(data=data)\n\n    return data.max(axis=0)\n</code></pre>"},{"location":"prediction/fuzzy_overlay/#eis_toolkit.prediction.fuzzy_overlay.product_overlay","title":"<code>product_overlay(data)</code>","text":"<p>Compute a 'product' overlay operation with fuzzy logic.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>The input data as a 3D Numpy array. Each 2D array represents a raster band. Data points should be in the range [0, 1].</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>2D Numpy array with the result of the 'product' overlay operation. Values are in range [0, 1].</p> <p>Raises:</p> Type Description <code>InvalidParameterValueException</code> <p>If data values are not in range [0, 1].</p> Source code in <code>eis_toolkit/prediction/fuzzy_overlay.py</code> <pre><code>@beartype\ndef product_overlay(data: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Compute a 'product' overlay operation with fuzzy logic.\n\n    Args:\n        data: The input data as a 3D Numpy array. Each 2D array represents a raster band.\n            Data points should be in the range [0, 1].\n\n    Returns:\n        2D Numpy array with the result of the 'product' overlay operation. Values are in range [0, 1].\n\n    Raises:\n        InvalidParameterValueException: If data values are not in range [0, 1].\n    \"\"\"\n    _check_input_data(data=data)\n\n    return np.prod(data, axis=0)\n</code></pre>"},{"location":"prediction/fuzzy_overlay/#eis_toolkit.prediction.fuzzy_overlay.sum_overlay","title":"<code>sum_overlay(data)</code>","text":"<p>Compute a 'sum' overlay operation with fuzzy logic.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>The input data as a 3D Numpy array. Each 2D array represents a raster band. Data points should be in the range [0, 1].</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>2D Numpy array with the result of the 'sum' overlay operation. Values are in range [0, 1].</p> <p>Raises:</p> Type Description <code>InvalidParameterValueException</code> <p>If data values are not in range [0, 1].</p> Source code in <code>eis_toolkit/prediction/fuzzy_overlay.py</code> <pre><code>@beartype\ndef sum_overlay(data: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Compute a 'sum' overlay operation with fuzzy logic.\n\n    Args:\n        data: The input data as a 3D Numpy array. Each 2D array represents a raster band.\n            Data points should be in the range [0, 1].\n\n    Returns:\n        2D Numpy array with the result of the 'sum' overlay operation. Values are in range [0, 1].\n\n    Raises:\n        InvalidParameterValueException: If data values are not in range [0, 1].\n    \"\"\"\n    _check_input_data(data=data)\n\n    return data.sum(axis=0) - np.prod(data, axis=0)\n</code></pre>"},{"location":"prediction/weights_of_evidence/","title":"Weights of evidence","text":""},{"location":"prediction/weights_of_evidence/#eis_toolkit.prediction.weights_of_evidence.weights_of_evidence_calculate_responses","title":"<code>weights_of_evidence_calculate_responses(output_arrays, nr_of_deposits, nr_of_pixels)</code>","text":"<p>Calculate the posterior probabilities for the given generalized weight arrays.</p> <p>Parameters:</p> Name Type Description Default <code>output_arrays</code> <code>Sequence[Dict[str, ndarray]]</code> <p>List of output array dictionaries returned by weights of evidence calculations. For each dictionary, generalized weight and generalized standard deviation arrays are used and summed together pixel-wise to calculate the posterior probabilities. If generalized arrays are not found, the W+ and S_W+ arrays are used (so if outputs from unique weight calculations are used for this function).</p> required <code>nr_of_deposits</code> <code>int</code> <p>Number of deposit pixels in the input data for weights of evidence calculations.</p> required <code>nr_of_pixels</code> <code>int</code> <p>Number of evidence pixels in the input data for weights of evidence calculations.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Array of posterior probabilites.</p> <code>ndarray</code> <p>Array of standard deviations in the posterior probability calculations.</p> <code>ndarray</code> <p>Array of confidence of the prospectivity values obtained in the posterior probability array.</p> Source code in <code>eis_toolkit/prediction/weights_of_evidence.py</code> <pre><code>@beartype\ndef weights_of_evidence_calculate_responses(\n    output_arrays: Sequence[Dict[str, np.ndarray]], nr_of_deposits: int, nr_of_pixels: int\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Calculate the posterior probabilities for the given generalized weight arrays.\n\n    Args:\n        output_arrays: List of output array dictionaries returned by weights of evidence calculations.\n            For each dictionary, generalized weight and generalized standard deviation arrays are used and summed\n            together pixel-wise to calculate the posterior probabilities. If generalized arrays are not found,\n            the W+ and S_W+ arrays are used (so if outputs from unique weight calculations are used for this function).\n        nr_of_deposits: Number of deposit pixels in the input data for weights of evidence calculations.\n        nr_of_pixels: Number of evidence pixels in the input data for weights of evidence calculations.\n\n    Returns:\n        Array of posterior probabilites.\n        Array of standard deviations in the posterior probability calculations.\n        Array of confidence of the prospectivity values obtained in the posterior probability array.\n    \"\"\"\n    gen_weights_sum = sum(\n        [\n            item[GENERALIZED_WEIGHT_PLUS_COLUMN]\n            if GENERALIZED_WEIGHT_PLUS_COLUMN in item.keys()\n            else item[WEIGHT_PLUS_COLUMN]\n            for item in output_arrays\n        ]\n    )\n    gen_weights_variance_sum = sum(\n        [\n            np.square(item[GENERALIZED_S_WEIGHT_PLUS_COLUMN])\n            if GENERALIZED_S_WEIGHT_PLUS_COLUMN in item.keys()\n            else np.square(item[WEIGHT_S_PLUS_COLUMN])\n            for item in output_arrays\n        ]\n    )\n\n    prior_probabilities = nr_of_deposits / nr_of_pixels\n    prior_odds = np.log(prior_probabilities / (1 - prior_probabilities))\n    posterior_probabilities = np.exp(gen_weights_sum + prior_odds) / (1 + np.exp(gen_weights_sum + prior_odds))\n\n    posterior_probabilities_squared = np.square(posterior_probabilities)\n    posterior_probabilities_std = np.sqrt(\n        (1 / nr_of_deposits + gen_weights_variance_sum) * posterior_probabilities_squared\n    )\n\n    confidence_array = posterior_probabilities / posterior_probabilities_std\n    return posterior_probabilities, posterior_probabilities_std, confidence_array\n</code></pre>"},{"location":"prediction/weights_of_evidence/#eis_toolkit.prediction.weights_of_evidence.weights_of_evidence_calculate_weights","title":"<code>weights_of_evidence_calculate_weights(evidential_raster, deposits, raster_nodata=None, weights_type='unique', studentized_contrast_threshold=1, arrays_to_generate=None)</code>","text":"<p>Calculate weights of spatial associations.</p> <p>Parameters:</p> Name Type Description Default <code>evidential_raster</code> <code>DatasetReader</code> <p>The evidential raster.</p> required <code>deposits</code> <code>GeoDataFrame</code> <p>Vector data representing the mineral deposits or occurences point data.</p> required <code>raster_nodata</code> <code>Optional[Number]</code> <p>If nodata value of raster is wanted to specify manually. Optional parameter, defaults to None (nodata from raster metadata is used).</p> <code>None</code> <code>weights_type</code> <code>Literal[unique, categorical, ascending, descending]</code> <p>Accepted values are 'unique', 'categorical', 'ascending' and 'descending'. Unique weights does not create generalized classes and does not use a studentized contrast threshold value while categorical, cumulative ascending and cumulative descending do. Categorical weights are calculated so that all classes with studentized contrast below the defined threshold are grouped into one generalized class. Cumulative ascending and descending weights find the class with max contrast and group classes above/below into generalized classes. Generalized weights are also calculated for generalized classes.</p> <code>'unique'</code> <code>studentized_contrast_threshold</code> <code>Number</code> <p>Studentized contrast threshold value used with 'categorical', 'ascending' and 'descending' weight types. Used either as reclassification threshold directly (categorical) or to check that class with max contrast has studentized contrast value at least the defined value (cumulative). Defaults to 1.</p> <code>1</code> <code>arrays_to_generate</code> <code>Optional[Sequence[str]]</code> <p>Arrays to generate from the computed weight metrics. All column names in the produced weights_df are valid choices. Defaults to [\"Class\", \"W+\", \"S_W+] for \"unique\" weights_type and [\"Class\", \"W+\", \"S_W+\", \"Generalized W+\", \"Generalized S_W+\"] for the cumulative weight types.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Dataframe with weights of spatial association between the input data.</p> <code>dict</code> <p>Dictionary of arrays for specified metrics.</p> <code>dict</code> <p>Raster metadata.</p> <code>int</code> <p>Number of deposit pixels.</p> <code>int</code> <p>Number of all evidence pixels.</p> Source code in <code>eis_toolkit/prediction/weights_of_evidence.py</code> <pre><code>@beartype\ndef weights_of_evidence_calculate_weights(\n    evidential_raster: rasterio.io.DatasetReader,\n    deposits: gpd.GeoDataFrame,\n    raster_nodata: Optional[Number] = None,\n    weights_type: Literal[\"unique\", \"categorical\", \"ascending\", \"descending\"] = \"unique\",\n    studentized_contrast_threshold: Number = 1,\n    arrays_to_generate: Optional[Sequence[str]] = None,\n) -&gt; Tuple[pd.DataFrame, dict, dict, int, int]:\n    \"\"\"\n    Calculate weights of spatial associations.\n\n    Args:\n        evidential_raster: The evidential raster.\n        deposits: Vector data representing the mineral deposits or occurences point data.\n        raster_nodata: If nodata value of raster is wanted to specify manually. Optional parameter, defaults to None\n            (nodata from raster metadata is used).\n        weights_type: Accepted values are 'unique', 'categorical', 'ascending' and 'descending'.\n            Unique weights does not create generalized classes and does not use a studentized contrast threshold value\n            while categorical, cumulative ascending and cumulative descending do. Categorical weights are calculated so\n            that all classes with studentized contrast below the defined threshold are grouped into one generalized\n            class. Cumulative ascending and descending weights find the class with max contrast and group classes\n            above/below into generalized classes. Generalized weights are also calculated for generalized classes.\n        studentized_contrast_threshold: Studentized contrast threshold value used with 'categorical', 'ascending' and\n            'descending' weight types. Used either as reclassification threshold directly (categorical) or to check\n            that class with max contrast has studentized contrast value at least the defined value (cumulative).\n            Defaults to 1.\n        arrays_to_generate: Arrays to generate from the computed weight metrics. All column names\n            in the produced weights_df are valid choices. Defaults to [\"Class\", \"W+\", \"S_W+]\n            for \"unique\" weights_type and [\"Class\", \"W+\", \"S_W+\", \"Generalized W+\", \"Generalized S_W+\"]\n            for the cumulative weight types.\n\n    Returns:\n        Dataframe with weights of spatial association between the input data.\n        Dictionary of arrays for specified metrics.\n        Raster metadata.\n        Number of deposit pixels.\n        Number of all evidence pixels.\n    \"\"\"\n\n    if arrays_to_generate is None:\n        if weights_type == \"unique\":\n            metrics_to_arrays = DEFAULT_METRICS_UNIQUE\n        else:\n            metrics_to_arrays = DEFAULT_METRICS_CUMULATIVE\n    else:\n        for col_name in arrays_to_generate:\n            if col_name not in VALID_DF_COLUMNS:\n                raise exceptions.InvalidColumnException(\n                    f\"Arrays to generate contains invalid metric / column name: {col_name}.\"\n                )\n        metrics_to_arrays = arrays_to_generate.copy()\n\n    # 1. Preprocess data\n    evidence_array = _read_and_preprocess_evidence(evidential_raster, raster_nodata)\n    raster_meta = evidential_raster.meta\n\n    # Rasterize deposits\n    deposit_array, _ = rasterize_vector(\n        geodataframe=deposits, default_value=1.0, base_raster_profile=raster_meta, fill_value=0.0\n    )\n\n    # Mask NaN out of the array\n    nodata_mask = np.isnan(evidence_array)\n    masked_evidence_array = evidence_array[~nodata_mask]\n    masked_deposit_array = deposit_array[~nodata_mask]\n\n    # 2. WofE calculations\n    if weights_type == \"unique\" or weights_type == \"categorical\":\n        wofe_weights = _unique_weights(masked_deposit_array, masked_evidence_array)\n    elif weights_type == \"ascending\":\n        wofe_weights = _cumulative_weights(masked_deposit_array, masked_evidence_array, ascending=True)\n    elif weights_type == \"descending\":\n        wofe_weights = _cumulative_weights(masked_deposit_array, masked_evidence_array, ascending=False)\n    else:\n        raise exceptions.InvalidParameterValueException(\n            \"Expected weights_type to be one of unique, categorical, ascending or descending.\"\n        )\n\n    # 3. Create DataFrame based on calculated metrics\n    df_entries = []\n    for cls, metrics in wofe_weights.items():\n        metrics = [round(metric, 4) if isinstance(metric, np.floating) else metric for metric in metrics]\n        A, _, C, _, w_plus, s_w_plus, w_minus, s_w_minus, contrast, s_contrast, studentized_contrast = metrics\n        df_entries.append(\n            {\n                CLASS_COLUMN: cls,\n                PIXEL_COUNT_COLUMN: A + C,\n                DEPOSIT_COUNT_COLUMN: A,\n                WEIGHT_PLUS_COLUMN: w_plus,\n                WEIGHT_S_PLUS_COLUMN: s_w_plus,\n                WEIGHT_MINUS_COLUMN: w_minus,\n                WEIGHT_S_MINUS_COLUMN: s_w_minus,\n                CONTRAST_COLUMN: contrast,\n                S_CONTRAST_COLUMN: s_contrast,\n                STUDENTIZED_CONTRAST_COLUMN: studentized_contrast,\n            }\n        )\n    weights_df = pd.DataFrame(df_entries)\n\n    # 4. If we use cumulative weights type, calculate generalized classes and weights\n    if weights_type == \"categorical\":\n        weights_df = _generalized_classes_categorical(weights_df, studentized_contrast_threshold)\n        weights_df = _generalized_weights_categorical(weights_df, masked_deposit_array)\n    elif weights_type == \"ascending\" or weights_type == \"descending\":\n        weights_df = _generalized_classes_cumulative(weights_df, studentized_contrast_threshold)\n        weights_df = _generalized_weights_cumulative(weights_df, masked_deposit_array)\n\n    # 5. Generate arrays for desired metrics\n    arrays_dict = _generate_arrays_from_metrics(evidence_array, weights_df, metrics_to_arrays)\n\n    # Return nr. of deposit pixels  and nr. of all evidence pixels for to be used in calculate responses\n    nr_of_deposits = int(np.sum(masked_deposit_array == 1))\n    nr_of_pixels = int(np.size(masked_evidence_array))\n\n    return weights_df, arrays_dict, raster_meta, nr_of_deposits, nr_of_pixels\n</code></pre>"},{"location":"raster_processing/check_raster_grids/","title":"Check raster grids","text":""},{"location":"raster_processing/check_raster_grids/#eis_toolkit.raster_processing.check_raster_grids.check_raster_grids","title":"<code>check_raster_grids(rasters, same_extent=False)</code>","text":"<p>Check the set of input rasters for matching gridding and optionally matching bounds.</p> <p>Parameters:</p> Name Type Description Default <code>rasters</code> <code>List[DatasetReader]</code> <p>List of rasters to test for matching gridding.</p> required <code>same_extent</code> <code>bool</code> <p>optional boolean argument that determines if rasters are tested for matching bounds. Default set to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if gridding and optionally bounds matches, False if not.</p> Source code in <code>eis_toolkit/raster_processing/check_raster_grids.py</code> <pre><code>def check_raster_grids(  # type: ignore[no-any-unimported]\n    rasters: List[rasterio.io.DatasetReader], same_extent: bool = False\n) -&gt; bool:\n    \"\"\"\n    Check the set of input rasters for matching gridding and optionally matching bounds.\n\n    Args:\n        rasters: List of rasters to test for matching gridding.\n        same_extent: optional boolean argument that determines if rasters are tested for matching bounds.\n            Default set to False.\n\n    Returns:\n        True if gridding and optionally bounds matches, False if not.\n    \"\"\"\n    check = _check_raster_grids(rasters=rasters, same_extent=same_extent)\n    return check\n</code></pre>"},{"location":"raster_processing/clipping/","title":"Clipping","text":""},{"location":"raster_processing/clipping/#eis_toolkit.raster_processing.clipping.clip_raster","title":"<code>clip_raster(raster, geodataframe)</code>","text":"<p>Clips a raster with polygon geometries.</p> <p>Parameters:</p> Name Type Description Default <code>raster</code> <code>DatasetReader</code> <p>The raster to be clipped.</p> required <code>geodataframe</code> <code>GeoDataFrame</code> <p>A geodataframe containing the geometries to do the clipping with. Should contain only polygon features.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>The clipped raster data.</p> <code>dict</code> <p>The updated metadata.</p> <p>Raises:</p> Type Description <code>NonMatchingCrsException</code> <p>The raster and geodataframe are not in the same CRS.</p> <code>NotApplicableGeometryTypeException</code> <p>The input geometries contain non-polygon features.</p> Source code in <code>eis_toolkit/raster_processing/clipping.py</code> <pre><code>@beartype\ndef clip_raster(raster: rasterio.io.DatasetReader, geodataframe: geopandas.GeoDataFrame) -&gt; Tuple[np.ndarray, dict]:\n    \"\"\"Clips a raster with polygon geometries.\n\n    Args:\n        raster: The raster to be clipped.\n        geodataframe: A geodataframe containing the geometries to do the clipping with.\n            Should contain only polygon features.\n\n    Returns:\n        The clipped raster data.\n        The updated metadata.\n\n    Raises:\n        NonMatchingCrsException: The raster and geodataframe are not in the same CRS.\n        NotApplicableGeometryTypeException: The input geometries contain non-polygon features.\n    \"\"\"\n    geometries = geodataframe[\"geometry\"]\n\n    if not check_matching_crs(\n        objects=[raster, geometries],\n    ):\n        raise NonMatchingCrsException(\"The raster and geodataframe are not in the same CRS.\")\n\n    if not check_geometry_types(\n        geometries=geometries,\n        allowed_types=[\"Polygon\", \"MultiPolygon\"],\n    ):\n        raise NotApplicableGeometryTypeException(\"The input geometries contain non-polygon features.\")\n\n    out_image, out_meta = _clip_raster(\n        raster=raster,\n        geometries=geometries,\n    )\n\n    return out_image, out_meta\n</code></pre>"},{"location":"raster_processing/create_constant_raster/","title":"Create constant raster","text":""},{"location":"raster_processing/create_constant_raster/#eis_toolkit.raster_processing.create_constant_raster.create_constant_raster","title":"<code>create_constant_raster(constant_value, template_raster=None, coord_west=None, coord_north=None, coord_east=None, coord_south=None, target_epsg=None, target_pixel_size=None, raster_width=None, raster_height=None, nodata_value=None)</code>","text":"<p>Create a constant raster based on a user-defined value.</p> <p>Provide 3 methods for raster creation: 1. Set extent and coordinate system based on a template raster. 2. Set extent from origin, based on the western and northern coordinates and the pixel size. 3. Set extent from bounds, based on western, northern, eastern and southern points.</p> <p>Always provide values for height and width for the last two options, which correspond to the desired number of pixels for rows and columns.</p> <p>Parameters:</p> Name Type Description Default <code>constant_value</code> <code>Number</code> <p>The constant value to use in the raster.</p> required <code>template_raster</code> <code>Optional[DatasetReader]</code> <p>An optional raster to use as a template for the output.</p> <code>None</code> <code>coord_west</code> <code>Optional[Number]</code> <p>The western coordinate of the output raster in [m].</p> <code>None</code> <code>coord_east</code> <code>Optional[Number]</code> <p>The eastern coordinate of the output raster in [m].</p> <code>None</code> <code>coord_south</code> <code>Optional[Number]</code> <p>The southern coordinate of the output raster in [m].</p> <code>None</code> <code>coord_north</code> <code>Optional[Number]</code> <p>The northern coordinate of the output raster in [m].</p> <code>None</code> <code>target_epsg</code> <code>Optional[int]</code> <p>The EPSG code for the output raster.</p> <code>None</code> <code>target_pixel_size</code> <code>Optional[int]</code> <p>The pixel size of the output raster.</p> <code>None</code> <code>raster_width</code> <code>Optional[int]</code> <p>The width of the output raster.</p> <code>None</code> <code>raster_height</code> <code>Optional[int]</code> <p>The height of the output raster.</p> <code>None</code> <code>nodata_value</code> <code>Optional[Number]</code> <p>The nodata value of the output raster.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, dict]</code> <p>A tuple containing the output raster as a NumPy array and updated metadata.</p> <p>Raises:</p> Type Description <code>InvalidParameterValueException</code> <p>Provide invalid input parameter.</p> Source code in <code>eis_toolkit/raster_processing/create_constant_raster.py</code> <pre><code>@beartype\ndef create_constant_raster(  # type: ignore[no-any-unimported]\n    constant_value: Number,\n    template_raster: Optional[rasterio.io.DatasetReader] = None,\n    coord_west: Optional[Number] = None,\n    coord_north: Optional[Number] = None,\n    coord_east: Optional[Number] = None,\n    coord_south: Optional[Number] = None,\n    target_epsg: Optional[int] = None,\n    target_pixel_size: Optional[int] = None,\n    raster_width: Optional[int] = None,\n    raster_height: Optional[int] = None,\n    nodata_value: Optional[Number] = None,\n) -&gt; Tuple[np.ndarray, dict]:\n    \"\"\"Create a constant raster based on a user-defined value.\n\n    Provide 3 methods for raster creation:\n    1. Set extent and coordinate system based on a template raster.\n    2. Set extent from origin, based on the western and northern coordinates and the pixel size.\n    3. Set extent from bounds, based on western, northern, eastern and southern points.\n\n    Always provide values for height and width for the last two options, which correspond to\n    the desired number of pixels for rows and columns.\n\n    Args:\n        constant_value: The constant value to use in the raster.\n        template_raster: An optional raster to use as a template for the output.\n        coord_west: The western coordinate of the output raster in [m].\n        coord_east: The eastern coordinate of the output raster in [m].\n        coord_south: The southern coordinate of the output raster in [m].\n        coord_north: The northern coordinate of the output raster in [m].\n        target_epsg: The EPSG code for the output raster.\n        target_pixel_size: The pixel size of the output raster.\n        raster_width: The width of the output raster.\n        raster_height: The height of the output raster.\n        nodata_value: The nodata value of the output raster.\n\n    Returns:\n        A tuple containing the output raster as a NumPy array and updated metadata.\n\n    Raises:\n        InvalidParameterValueException: Provide invalid input parameter.\n    \"\"\"\n\n    if template_raster is not None:\n        out_array, out_meta = _create_constant_raster_from_template(constant_value, template_raster, nodata_value)\n\n    elif all(coords is not None for coords in [coord_west, coord_east, coord_south, coord_north]):\n        if raster_height &lt;= 0 or raster_width &lt;= 0:\n            raise InvalidParameterValueException(\"Invalid raster extent provided.\")\n        if not check_minmax_position((coord_west, coord_east) or not check_minmax_position((coord_south, coord_north))):\n            raise InvalidParameterValueException(\"Invalid coordinate values provided.\")\n\n        out_array, out_meta = _create_constant_raster_from_bounds(\n            constant_value,\n            coord_west,\n            coord_north,\n            coord_east,\n            coord_south,\n            target_epsg,\n            raster_width,\n            raster_height,\n            nodata_value,\n        )\n\n    elif all(coords is not None for coords in [coord_west, coord_north]) and all(\n        coords is None for coords in [coord_east, coord_south]\n    ):\n        if raster_height &lt;= 0 or raster_width &lt;= 0:\n            raise InvalidParameterValueException(\"Invalid raster extent provided.\")\n        if target_pixel_size &lt;= 0:\n            raise InvalidParameterValueException(\"Invalid pixel size.\")\n\n        out_array, out_meta = _create_constant_raster_from_origin(\n            constant_value,\n            coord_west,\n            coord_north,\n            target_epsg,\n            target_pixel_size,\n            raster_width,\n            raster_height,\n            nodata_value,\n        )\n\n    else:\n        raise InvalidParameterValueException(\"Suitable parameter values were not provided for any of the 3 methods.\")\n\n    constant_value = cast_scalar_to_int(constant_value)\n    nodata_value = cast_scalar_to_int(out_meta[\"nodata\"])\n\n    if isinstance(constant_value, int) and isinstance(nodata_value, int):\n        target_dtype = np.result_type(get_min_int_type(constant_value), get_min_int_type(nodata_value))\n        out_array = out_array.astype(target_dtype)\n        out_meta[\"dtype\"] = out_array.dtype\n    elif isinstance(constant_value, int) and isinstance(nodata_value, float):\n        out_array = out_array.astype(get_min_int_type(constant_value))\n        out_meta[\"dtype\"] = np.float64.__name__\n    elif isinstance(constant_value, float):\n        out_array = out_array.astype(np.float64)\n        out_meta[\"dtype\"] = out_array.dtype\n\n    return out_array, out_meta\n</code></pre>"},{"location":"raster_processing/extract_values_from_raster/","title":"Extract values from raster","text":""},{"location":"raster_processing/extract_values_from_raster/#eis_toolkit.raster_processing.extract_values_from_raster.extract_values_from_raster","title":"<code>extract_values_from_raster(raster_list, geodataframe, raster_column_names=None)</code>","text":"<p>Extract raster values using point data to a DataFrame.</p> <p>If custom column names are not given, column names are file_name for singleband files    and file_name_bandnumber for multiband files. If custom column names are given, there    should be column names for each raster provided in the raster list.</p> <p>Parameters:</p> Name Type Description Default <code>raster_list</code> <code>Sequence[DatasetReader]</code> <p>List to extract values from.</p> required <code>geodataframe</code> <code>GeoDataFrame</code> <p>Object to extract values with.</p> required <code>raster_column_names</code> <code>Optional[Sequence[str]]</code> <p>List of optional column names for bands.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Dataframe with x &amp; y coordinates and the values from the raster file(s) as columns.</p> <p>Raises:</p> Type Description <code>NonMatchingParameterLengthsException</code> <p>raster_list and raster_columns_names have different lengths.</p> Source code in <code>eis_toolkit/raster_processing/extract_values_from_raster.py</code> <pre><code>@beartype\ndef extract_values_from_raster(\n    raster_list: Sequence[rasterio.io.DatasetReader],\n    geodataframe: gpd.GeoDataFrame,\n    raster_column_names: Optional[Sequence[str]] = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Extract raster values using point data to a DataFrame.\n\n       If custom column names are not given, column names are file_name for singleband files\n       and file_name_bandnumber for multiband files. If custom column names are given, there\n       should be column names for each raster provided in the raster list.\n\n    Args:\n        raster_list: List to extract values from.\n        geodataframe: Object to extract values with.\n        raster_column_names: List of optional column names for bands.\n\n    Returns:\n        Dataframe with x &amp; y coordinates and the values from the raster file(s) as columns.\n\n    Raises:\n        NonMatchingParameterLengthsException: raster_list and raster_columns_names have different lengths.\n    \"\"\"\n    if raster_column_names == []:\n        raster_column_names = None\n\n    if raster_column_names is not None and len(raster_list) != len(raster_column_names):\n        raise NonMatchingParameterLengthsException(\"Raster list and raster columns names have different lengths.\")\n\n    data_frame = _extract_values_from_raster(\n        raster_list=raster_list, geodataframe=geodataframe, raster_column_names=raster_column_names\n    )\n\n    return data_frame\n</code></pre>"},{"location":"raster_processing/reprojecting/","title":"Reprojecting","text":""},{"location":"raster_processing/reprojecting/#eis_toolkit.raster_processing.reprojecting.reproject_raster","title":"<code>reproject_raster(raster, target_crs, resampling_method=warp.Resampling.nearest)</code>","text":"<p>Reprojects raster to match given coordinate reference system (EPSG).</p> <p>Parameters:</p> Name Type Description Default <code>raster</code> <code>DatasetReader</code> <p>The raster to be reprojected.</p> required <code>target_crs</code> <code>int</code> <p>Target CRS as EPSG code.</p> required <code>resampling_method</code> <code>Resampling</code> <p>Resampling method. Most suitable method depends on the dataset and context. Nearest, bilinear and cubic are some common choices. This parameter defaults to nearest.</p> <code>nearest</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The reprojected raster data.</p> <code>dict</code> <p>The updated metadata.</p> <p>Raises:</p> Type Description <code>NonMatchinCrsException</code> <p>Raster is already in the target CRS.</p> Source code in <code>eis_toolkit/raster_processing/reprojecting.py</code> <pre><code>@beartype\ndef reproject_raster(\n    raster: rasterio.io.DatasetReader, target_crs: int, resampling_method: warp.Resampling = warp.Resampling.nearest\n) -&gt; Tuple[np.ndarray, dict]:\n    \"\"\"Reprojects raster to match given coordinate reference system (EPSG).\n\n    Args:\n        raster: The raster to be reprojected.\n        target_crs: Target CRS as EPSG code.\n        resampling_method: Resampling method. Most suitable method depends on the dataset and context.\n            Nearest, bilinear and cubic are some common choices. This parameter defaults to nearest.\n\n    Returns:\n        The reprojected raster data.\n        The updated metadata.\n\n    Raises:\n        NonMatchinCrsException: Raster is already in the target CRS.\n    \"\"\"\n    if target_crs == int(raster.crs.to_string()[5:]):\n        raise MatchingCrsException(\"Raster is already in the target CRS.\")\n\n    out_image, out_meta = _reproject_raster(raster, target_crs, resampling_method)\n\n    return out_image, out_meta\n</code></pre>"},{"location":"raster_processing/resampling/","title":"Resampling","text":""},{"location":"raster_processing/resampling/#eis_toolkit.raster_processing.resampling.resample","title":"<code>resample(raster, resolution, resampling_method=Resampling.bilinear)</code>","text":"<p>Resamples raster according to given resolution.</p> <p>Parameters:</p> Name Type Description Default <code>raster</code> <code>DatasetReader</code> <p>The raster to be resampled.</p> required <code>resolution</code> <code>Number</code> <p>Target resolution i.e. cell size of the output raster.</p> required <code>resampling_method</code> <code>Resampling</code> <p>Resampling method. Most suitable method depends on the dataset and context. Nearest, bilinear and cubic are some common choices. This parameter defaults to bilinear.</p> <code>bilinear</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The resampled raster data.</p> <code>dict</code> <p>The updated metadata.</p> <p>Raises:</p> Type Description <code>NumericValueSignException</code> <p>Resolution is not a positive value.</p> Source code in <code>eis_toolkit/raster_processing/resampling.py</code> <pre><code>@beartype\ndef resample(\n    raster: rasterio.io.DatasetReader,\n    resolution: Number,\n    resampling_method: Resampling = Resampling.bilinear,\n) -&gt; Tuple[np.ndarray, dict]:\n    \"\"\"Resamples raster according to given resolution.\n\n    Args:\n        raster: The raster to be resampled.\n        resolution: Target resolution i.e. cell size of the output raster.\n        resampling_method: Resampling method. Most suitable\n            method depends on the dataset and context. Nearest, bilinear and cubic are some\n            common choices. This parameter defaults to bilinear.\n\n    Returns:\n        The resampled raster data.\n        The updated metadata.\n\n    Raises:\n        NumericValueSignException: Resolution is not a positive value.\n    \"\"\"\n    if resolution &lt;= 0:\n        raise exceptions.NumericValueSignException(f\"Expected a positive value for resolution: {resolution})\")\n\n    out_image, out_meta = _resample(raster, resolution, resampling_method)\n    return out_image, out_meta\n</code></pre>"},{"location":"raster_processing/snapping/","title":"Snapping","text":""},{"location":"raster_processing/snapping/#eis_toolkit.raster_processing.snapping.snap_with_raster","title":"<code>snap_with_raster(raster, snap_raster)</code>","text":"<p>Snaps/aligns raster to given snap raster.</p> <p>Raster is snapped from its left-bottom corner to nearest snap raster grid corner in left-bottom direction. If rasters are aligned, simply returns input raster data and metadata.</p> <p>Parameters:</p> Name Type Description Default <code>raster</code> <code>DatasetReader</code> <p>The raster to be clipped.</p> required <code>snap_raster</code> <code>DatasetReader</code> <p>The snap raster i.e. reference grid raster.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>The snapped raster data.</p> <code>dict</code> <p>The updated metadata.</p> <p>Raises:</p> Type Description <code>NonMatchingCrsException</code> <p>Raster and and snap raster are not in the same CRS.</p> <code>MatchingRasterGridException</code> <p>Raster grids are already aligned.</p> Source code in <code>eis_toolkit/raster_processing/snapping.py</code> <pre><code>@beartype\ndef snap_with_raster(raster: rasterio.DatasetReader, snap_raster: rasterio.DatasetReader) -&gt; Tuple[np.ndarray, dict]:\n    \"\"\"Snaps/aligns raster to given snap raster.\n\n    Raster is snapped from its left-bottom corner to nearest snap raster grid corner in left-bottom direction.\n    If rasters are aligned, simply returns input raster data and metadata.\n\n    Args:\n        raster: The raster to be clipped.\n        snap_raster: The snap raster i.e. reference grid raster.\n\n    Returns:\n        The snapped raster data.\n        The updated metadata.\n\n    Raises:\n        NonMatchingCrsException: Raster and and snap raster are not in the same CRS.\n        MatchingRasterGridException: Raster grids are already aligned.\n    \"\"\"\n\n    if not check_matching_crs(\n        objects=[raster, snap_raster],\n    ):\n        raise NonMatchingCrsException(\"Raster and and snap raster have different CRS.\")\n\n    if snap_raster.bounds.bottom == raster.bounds.bottom and snap_raster.bounds.left == raster.bounds.left:\n        raise MatchingRasterGridException(\"Raster grids are already aligned.\")\n\n    out_image, out_meta = _snap(raster, snap_raster)\n    return out_image, out_meta\n</code></pre>"},{"location":"raster_processing/unifying/","title":"Unifying","text":""},{"location":"raster_processing/unifying/#eis_toolkit.raster_processing.unifying.unify_raster_grids","title":"<code>unify_raster_grids(base_raster, rasters_to_unify, resampling_method=Resampling.nearest, same_extent=False)</code>","text":"<p>Unifies (reprojects, resamples, aligns and optionally clips) given rasters relative to base raster.</p> <p>Parameters:</p> Name Type Description Default <code>base_raster</code> <code>DatasetReader</code> <p>The base raster to determine target raster grid properties.</p> required <code>rasters_to_unify</code> <code>Sequence[DatasetReader]</code> <p>Rasters to be unified with the base raster.</p> required <code>resampling_method</code> <code>Resampling</code> <p>Resampling method. Most suitable method depends on the dataset and context. Nearest, bilinear and cubic are some common choices. This parameter defaults to nearest.</p> <code>nearest</code> <code>same_extent</code> <code>bool</code> <p>If the unified rasters will be forced to have the same extent/bounds as the base raster. Expands smaller rasters with nodata cells. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>List[Tuple[ndarray, dict]]</code> <p>List of unified rasters' data and metadata. First element is the base raster.</p> <p>Raises:</p> Type Description <code>InvalidParameterValueException</code> <p>Rasters to unify is empty.</p> Source code in <code>eis_toolkit/raster_processing/unifying.py</code> <pre><code>@beartype\ndef unify_raster_grids(\n    base_raster: rasterio.io.DatasetReader,\n    rasters_to_unify: Sequence[rasterio.io.DatasetReader],\n    resampling_method: Resampling = Resampling.nearest,\n    same_extent: bool = False,\n) -&gt; List[Tuple[np.ndarray, dict]]:\n    \"\"\"Unifies (reprojects, resamples, aligns and optionally clips) given rasters relative to base raster.\n\n    Args:\n        base_raster: The base raster to determine target raster grid properties.\n        rasters_to_unify: Rasters to be unified with the base raster.\n        resampling_method: Resampling method. Most suitable\n            method depends on the dataset and context. Nearest, bilinear and cubic are some\n            common choices. This parameter defaults to nearest.\n        same_extent: If the unified rasters will be forced to have the same extent/bounds\n            as the base raster. Expands smaller rasters with nodata cells. Defaults to False.\n\n    Returns:\n        List of unified rasters' data and metadata. First element is the base raster.\n\n    Raises:\n        InvalidParameterValueException: Rasters to unify is empty.\n    \"\"\"\n    if len(rasters_to_unify) == 0:\n        raise InvalidParameterValueException(\"Rasters to unify is empty.\")\n\n    out_rasters = _unify_raster_grids(base_raster, rasters_to_unify, resampling_method, same_extent)\n    return out_rasters\n</code></pre>"},{"location":"raster_processing/windowing/","title":"Windowing","text":""},{"location":"raster_processing/windowing/#eis_toolkit.raster_processing.windowing.extract_window","title":"<code>extract_window(raster, center_coords, height, width)</code>","text":"<p>Extract window from raster.</p> <p>Center coordinate must be inside the raster but window can extent outside the raster in which case padding with    raster nodata value is used. Args:     raster: Source raster.     center_coords: Center coordinates for window in form (x, y). The coordinates should be in the raster's CRS.     height: Window height in pixels.     width: Window width in pixels.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>The extracted raster window.</p> <code>dict</code> <p>The updated metadata.</p> <p>Raises:</p> Type Description <code>InvalidParameterValueException</code> <p>Window size is too small.</p> <code>CoordinatesOutOfBoundException</code> <p>Window center coordinates are out of raster bounds.</p> Source code in <code>eis_toolkit/raster_processing/windowing.py</code> <pre><code>@beartype\ndef extract_window(\n    raster: rasterio.io.DatasetReader,\n    center_coords: Tuple[Number, Number],\n    height: int,\n    width: int,\n) -&gt; Tuple[np.ndarray, dict]:\n    \"\"\"Extract window from raster.\n\n       Center coordinate must be inside the raster but window can extent outside the raster in which case padding with\n       raster nodata value is used.\n    Args:\n        raster: Source raster.\n        center_coords: Center coordinates for window in form (x, y). The coordinates should be in the raster's CRS.\n        height: Window height in pixels.\n        width: Window width in pixels.\n\n    Returns:\n        The extracted raster window.\n        The updated metadata.\n\n    Raises:\n        InvalidParameterValueException: Window size is too small.\n        CoordinatesOutOfBoundException: Window center coordinates are out of raster bounds.\n    \"\"\"\n\n    if height &lt; 1 or width &lt; 1:\n        raise InvalidParameterValueException(f\"Window size is too small: {height}, {width}.\")\n\n    center_x = center_coords[0]\n    center_y = center_coords[1]\n\n    if (\n        center_x &lt; raster.bounds.left\n        or center_x &gt; raster.bounds.right\n        or center_y &lt; raster.bounds.bottom\n        or center_y &gt; raster.bounds.top\n    ):\n        raise CoordinatesOutOfBoundsException(\"Window center coordinates are out of raster bounds.\")\n\n    out_image, out_meta = _extract_window(raster, center_coords, height, width)\n\n    return out_image, out_meta\n</code></pre>"},{"location":"spatial_analyses/cell_based_association/","title":"Cell-Based Association","text":""},{"location":"spatial_analyses/cell_based_association/#eis_toolkit.spatial_analyses.cell_based_association.cell_based_association","title":"<code>cell_based_association(cell_size, geodata, output_path, column=None, subset_target_attribute_values=None, add_name=None, add_buffer=None)</code>","text":"<p>Creation of CBA matrix.</p> <p>Initializes a CBA matrix from a vector file. The mesh is calculated according to the geometries contained in this file and the size of cells. Allows to add multiple vector data to the matrix, based on targeted shapes and/or attributes.</p> <p>Parameters:</p> Name Type Description Default <code>cell_size</code> <code>int</code> <p>Size of the cells.</p> required <code>geodata</code> <code>List[GeoDataFrame]</code> <p>GeoDataFrame to create the CBA matrix. Additional GeoDataFrame(s) can be imputed to add to the CBA matrix.</p> required <code>output_path</code> <code>str</code> <p>Name of the saved .tif file.</p> required <code>column</code> <code>Optional[List[str]]</code> <p>Name of the column of interest. If no attribute is specified, then an artificial attribute is created representing the presence or absence of the geometries of this file for each cell of the CBA grid. A categorical attribute will generate as many columns (binary) in the CBA matrix than values considered of interest (dummification). See parameter . Additional column(s) can be imputed for each added GeoDataFrame(s). <code>None</code> <code>subset_target_attribute_values</code> <code>Optional[List[Union[None, list, str]]]</code> <p>List of values of interest of the target attribute, in case a categorical target attribute has been specified. Allows to filter a subset of relevant values. Additional values can be imputed for each added GeoDataFrame(s).</p> <code>None</code> <code>add_name</code> <code>Optional[List[Union[str, None]]]</code> <p>Name of the column(s) to add to the matrix.</p> <code>None</code> <code>add_buffer</code> <code>Optional[List[Union[Number, bool]]]</code> <p>Allow the use of a buffer around shapes before the intersection with CBA cells for the added GeoDataFrame(s). Minimize border effects or allow increasing positive samples (i.e. cells with mineralization). The size of the buffer is computed using the CRS (if projected CRS in meters: value in meters).</p> <code>None</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>CBA matrix is created.</p> Source code in <code>eis_toolkit/spatial_analyses/cell_based_association.py</code> <pre><code>@beartype\ndef cell_based_association(\n    cell_size: int,\n    geodata: List[gpd.GeoDataFrame],\n    output_path: str,\n    column: Optional[List[str]] = None,\n    subset_target_attribute_values: Optional[List[Union[None, list, str]]] = None,\n    add_name: Optional[List[Union[str, None]]] = None,\n    add_buffer: Optional[List[Union[Number, bool]]] = None,\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"Creation of CBA matrix.\n\n    Initializes a CBA matrix from a vector file. The mesh is calculated\n    according to the geometries contained in this file and the size of cells.\n    Allows to add multiple vector data to the matrix, based on targeted shapes\n    and/or attributes.\n\n    Args:\n        cell_size: Size of the cells.\n        geodata: GeoDataFrame to create the CBA matrix. Additional\n            GeoDataFrame(s) can be imputed to add to the CBA matrix.\n        output_path: Name of the saved .tif file.\n        column: Name of the column of interest. If no attribute is specified,\n            then an artificial attribute is created representing the presence\n            or absence of the geometries of this file for each cell of the CBA\n            grid. A categorical attribute will generate as many columns (binary)\n            in the CBA matrix than values considered of interest (dummification).\n            See parameter &lt;subset_target_attribute_values&gt;. Additional\n            column(s) can be imputed for each added GeoDataFrame(s).\n        subset_target_attribute_values: List of values of interest of the\n            target attribute, in case a categorical target attribute has been\n            specified. Allows to filter a subset of relevant values. Additional\n            values can be imputed for each added GeoDataFrame(s).\n        add_name: Name of the column(s) to add to the matrix.\n        add_buffer: Allow the use of a buffer around shapes before the\n            intersection with CBA cells for the added GeoDataFrame(s). Minimize\n            border effects or allow increasing positive samples (i.e. cells\n            with mineralization). The size of the buffer is computed using the\n            CRS (if projected CRS in meters: value in meters).\n\n    Returns:\n        CBA matrix is created.\n    \"\"\"\n\n    # Swapping None to list values\n    if column is None:\n        column = [\"\"]\n    if add_buffer is None:\n        add_buffer = [False]\n\n    # Consistency checks on input data\n    for frame in geodata:\n        if frame.empty:\n            raise exceptions.EmptyDataFrameException(\"The input GeoDataFrame is empty.\")\n\n    if cell_size &lt;= 0:\n        raise exceptions.InvalidParameterValueException(\"Expected cell size to be positive and non-zero.\")\n\n    add_buffer = [False if x == 0 else x for x in add_buffer]\n    if any(num &lt; 0 for num in add_buffer):\n        raise exceptions.InvalidParameterValueException(\"Expected buffer value to be positive, null or False.\")\n\n    for i, name in enumerate(column):\n        if column[i] == \"\":\n            if subset_target_attribute_values[i] is not None:\n                raise exceptions.InvalidParameterValueException(\"Can't use subset of values if no column is targeted.\")\n        elif column[i] not in geodata[i]:\n            raise exceptions.InvalidColumnException(\"Targeted column not found in the GeoDataFrame.\")\n\n    for i, subset in enumerate(subset_target_attribute_values):\n        if subset is not None:\n            for value in subset:\n                if value not in geodata[i][column[i]].unique():\n                    raise exceptions.InvalidParameterValueException(\n                        \"Subset of value(s) not found in the targeted column.\"\n                    )\n\n    # Computation\n    for i, data in enumerate(geodata):\n        if i == 0:\n            # Initialization of the CBA matrix\n            grid, cba = _init_from_vector_data(cell_size, geodata[0], column[0], subset_target_attribute_values[0])\n        else:\n            # If necessary, adding data to matrix\n            cba = _add_layer(\n                cba,\n                grid,\n                geodata[i],\n                column[i],\n                subset_target_attribute_values[i],\n                add_name[i - 1],\n                add_buffer[i - 1],\n            )\n\n    # Export\n    _to_raster(cba, output_path)\n\n    return cba\n</code></pre>"},{"location":"spatial_analyses/distance_computation/","title":"Distance computation","text":""},{"location":"spatial_analyses/distance_computation/#eis_toolkit.spatial_analyses.distance_computation.distance_computation","title":"<code>distance_computation(raster_profile, geometries)</code>","text":"<p>Calculate distance from raster cell to nearest geometry.</p> <p>Parameters:</p> Name Type Description Default <code>raster_profile</code> <code>Union[Profile, dict]</code> <p>The raster profile of the raster in which the distances to the nearest geometry are determined.</p> required <code>geometries</code> <code>GeoDataFrame</code> <p>The geometries to determine distance to.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>A 2D numpy array with the distances computed.</p> Source code in <code>eis_toolkit/spatial_analyses/distance_computation.py</code> <pre><code>@beartype\ndef distance_computation(raster_profile: Union[profiles.Profile, dict], geometries: gpd.GeoDataFrame) -&gt; np.ndarray:\n    \"\"\"Calculate distance from raster cell to nearest geometry.\n\n    Args:\n        raster_profile: The raster profile of the raster in which the distances\n            to the nearest geometry are determined.\n        geometries: The geometries to determine distance to.\n\n    Returns:\n        A 2D numpy array with the distances computed.\n\n    \"\"\"\n    if raster_profile.get(\"crs\") != geometries.crs:\n        raise exceptions.NonMatchingCrsException(\"Expected coordinate systems to match between raster and geometries. \")\n    if geometries.shape[0] == 0:\n        raise exceptions.EmptyDataFrameException(\"Expected GeoDataFrame to not be empty.\")\n\n    raster_width = raster_profile.get(\"width\")\n    raster_height = raster_profile.get(\"height\")\n\n    if not isinstance(raster_width, int) or not isinstance(raster_height, int):\n        raise exceptions.InvalidParameterValueException(\n            f\"Expected raster_profile to contain integer width and height. {raster_profile}\"\n        )\n\n    raster_transform = raster_profile.get(\"transform\")\n\n    if not isinstance(raster_transform, transform.Affine):\n        raise exceptions.InvalidParameterValueException(\n            f\"Expected raster_profile to contain an affine transformation. {raster_profile}\"\n        )\n\n    return _distance_computation(\n        raster_width=raster_width, raster_height=raster_height, raster_transform=raster_transform, geometries=geometries\n    )\n</code></pre>"},{"location":"statistical_analyses/descriptive_statistics/","title":"Descriptive statistics","text":""},{"location":"statistical_analyses/descriptive_statistics/#eis_toolkit.statistical_analyses.descriptive_statistics.descriptive_statistics_dataframe","title":"<code>descriptive_statistics_dataframe(input_data, column)</code>","text":"<p>Generate descriptive statistics from vector data.</p> <p>Generates min, max, mean, quantiles(25%, 50% and 75%), standard deviation, relative standard deviation and skewness.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Union[DataFrame, GeoDataFrame]</code> <p>Data to generate descriptive statistics from.</p> required <code>column</code> <code>str</code> <p>Specify the column to generate descriptive statistics from.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>The descriptive statistics in previously described order.</p> Source code in <code>eis_toolkit/statistical_analyses/descriptive_statistics.py</code> <pre><code>@beartype\ndef descriptive_statistics_dataframe(input_data: Union[pd.DataFrame, gpd.GeoDataFrame], column: str) -&gt; dict:\n    \"\"\"Generate descriptive statistics from vector data.\n\n    Generates min, max, mean, quantiles(25%, 50% and 75%), standard deviation, relative standard deviation and skewness.\n\n    Args:\n        input_data: Data to generate descriptive statistics from.\n        column: Specify the column to generate descriptive statistics from.\n\n    Returns:\n        The descriptive statistics in previously described order.\n    \"\"\"\n    if column not in input_data.columns:\n        raise InvalidColumnException\n    data = input_data[column]\n    statistics = _descriptive_statistics(data)\n    return statistics\n</code></pre>"},{"location":"statistical_analyses/descriptive_statistics/#eis_toolkit.statistical_analyses.descriptive_statistics.descriptive_statistics_raster","title":"<code>descriptive_statistics_raster(input_data)</code>","text":"<p>Generate descriptive statistics from raster data.</p> <p>Generates min, max, mean, quantiles(25%, 50% and 75%), standard deviation, relative standard deviation and skewness.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>DatasetReader</code> <p>Data to generate descriptive statistics from.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>The descriptive statistics in previously described order.</p> Source code in <code>eis_toolkit/statistical_analyses/descriptive_statistics.py</code> <pre><code>@beartype\ndef descriptive_statistics_raster(input_data: rasterio.io.DatasetReader) -&gt; dict:\n    \"\"\"Generate descriptive statistics from raster data.\n\n    Generates min, max, mean, quantiles(25%, 50% and 75%), standard deviation, relative standard deviation and skewness.\n\n    Args:\n        input_data: Data to generate descriptive statistics from.\n\n    Returns:\n        The descriptive statistics in previously described order.\n    \"\"\"\n    data = input_data.read().flatten()\n    statistics = _descriptive_statistics(data)\n    return statistics\n</code></pre>"},{"location":"validation/calculate_auc/","title":"Calculate AUC","text":""},{"location":"validation/calculate_auc/#eis_toolkit.validation.calculate_auc.calculate_auc","title":"<code>calculate_auc(x_values, y_values)</code>","text":"<p>Calculate area under curve (AUC).</p> <p>Calculates AUC for curve. X-axis should be either proportion of area ore false positive rate. Y-axis should be always true positive rate. AUC is calculated with sklearn.metrics.auc which uses trapezoidal rule for calculation.</p> <p>Parameters:</p> Name Type Description Default <code>x_values</code> <code>ndarray</code> <p>Either proportion of area or false positive rate values.</p> required <code>y_values</code> <code>ndarray</code> <p>True positive rate values.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The area under curve.</p> <p>Raises:</p> Type Description <code>InvalidParameterValueException</code> <p>x_values or y_values are out of bounds.</p> Source code in <code>eis_toolkit/validation/calculate_auc.py</code> <pre><code>@beartype\ndef calculate_auc(x_values: np.ndarray, y_values: np.ndarray) -&gt; float:\n    \"\"\"Calculate area under curve (AUC).\n\n    Calculates AUC for curve. X-axis should be either proportion of area ore false positive rate. Y-axis should be\n    always true positive rate. AUC is calculated with sklearn.metrics.auc which uses trapezoidal rule for calculation.\n\n    Args:\n        x_values: Either proportion of area or false positive rate values.\n        y_values: True positive rate values.\n\n    Returns:\n        The area under curve.\n\n    Raises:\n        InvalidParameterValueException: x_values or y_values are out of bounds.\n    \"\"\"\n    if x_values.max() &gt; 1 or x_values.min() &lt; 0:\n        raise InvalidParameterValueException(\"x_values should be within range 0-1\")\n\n    if y_values.max() &gt; 1 or y_values.min() &lt; 0:\n        raise InvalidParameterValueException(\"y_values should be within range 0-1\")\n\n    auc_value = _calculate_auc(x_values=x_values, y_values=y_values)\n    return auc_value\n</code></pre>"},{"location":"validation/calculate_base_metrics/","title":"Calculate base metrics","text":""},{"location":"validation/calculate_base_metrics/#eis_toolkit.validation.calculate_base_metrics.calculate_base_metrics","title":"<code>calculate_base_metrics(raster, deposits, band=1, negatives=None)</code>","text":"<p>Calculate true positive rate, proportion of area and false positive rate values for different thresholds.</p> <p>Function calculates true positive rate, proportion of area and false positive rate values for different thresholds which are determined from inputted deposit locations and mineral prospectivity map. Note that calculation of false positive rate is optional and is only done if negative point locations are provided.</p> <p>Parameters:</p> Name Type Description Default <code>raster</code> <code>DatasetReader</code> <p>Mineral prospectivity map or evidence layer.</p> required <code>deposits</code> <code>GeoDataFrame</code> <p>Mineral deposit locations as points.</p> required <code>band</code> <code>int</code> <p>Band index of the mineral prospectivity map. Defaults to 1.</p> <code>1</code> <code>negatives</code> <code>Optional[GeoDataFrame]</code> <p>Negative locations as points.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame containing true positive rate, proportion of area, threshold values and false positive rate (optional) values.</p> <p>Raises:</p> Type Description <code>NonMatchingCrsException</code> <p>The raster and point data are not in the same CRS.</p> <code>NotApplicableGeometryTypeException</code> <p>The input geometries contain non-point features.</p> Source code in <code>eis_toolkit/validation/calculate_base_metrics.py</code> <pre><code>@beartype\ndef calculate_base_metrics(\n    raster: rasterio.io.DatasetReader,\n    deposits: geopandas.GeoDataFrame,\n    band: int = 1,\n    negatives: Optional[geopandas.GeoDataFrame] = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Calculate true positive rate, proportion of area and false positive rate values for different thresholds.\n\n    Function calculates true positive rate, proportion of area and false positive rate values for different thresholds\n    which are determined from inputted deposit locations and mineral prospectivity map. Note that calculation of false\n    positive rate is optional and is only done if negative point locations are provided.\n\n    Args:\n        raster: Mineral prospectivity map or evidence layer.\n        deposits: Mineral deposit locations as points.\n        band: Band index of the mineral prospectivity map. Defaults to 1.\n        negatives: Negative locations as points.\n\n    Returns:\n        DataFrame containing true positive rate, proportion of area, threshold values and false positive\n            rate (optional) values.\n\n    Raises:\n        NonMatchingCrsException: The raster and point data are not in the same CRS.\n        NotApplicableGeometryTypeException: The input geometries contain non-point features.\n    \"\"\"\n    if negatives is not None:\n        geometries = pd.concat([deposits, negatives]).geometry\n    else:\n        geometries = deposits[\"geometry\"]\n\n    if not check_matching_crs(\n        objects=[raster, geometries],\n    ):\n        raise NonMatchingCrsException(\"The raster and deposits are not in the same CRS.\")\n\n    if not check_geometry_types(\n        geometries=geometries,\n        allowed_types=[\"Point\"],\n    ):\n        raise NotApplicableGeometryTypeException(\"The input geometries contain non-point features.\")\n\n    base_metrics = _calculate_base_metrics(raster=raster, deposits=deposits, band=band, negatives=negatives)\n\n    return base_metrics\n</code></pre>"},{"location":"validation/get_pa_intersection/","title":"Get P-A plot intersection point","text":""},{"location":"validation/get_pa_intersection/#eis_toolkit.validation.get_pa_intersection.get_pa_intersection","title":"<code>get_pa_intersection(true_positive_rate_values, proportion_of_area_values, threshold_values)</code>","text":"<p>Calculate the intersection point for prediction rate and area curves in (P-A plot).</p> <p>Threshold_values values act as x-axis for both curves. Prediction rate curve uses true positive rate for y-axis. Area curve uses inverted proportion of area as y-axis.</p> <p>Parameters:</p> Name Type Description Default <code>true_positive_rate_values</code> <code>ndarray</code> <p>True positive rate values, values should be within range 0-1.</p> required <code>proportion_of_area_values</code> <code>ndarray</code> <p>Proportion of area values, values should be within range 0-1.</p> required <code>threshold_values</code> <code>ndarray</code> <p>Threshold values that were used to calculate true positive rate and proportion of area.</p> required <p>Returns:</p> Type Description <code>Tuple[float, float]</code> <p>X and y coordinates of the intersection point.</p> <p>Raises:</p> Type Description <code>InvalidParameterValueException</code> <p>true_positive_rate_values or proportion_of_area_values values are out of bounds.</p> Source code in <code>eis_toolkit/validation/get_pa_intersection.py</code> <pre><code>@beartype\ndef get_pa_intersection(\n    true_positive_rate_values: np.ndarray, proportion_of_area_values: np.ndarray, threshold_values: np.ndarray\n) -&gt; Tuple[float, float]:\n    \"\"\"Calculate the intersection point for prediction rate and area curves in (P-A plot).\n\n    Threshold_values values act as x-axis for both curves. Prediction rate curve uses true positive rate for y-axis.\n    Area curve uses inverted proportion of area as y-axis.\n\n    Args:\n        true_positive_rate_values: True positive rate values, values should be within range 0-1.\n        proportion_of_area_values: Proportion of area values, values should be within range 0-1.\n        threshold_values: Threshold values that were used to calculate true positive rate and proportion of area.\n\n    Returns:\n        X and y coordinates of the intersection point.\n\n    Raises:\n        InvalidParameterValueException: true_positive_rate_values or proportion_of_area_values values are out of bounds.\n    \"\"\"\n    if true_positive_rate_values.max() &gt; 1 or true_positive_rate_values.min() &lt; 0:\n        raise InvalidParameterValueException(\"true_positive_rate_values values should be within range 0-1\")\n\n    if proportion_of_area_values.max() &gt; 1 or proportion_of_area_values.min() &lt; 0:\n        raise InvalidParameterValueException(\"proportion_of_area_values values should be within range 0-1\")\n\n    intersection = _get_pa_intersection(\n        true_positive_rate_values=true_positive_rate_values,\n        proportion_of_area_values=proportion_of_area_values,\n        threshold_values=threshold_values,\n    )\n\n    return intersection.x, intersection.y\n</code></pre>"},{"location":"validation/plot_prediction_area_curves/","title":"Plot prediction-area (P-A) curves","text":""},{"location":"validation/plot_prediction_area_curves/#eis_toolkit.validation.plot_prediction_area_curves.plot_prediction_area_curves","title":"<code>plot_prediction_area_curves(true_positive_rate_values, proportion_of_area_values, threshold_values)</code>","text":"<p>Plot prediction-area (P-A) plot.</p> <p>Plots prediction area plot that can be used to evaluate mineral prospectivity maps and evidential layers. See e.g., Yousefi and Carranza (2015).</p> <p>Parameters:</p> Name Type Description Default <code>true_positive_rate_values</code> <code>ndarray</code> <p>True positive rate values.</p> required <code>proportion_of_area_values</code> <code>ndarray</code> <p>Proportion of area values.</p> required <code>threshold_values</code> <code>ndarray</code> <p>Threshold values.</p> required <p>Returns:</p> Type Description <code>Figure</code> <p>P-A plot figure object.</p> <p>Raises:</p> Type Description <code>InvalidParameterValueException</code> <p>true_positive_rate_values or proportion_of_area_values values are out of bounds.</p> References <p>Yousefi, Mahyar, and Emmanuel John M. Carranza. \"Fuzzification of continuous-value spatial evidence for mineral prospectivity mapping.\" Computers &amp; Geosciences 74 (2015): 97-109.</p> Source code in <code>eis_toolkit/validation/plot_prediction_area_curves.py</code> <pre><code>@beartype\ndef plot_prediction_area_curves(\n    true_positive_rate_values: np.ndarray, proportion_of_area_values: np.ndarray, threshold_values: np.ndarray\n) -&gt; matplotlib.figure.Figure:\n    \"\"\"Plot prediction-area (P-A) plot.\n\n    Plots prediction area plot that can be used to evaluate mineral prospectivity maps and evidential layers. See e.g.,\n    Yousefi and Carranza (2015).\n\n    Args:\n        true_positive_rate_values: True positive rate values.\n        proportion_of_area_values: Proportion of area values.\n        threshold_values: Threshold values.\n\n    Returns:\n        P-A plot figure object.\n\n    Raises:\n        InvalidParameterValueException: true_positive_rate_values or proportion_of_area_values values are out of bounds.\n\n    References:\n        Yousefi, Mahyar, and Emmanuel John M. Carranza. \"Fuzzification of continuous-value spatial evidence for mineral\n        prospectivity mapping.\" Computers &amp; Geosciences 74 (2015): 97-109.\n    \"\"\"\n    if true_positive_rate_values.max() &gt; 1 or true_positive_rate_values.min() &lt; 0:\n        raise InvalidParameterValueException(\"true_positive_rate values should be within range 0-1\")\n\n    if proportion_of_area_values.max() &gt; 1 or proportion_of_area_values.min() &lt; 0:\n        raise InvalidParameterValueException(\"proportion_of_area values should be within range 0-1\")\n\n    fig = _plot_prediction_area_curves(\n        true_positive_rate_values=true_positive_rate_values,\n        proportion_of_area_values=proportion_of_area_values,\n        threshold_values=threshold_values,\n    )\n    return fig\n</code></pre>"},{"location":"validation/plot_rate_curve/","title":"Plot rate curve","text":""},{"location":"validation/plot_rate_curve/#eis_toolkit.validation.plot_rate_curve.plot_rate_curve","title":"<code>plot_rate_curve(x_values, y_values, plot_type='success_rate')</code>","text":"<p>Plot success rate, prediction rate or ROC curve.</p> <p>Plot type depends on plot_type argument. Y-axis is always true positive rate, while x-axis can be either false positive rate (roc) or proportion of area (success and prediction rate) depending on plot type.</p> <p>Parameters:</p> Name Type Description Default <code>x_values</code> <code>ndarray</code> <p>False positive rate values or proportion of area values.</p> required <code>y_values</code> <code>ndarray</code> <p>True positive rate values.</p> required <code>plot_type</code> <code>str</code> <p>Plot type. Can be either: \"success_rate\", \"prediction_rate\" or \"roc\".</p> <code>'success_rate'</code> <p>Returns:</p> Type Description <code>Figure</code> <p>Success rate, prediction rate or ROC plot figure object.</p> <p>Raises:</p> Type Description <code>InvalidParameterValueException</code> <p>Invalid plot type.</p> <code>InvalidParameterValueException</code> <p>x_values or y_values are out of bounds.</p> Source code in <code>eis_toolkit/validation/plot_rate_curve.py</code> <pre><code>@beartype\ndef plot_rate_curve(\n    x_values: np.ndarray,\n    y_values: np.ndarray,\n    plot_type: str = \"success_rate\",\n) -&gt; matplotlib.figure.Figure:\n    \"\"\"Plot success rate, prediction rate or ROC curve.\n\n    Plot type depends on plot_type argument. Y-axis is always true positive rate, while x-axis can be either false\n    positive rate (roc) or proportion of area (success and prediction rate) depending on plot type.\n\n    Args:\n        x_values: False positive rate values or proportion of area values.\n        y_values: True positive rate values.\n        plot_type: Plot type. Can be either: \"success_rate\", \"prediction_rate\" or \"roc\".\n\n    Returns:\n        Success rate, prediction rate or ROC plot figure object.\n\n    Raises:\n        InvalidParameterValueException: Invalid plot type.\n        InvalidParameterValueException: x_values or y_values are out of bounds.\n    \"\"\"\n    if plot_type == \"success_rate\":\n        label = \"Success rate\"\n        xlab = \"Proportion of area\"\n    elif plot_type == \"prediction_rate\":\n        label = \"Prediction rate\"\n        xlab = \"Proportion of area\"\n    elif plot_type == \"roc\":\n        label = \"ROC\"\n        xlab = \"False positive rate\"\n    else:\n        raise InvalidParameterValueException(\"Invalid plot type\")\n\n    if x_values.max() &gt; 1 or x_values.min() &lt; 0:\n        raise InvalidParameterValueException(\"x_values should be within range 0-1\")\n\n    if y_values.max() &gt; 1 or y_values.min() &lt; 0:\n        raise InvalidParameterValueException(\"y_values should be within range 0-1\")\n\n    fig = _plot_rate_curve(x_values=x_values, y_values=y_values, label=label, xlab=xlab)\n\n    return fig\n</code></pre>"},{"location":"vector_processing/idw_interpolation/","title":"IDW","text":""},{"location":"vector_processing/idw_interpolation/#eis_toolkit.vector_processing.idw_interpolation.idw","title":"<code>idw(geodataframe, target_column, resolution, extent=None, power=2)</code>","text":"<p>Calculate inverse distance weighted (IDW) interpolation.</p> <p>Parameters:</p> Name Type Description Default <code>geodataframe</code> <code>GeoDataFrame</code> <p>The vector dataframe to be interpolated.</p> required <code>target_column</code> <code>str</code> <p>The column name with values for each geometry.</p> required <code>resolution</code> <code>Tuple[Number, Number]</code> <p>The resolution i.e. cell size of the output raster as (pixel_size_x, pixel_size_y).</p> required <code>extent</code> <code>Optional[Tuple[Number, Number, Number, Number]]</code> <p>The extent of the output raster as (x_min, x_max, y_min, y_max). If None, calculate extent from the input vector data.</p> <code>None</code> <code>power</code> <code>Number</code> <p>The value for determining the rate at which the weights decrease. As power increases, the weights for distant points decrease rapidly. Defaults to 2.</p> <code>2</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, dict]</code> <p>Rasterized vector data and metadata.</p> <p>Raises:</p> Type Description <code>EmptyDataFrameException</code> <p>The input GeoDataFrame is empty.</p> <code>InvalidParameterValueException</code> <p>Invalid resolution or target_column.</p> Source code in <code>eis_toolkit/vector_processing/idw_interpolation.py</code> <pre><code>@beartype\ndef idw(\n    geodataframe: gpd.GeoDataFrame,\n    target_column: str,\n    resolution: Tuple[Number, Number],\n    extent: Optional[Tuple[Number, Number, Number, Number]] = None,\n    power: Number = 2,\n) -&gt; Tuple[np.ndarray, dict]:\n    \"\"\"Calculate inverse distance weighted (IDW) interpolation.\n\n    Args:\n        geodataframe: The vector dataframe to be interpolated.\n        target_column: The column name with values for each geometry.\n        resolution: The resolution i.e. cell size of the output raster as (pixel_size_x, pixel_size_y).\n        extent: The extent of the output raster as (x_min, x_max, y_min, y_max).\n            If None, calculate extent from the input vector data.\n        power: The value for determining the rate at which the weights decrease.\n            As power increases, the weights for distant points decrease rapidly.\n            Defaults to 2.\n\n    Returns:\n        Rasterized vector data and metadata.\n\n    Raises:\n        EmptyDataFrameException: The input GeoDataFrame is empty.\n        InvalidParameterValueException: Invalid resolution or target_column.\n    \"\"\"\n\n    if geodataframe.shape[0] == 0:\n        raise EmptyDataFrameException(\"Expected geodataframe to contain geometries.\")\n\n    if target_column not in geodataframe.columns:\n        raise InvalidParameterValueException(\n            f\"Expected target_column ({target_column}) to be contained in geodataframe columns.\"\n        )\n\n    if resolution[0] &lt;= 0 or resolution[1] &lt;= 0:\n        raise InvalidParameterValueException(\"Expected height and width greater than zero.\")\n\n    interpolated_values, out_meta = _idw_interpolation(geodataframe, target_column, resolution, power, extent)\n\n    return interpolated_values, out_meta\n</code></pre>"},{"location":"vector_processing/kriging_interpolation/","title":"Kriging interpolation","text":""},{"location":"vector_processing/kriging_interpolation/#eis_toolkit.vector_processing.kriging_interpolation.kriging","title":"<code>kriging(data, target_column, resolution, extent=None, variogram_model='linear', coordinates_type='geographic', method='ordinary')</code>","text":"<p>Perform Kriging interpolation on the input data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>GeoDataFrame</code> <p>GeoDataFrame containing the input data.</p> required <code>target_column</code> <code>str</code> <p>The column name with values for each geometry.</p> required <code>resolution</code> <code>Tuple[Number, Number]</code> <p>The resolution i.e. cell size of the output raster as (pixel_size_x, pixel_size_y).</p> required <code>extent</code> <code>Optional[Tuple[Number, Number, Number, Number]]</code> <p>The extent of the output raster as (x_min, x_max, y_min, y_max). If None, calculate extent from the input vector data.</p> <code>None</code> <code>variogram_model</code> <code>Literal[linear, power, gaussian, spherical, exponential]</code> <p>Variogram model to be used. Either 'linear', 'power', 'gaussian', 'spherical' or 'exponential'. Defaults to 'linear'.</p> <code>'linear'</code> <code>coordinates_type</code> <code>Literal[euclidean, geographic]</code> <p>Determines are coordinates on a plane ('euclidean') or a sphere ('geographic'). Used only in ordinary kriging. Defaults to 'geographic'.</p> <code>'geographic'</code> <code>method</code> <code>Literal[ordinary, universal]</code> <p>Ordinary or universal kriging. Defaults to 'ordinary'.</p> <code>'ordinary'</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, dict]</code> <p>Grid containing the interpolated values and metadata.</p> <p>Raises:</p> Type Description <code>EmptyDataFrameException</code> <p>The input GeoDataFrame is empty.</p> <code>InvalidParameterValueException</code> <p>Target column name is invalid or resolution is not greater than zero.</p> Source code in <code>eis_toolkit/vector_processing/kriging_interpolation.py</code> <pre><code>@beartype\ndef kriging(\n    data: gpd.GeoDataFrame,\n    target_column: str,\n    resolution: Tuple[Number, Number],\n    extent: Optional[Tuple[Number, Number, Number, Number]] = None,\n    variogram_model: Literal[\"linear\", \"power\", \"gaussian\", \"spherical\", \"exponential\"] = \"linear\",\n    coordinates_type: Literal[\"euclidean\", \"geographic\"] = \"geographic\",\n    method: Literal[\"ordinary\", \"universal\"] = \"ordinary\",\n) -&gt; Tuple[np.ndarray, dict]:\n    \"\"\"\n    Perform Kriging interpolation on the input data.\n\n    Args:\n        data: GeoDataFrame containing the input data.\n        target_column: The column name with values for each geometry.\n        resolution: The resolution i.e. cell size of the output raster as (pixel_size_x, pixel_size_y).\n        extent: The extent of the output raster as (x_min, x_max, y_min, y_max).\n            If None, calculate extent from the input vector data.\n        variogram_model: Variogram model to be used.\n            Either 'linear', 'power', 'gaussian', 'spherical' or 'exponential'. Defaults to 'linear'.\n        coordinates_type: Determines are coordinates on a plane ('euclidean') or a sphere ('geographic').\n            Used only in ordinary kriging. Defaults to 'geographic'.\n        method: Ordinary or universal kriging. Defaults to 'ordinary'.\n\n    Returns:\n        Grid containing the interpolated values and metadata.\n\n    Raises:\n        EmptyDataFrameException: The input GeoDataFrame is empty.\n        InvalidParameterValueException: Target column name is invalid or resolution is not greater than zero.\n    \"\"\"\n\n    if data.empty:\n        raise EmptyDataFrameException(\"The input GeoDataFrame is empty.\")\n\n    if target_column not in data.columns:\n        raise InvalidParameterValueException(\n            f\"Expected target_column ({target_column}) to be contained in geodataframe columns.\"\n        )\n\n    if resolution[0] &lt;= 0 or resolution[1] &lt;= 0:\n        raise InvalidParameterValueException(\"The resolution must be greater than zero.\")\n\n    data_interpolated, out_meta = _kriging(\n        data, target_column, resolution, extent, variogram_model, coordinates_type, method\n    )\n\n    return data_interpolated, out_meta\n</code></pre>"},{"location":"vector_processing/rasterize_vector/","title":"Rasterize vector","text":""},{"location":"vector_processing/rasterize_vector/#eis_toolkit.vector_processing.rasterize_vector.rasterize_vector","title":"<code>rasterize_vector(geodataframe, resolution=None, value_column=None, default_value=1.0, fill_value=0.0, base_raster_profile=None, buffer_value=None, merge_strategy='replace')</code>","text":"<p>Transform vector data into raster data.</p> <p>Parameters:</p> Name Type Description Default <code>geodataframe</code> <code>GeoDataFrame</code> <p>The vector dataframe to be rasterized.</p> required <code>resolution</code> <code>Optional[float]</code> <p>The resolution i.e. cell size of the output raster. Optional if base_raster_profile is given.</p> <code>None</code> <code>value_column</code> <code>Optional[str]</code> <p>The column name with values for each geometry. If None, then default_value is used for all geometries.</p> <code>None</code> <code>default_value</code> <code>float</code> <p>Default value burned into raster cells based on geometries.</p> <code>1.0</code> <code>base_raster_profile</code> <code>Optional[Union[Profile, dict]]</code> <p>Base raster profile to be used for determining the grid on which vectors are burned in. If None, the geometries and provided resolution value are used to compute grid.</p> <code>None</code> <code>fill_value</code> <code>float</code> <p>Value used outside the burned/rasterized geometry cells.</p> <code>0.0</code> <code>buffer_value</code> <code>Optional[float]</code> <p>For adding a buffer around passed geometries before rasterization.</p> <code>None</code> <code>merge_strategy</code> <code>Literal[replace, add]</code> <p>How to handle overlapping geometries. \"add\" causes overlapping geometries to add together the values while \"replace\" does not. Adding them together is the basis for density computations where the density can be calculated by using a default value of 1.0 and the sum in each cell is the count of intersecting geometries.</p> <code>'replace'</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, dict]</code> <p>Rasterized vector data and metadata.</p> Source code in <code>eis_toolkit/vector_processing/rasterize_vector.py</code> <pre><code>@beartype\ndef rasterize_vector(\n    geodataframe: gpd.GeoDataFrame,\n    resolution: Optional[float] = None,\n    value_column: Optional[str] = None,\n    default_value: float = 1.0,\n    fill_value: float = 0.0,\n    base_raster_profile: Optional[Union[profiles.Profile, dict]] = None,\n    buffer_value: Optional[float] = None,\n    merge_strategy: Literal[\"replace\", \"add\"] = \"replace\",\n) -&gt; Tuple[np.ndarray, dict]:\n    \"\"\"Transform vector data into raster data.\n\n    Args:\n        geodataframe: The vector dataframe to be rasterized.\n        resolution: The resolution i.e. cell size of the output raster.\n            Optional if base_raster_profile is given.\n        value_column: The column name with values for each geometry.\n            If None, then default_value is used for all geometries.\n        default_value: Default value burned into raster cells based on geometries.\n        base_raster_profile: Base raster profile\n            to be used for determining the grid on which vectors are\n            burned in. If None, the geometries and provided resolution\n            value are used to compute grid.\n        fill_value: Value used outside the burned/rasterized geometry cells.\n        buffer_value: For adding a buffer around passed\n            geometries before rasterization.\n        merge_strategy: How to handle overlapping geometries.\n            \"add\" causes overlapping geometries to add together the\n            values while \"replace\" does not. Adding them together is the\n            basis for density computations where the density can be\n            calculated by using a default value of 1.0 and the sum in\n            each cell is the count of intersecting geometries.\n\n    Returns:\n        Rasterized vector data and metadata.\n    \"\"\"\n\n    if geodataframe.shape[0] == 0:\n        # Empty GeoDataFrame\n        raise exceptions.EmptyDataFrameException(\"Expected geodataframe to contain geometries.\")\n\n    if resolution is None and base_raster_profile is None:\n        raise exceptions.InvalidParameterValueException(\n            \"Expected either resolution or base_raster_profile to be given.\"\n        )\n    if resolution is not None and resolution &lt;= 0:\n        raise exceptions.NumericValueSignException(\n            f\"Expected a positive value resolution ({dict(resolution=resolution)})\"\n        )\n    if value_column is not None and value_column not in geodataframe.columns:\n        raise exceptions.InvalidParameterValueException(\n            f\"Expected value_column ({value_column}) to be contained in geodataframe columns.\"\n        )\n    if buffer_value is not None and buffer_value &lt; 0:\n        raise exceptions.NumericValueSignException(\n            f\"Expected a positive buffer_value ({dict(buffer_value=buffer_value)})\"\n        )\n\n    if base_raster_profile is not None and not isinstance(base_raster_profile, (profiles.Profile, dict)):\n        raise exceptions.InvalidParameterValueException(\n            f\"Expected base_raster_profile ({type(base_raster_profile)}) to be dict or rasterio.profiles.Profile.\"\n        )\n\n    if buffer_value is not None:\n        geodataframe = geodataframe.copy()\n        geodataframe[\"geometry\"] = geodataframe[\"geometry\"].apply(lambda geom: geom.buffer(buffer_value))\n\n    return _rasterize_vector(\n        geodataframe=geodataframe,\n        value_column=value_column,\n        default_value=default_value,\n        fill_value=fill_value,\n        base_raster_profile=base_raster_profile,\n        resolution=resolution,\n        merge_alg=getattr(MergeAlg, merge_strategy),\n    )\n</code></pre>"},{"location":"vector_processing/reproject_vector/","title":"Reproject vector","text":""},{"location":"vector_processing/reproject_vector/#eis_toolkit.vector_processing.reproject_vector.reproject_vector","title":"<code>reproject_vector(geodataframe, target_crs)</code>","text":"<p>Reprojects vector data to match given coordinate reference system (EPSG).</p> <p>Parameters:</p> Name Type Description Default <code>geodataframe</code> <code>GeoDataFrame</code> <p>The vector dataframe to be reprojected.</p> required <code>target_crs</code> <code>int</code> <p>Target CRS as an EPSG code.</p> required <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>Reprojected vector data.</p> Source code in <code>eis_toolkit/vector_processing/reproject_vector.py</code> <pre><code>@beartype\ndef reproject_vector(geodataframe: geopandas.GeoDataFrame, target_crs: int) -&gt; geopandas.GeoDataFrame:\n    \"\"\"Reprojects vector data to match given coordinate reference system (EPSG).\n\n    Args:\n        geodataframe: The vector dataframe to be reprojected.\n        target_crs: Target CRS as an EPSG code.\n\n    Returns:\n        Reprojected vector data.\n    \"\"\"\n\n    if geodataframe.crs.to_epsg() == target_crs:\n        raise MatchingCrsException(\"Vector data is already in the target CRS.\")\n\n    reprojected_gdf = geodataframe.to_crs(\"epsg:\" + str(target_crs))\n    return reprojected_gdf\n</code></pre>"},{"location":"vector_processing/vector_density/","title":"Vector density","text":""},{"location":"vector_processing/vector_density/#eis_toolkit.vector_processing.vector_density.vector_density","title":"<code>vector_density(geodataframe, resolution=None, base_raster_profile=None, buffer_value=None, statistic='density')</code>","text":"<p>Compute density of geometries within raster.</p> <p>Parameters:</p> Name Type Description Default <code>geodataframe</code> <code>GeoDataFrame</code> <p>The dataframe with vectors of which density is computed.</p> required <code>resolution</code> <code>Optional[float]</code> <p>The resolution i.e. cell size of the output raster. Optional if base_raster_profile is given.</p> <code>None</code> <code>base_raster_profile</code> <code>Optional[Union[Profile, dict]]</code> <p>Base raster profile to be used for determining the grid on which vectors are burned in. If None, the geometries and provided resolution value are used to compute grid.</p> <code>None</code> <code>buffer_value</code> <code>Optional[float]</code> <p>For adding a buffer around passed geometries before computing density.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, dict]</code> <p>Computed density of vector data and metadata.</p> Source code in <code>eis_toolkit/vector_processing/vector_density.py</code> <pre><code>@beartype\ndef vector_density(\n    geodataframe: gpd.GeoDataFrame,\n    resolution: Optional[float] = None,\n    base_raster_profile: Optional[Union[profiles.Profile, dict]] = None,\n    buffer_value: Optional[float] = None,\n    statistic: Literal[\"density\", \"count\"] = \"density\",\n) -&gt; Tuple[np.ndarray, dict]:\n    \"\"\"Compute density of geometries within raster.\n\n    Args:\n        geodataframe: The dataframe with vectors\n            of which density is computed.\n        resolution: The resolution i.e. cell size of the output raster.\n            Optional if base_raster_profile is given.\n        base_raster_profile: Base raster profile\n            to be used for determining the grid on which vectors are\n            burned in. If None, the geometries and provided resolution\n            value are used to compute grid.\n        buffer_value: For adding a buffer around passed\n            geometries before computing density.\n\n    Returns:\n        Computed density of vector data and metadata.\n    \"\"\"\n    out_raster_array, out_metadata = rasterize_vector(\n        geodataframe=geodataframe,\n        resolution=resolution,\n        base_raster_profile=base_raster_profile,\n        buffer_value=buffer_value,\n        value_column=None,\n        default_value=1.0,\n        fill_value=0.0,\n        merge_strategy=\"add\",\n    )\n    max_count = np.max(out_raster_array)\n    if statistic == \"count\" or np.isclose(max_count, 0.0):\n        return out_raster_array, out_metadata\n    else:\n        return (out_raster_array / max_count), out_metadata\n</code></pre>"}]}