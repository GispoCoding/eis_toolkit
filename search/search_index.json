{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"General","text":"<p>This is the documentation site of the eis_toolkit python package. Here you can find documentation for each module. The documentation is automatically generated from docstrings.</p> <p>Development of eis_toolkit is related to EIS Horizon EU project.</p>"},{"location":"dependency_licenses/","title":"Dependency licenses","text":"Name Version License protobuf 4.24.4 3-Clause BSD License absl-py 2.0.0 Apache Software License flatbuffers 23.5.26 Apache Software License google-auth 2.23.3 Apache Software License google-auth-oauthlib 1.0.0 Apache Software License google-pasta 0.2.0 Apache Software License grpcio 1.59.0 Apache Software License keras 2.14.0 Apache Software License libclang 16.0.6 Apache Software License ml-dtypes 0.2.0 Apache Software License requests 2.31.0 Apache Software License rsa 4.9 Apache Software License tensorboard 2.14.1 Apache Software License tensorboard-data-server 0.7.2 Apache Software License tensorflow 2.14.0 Apache Software License tensorflow-estimator 2.14.0 Apache Software License tensorflow-io-gcs-filesystem 0.34.0 Apache Software License tzdata 2023.3 Apache Software License packaging 23.2 Apache Software License; BSD License python-dateutil 2.8.2 Apache Software License; BSD License cligj 0.7.2 BSD geopandas 0.11.1 BSD Markdown 3.5 BSD License MarkupSafe 2.1.3 BSD License PyKrige 1.7.1 BSD License Pygments 2.16.1 BSD License Shapely 1.8.5.post1 BSD License Werkzeug 3.0.0 BSD License affine 2.4.0 BSD License astunparse 1.6.3 BSD License click 8.1.7 BSD License click-plugins 1.1.1 BSD License colorama 0.4.6 BSD License contourpy 1.1.1 BSD License cycler 0.12.1 BSD License fiona 1.9.5 BSD License gast 0.5.4 BSD License h5py 3.10.0 BSD License idna 3.4 BSD License joblib 1.3.2 BSD License kiwisolver 1.4.5 BSD License numpy 1.26.1 BSD License oauthlib 3.2.2 BSD License pandas 2.1.1 BSD License patsy 0.5.3 BSD License pyasn1 0.5.0 BSD License pyasn1-modules 0.3.0 BSD License rasterio 1.3.9 BSD License requests-oauthlib 1.3.1 BSD License scikit-learn 1.3.2 BSD License scipy 1.11.3 BSD License seaborn 0.13.0 BSD License statsmodels 0.14.0 BSD License threadpoolctl 3.2.0 BSD License wrapt 1.14.1 BSD License eis-toolkit 0.1.0 European Union Public Licence 1.2 (EUPL 1.2) Pillow 10.1.0 Historical Permission Notice and Disclaimer (HPND) shellingham 1.5.4 ISC License (ISCL) imbalanced-learn 0.11.0 MIT opt-einsum 3.3.0 MIT snuggs 1.4.7 MIT GDAL 3.4.3 MIT License Rtree 1.1.0 MIT License attrs 23.1.0 MIT License beartype 0.13.1 MIT License cachetools 5.3.1 MIT License charset-normalizer 3.3.1 MIT License fonttools 4.43.1 MIT License markdown-it-py 3.0.0 MIT License mdurl 0.1.2 MIT License pyparsing 3.1.1 MIT License pyproj 3.6.1 MIT License pytz 2023.3.post1 MIT License rich 13.6.0 MIT License setuptools-scm 8.0.4 MIT License six 1.16.0 MIT License termcolor 2.3.0 MIT License tomli 2.0.1 MIT License typer 0.9.0 MIT License urllib3 2.0.7 MIT License certifi 2023.7.22 Mozilla Public License 2.0 (MPL 2.0) matplotlib 3.8.0 Python Software Foundation License typing_extensions 4.8.0 Python Software Foundation License"},{"location":"conversions/csv_to_geodataframe/","title":"Convert csv to geodataframe","text":""},{"location":"conversions/csv_to_geodataframe/#eis_toolkit.conversions.csv_to_geodataframe.csv_to_geodataframe","title":"<code>csv_to_geodataframe(csv, indexes, target_crs)</code>","text":"<p>Read CSV file to a GeoDataFrame.</p> <p>Usage of single index expects valid WKT geometry. Usage of two indexes expects POINT feature(s) X-coordinate as the first index and Y-coordinate as the second index.</p> <p>Parameters:</p> Name Type Description Default <code>csv</code> <code>Path</code> <p>Path to the .csv file to be read.</p> required <code>indexes</code> <code>Sequence[int]</code> <p>Index(es) of the geometry column(s).</p> required <code>target_crs</code> <code>int</code> <p>Target CRS as an EPSG code.</p> required <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>CSV file read to a GeoDataFrame.</p> <p>Raises:</p> Type Description <code>InvalidColumnIndexException</code> <p>There is a mismatch between the provided indexes and the shape of the dataframe read from the csv.</p> <code>InvalidParameterValueException</code> <p>Unable to create a GeoDataFrame with point features from the given input parameters.</p> <code>InvalidWktFormatException</code> <p>Unable to create a GeoDataFrame of WKT geometry from the given input parameters.</p> Source code in <code>eis_toolkit/conversions/csv_to_geodataframe.py</code> <pre><code>@beartype\ndef csv_to_geodataframe(\n    csv: Path,\n    indexes: Sequence[int],\n    target_crs: int,\n) -&gt; geopandas.GeoDataFrame:\n    \"\"\"\n    Read CSV file to a GeoDataFrame.\n\n    Usage of single index expects valid WKT geometry.\n    Usage of two indexes expects POINT feature(s) X-coordinate as the first index and Y-coordinate as the second index.\n\n    Args:\n        csv: Path to the .csv file to be read.\n        indexes: Index(es) of the geometry column(s).\n        target_crs: Target CRS as an EPSG code.\n\n    Returns:\n        CSV file read to a GeoDataFrame.\n\n    Raises:\n        InvalidColumnIndexException: There is a mismatch between the provided indexes and the shape of\n            the dataframe read from the csv.\n        InvalidParameterValueException: Unable to create a GeoDataFrame with point features from the given input\n            parameters.\n        InvalidWktFormatException: Unable to create a GeoDataFrame of WKT geometry from the given input parameters.\n    \"\"\"\n\n    data_frame = _csv_to_geodataframe(\n        csv=csv,\n        indexes=indexes,\n        target_crs=target_crs,\n    )\n    return data_frame\n</code></pre>"},{"location":"conversions/raster_to_dataframe/","title":"Convert raster to dataframe","text":""},{"location":"conversions/raster_to_dataframe/#eis_toolkit.conversions.raster_to_dataframe.raster_to_dataframe","title":"<code>raster_to_dataframe(raster, bands=None, add_coordinates=False)</code>","text":"<p>Convert raster to Pandas DataFrame.</p> <p>If bands are not given, all bands are used for conversion. Selected bands are named based on their index e.g., band_1, band_2,...,band_n. If wanted, image coordinates (row, col) for each pixel can be written to dataframe by setting add_coordinates to True.</p> <p>Parameters:</p> Name Type Description Default <code>raster</code> <code>DatasetReader</code> <p>Raster to be converted.</p> required <code>bands</code> <code>Optional[Sequence[int]]</code> <p>Selected bands from multiband raster. Indexing begins from one. Defaults to None.</p> <code>None</code> <code>add_coordinates</code> <code>bool</code> <p>Determines if pixel coordinates are written into dataframe. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Raster converted to a DataFrame.</p> Source code in <code>eis_toolkit/conversions/raster_to_dataframe.py</code> <pre><code>@beartype\ndef raster_to_dataframe(\n    raster: rasterio.io.DatasetReader,\n    bands: Optional[Sequence[int]] = None,\n    add_coordinates: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"Convert raster to Pandas DataFrame.\n\n    If bands are not given, all bands are used for conversion. Selected bands are named based on their index e.g.,\n    band_1, band_2,...,band_n. If wanted, image coordinates (row, col) for each pixel can be written to\n    dataframe by setting add_coordinates to True.\n\n    Args:\n        raster: Raster to be converted.\n        bands: Selected bands from multiband raster. Indexing begins from one. Defaults to None.\n        add_coordinates: Determines if pixel coordinates are written into dataframe. Defaults to False.\n\n    Returns:\n        Raster converted to a DataFrame.\n    \"\"\"\n\n    data_frame = _raster_to_dataframe(\n        raster=raster,\n        bands=bands,\n        add_coordinates=add_coordinates,\n    )\n    return data_frame\n</code></pre>"},{"location":"exploratory_analyses/dbscan/","title":"DBSCAN","text":""},{"location":"exploratory_analyses/dbscan/#eis_toolkit.exploratory_analyses.dbscan.dbscan","title":"<code>dbscan(data, max_distance=0.5, min_samples=5)</code>","text":"<p>Perform DBSCAN clustering on the input data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>GeoDataFrame</code> <p>GeoDataFrame containing the input data.</p> required <code>max_distance</code> <code>float</code> <p>The maximum distance between two samples for one to be considered as in the neighborhood of the other. Defaults to 0.5.</p> <code>0.5</code> <code>min_samples</code> <code>int</code> <p>The number of samples in a neighborhood for a point to be considered as a core point. Defaults to 5.</p> <code>5</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>GeoDataFrame containing two new columns: one with assigned cluster labels and one indicating whether a point is a core point (1) or not (0).</p> <p>Raises:</p> Type Description <code>EmptyDataFrameException</code> <p>The input GeoDataFrame is empty.</p> <code>InvalidParameterException</code> <p>The maximum distance between two samples in a neighborhood is not greater than zero or the number of samples in a neighborhood is not greater than one.</p> Source code in <code>eis_toolkit/exploratory_analyses/dbscan.py</code> <pre><code>@beartype\ndef dbscan(data: gdp.GeoDataFrame, max_distance: float = 0.5, min_samples: int = 5) -&gt; gdp.GeoDataFrame:\n    \"\"\"\n    Perform DBSCAN clustering on the input data.\n\n    Args:\n        data: GeoDataFrame containing the input data.\n        max_distance: The maximum distance between two samples for one to be considered as in the neighborhood of\n            the other. Defaults to 0.5.\n        min_samples: The number of samples in a neighborhood for a point to be considered as a core point.\n            Defaults to 5.\n\n    Returns:\n        GeoDataFrame containing two new columns: one with assigned cluster labels and one indicating whether a\n            point is a core point (1) or not (0).\n\n    Raises:\n        EmptyDataFrameException: The input GeoDataFrame is empty.\n        InvalidParameterException: The maximum distance between two samples in a neighborhood is not greater\n            than zero or the number of samples in a neighborhood is not greater than one.\n    \"\"\"\n\n    if data.empty:\n        raise EmptyDataFrameException(\"The input GeoDataFrame is empty.\")\n\n    if max_distance &lt;= 0:\n        raise InvalidParameterValueException(\n            \"The input value for the maximum distance between two samples in a neighborhood must be greater than zero.\"\n        )\n\n    if min_samples &lt;= 1:\n        raise InvalidParameterValueException(\n            \"The input value for the minimum number of samples in a neighborhood must be greater than one.\"\n        )\n\n    dbscan_gdf = _dbscan(data, max_distance, min_samples)\n\n    return dbscan_gdf\n</code></pre>"},{"location":"exploratory_analyses/descriptive_statistics/","title":"Descriptive statistics","text":""},{"location":"exploratory_analyses/descriptive_statistics/#eis_toolkit.exploratory_analyses.descriptive_statistics.descriptive_statistics_dataframe","title":"<code>descriptive_statistics_dataframe(input_data, column)</code>","text":"<p>Generate descriptive statistics from vector data.</p> <p>Generates min, max, mean, quantiles(25%, 50% and 75%), standard deviation, relative standard deviation and skewness.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Union[DataFrame, GeoDataFrame]</code> <p>Data to generate descriptive statistics from.</p> required <code>column</code> <code>str</code> <p>Specify the column to generate descriptive statistics from.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>The descriptive statistics in previously described order.</p> Source code in <code>eis_toolkit/exploratory_analyses/descriptive_statistics.py</code> <pre><code>@beartype\ndef descriptive_statistics_dataframe(input_data: Union[pd.DataFrame, gpd.GeoDataFrame], column: str) -&gt; dict:\n    \"\"\"Generate descriptive statistics from vector data.\n\n    Generates min, max, mean, quantiles(25%, 50% and 75%), standard deviation, relative standard deviation and skewness.\n\n    Args:\n        input_data: Data to generate descriptive statistics from.\n        column: Specify the column to generate descriptive statistics from.\n\n    Returns:\n        The descriptive statistics in previously described order.\n    \"\"\"\n    if column not in input_data.columns:\n        raise InvalidColumnException\n    data = input_data[column]\n    statistics = _descriptive_statistics(data)\n    return statistics\n</code></pre>"},{"location":"exploratory_analyses/descriptive_statistics/#eis_toolkit.exploratory_analyses.descriptive_statistics.descriptive_statistics_raster","title":"<code>descriptive_statistics_raster(input_data)</code>","text":"<p>Generate descriptive statistics from raster data.</p> <p>Generates min, max, mean, quantiles(25%, 50% and 75%), standard deviation, relative standard deviation and skewness. Nodata values are removed from the data before the statistics are computed.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>DatasetReader</code> <p>Data to generate descriptive statistics from.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>The descriptive statistics in previously described order.</p> Source code in <code>eis_toolkit/exploratory_analyses/descriptive_statistics.py</code> <pre><code>@beartype\ndef descriptive_statistics_raster(input_data: rasterio.io.DatasetReader) -&gt; dict:\n    \"\"\"Generate descriptive statistics from raster data.\n\n    Generates min, max, mean, quantiles(25%, 50% and 75%), standard deviation, relative standard deviation and skewness.\n    Nodata values are removed from the data before the statistics are computed.\n\n    Args:\n        input_data: Data to generate descriptive statistics from.\n\n    Returns:\n        The descriptive statistics in previously described order.\n    \"\"\"\n    data = input_data.read().flatten()\n    nodata_value = input_data.nodata\n    data = data[data != nodata_value]\n    statistics = _descriptive_statistics(data)\n    return statistics\n</code></pre>"},{"location":"exploratory_analyses/feature_importance/","title":"Feature importance","text":""},{"location":"exploratory_analyses/feature_importance/#eis_toolkit.exploratory_analyses.feature_importance.evaluate_feature_importance","title":"<code>evaluate_feature_importance(classifier, x_test, y_test, feature_names, number_of_repetition=50, random_state=0)</code>","text":"<p>Evaluate the feature importance of a sklearn classifier or linear model.</p> <p>Parameters:</p> Name Type Description Default <code>classifier</code> <code>BaseEstimator</code> <p>Trained classifier.</p> required <code>x_test</code> <code>ndarray</code> <p>Testing feature data (X data need to be normalized / standardized).</p> required <code>y_test</code> <code>ndarray</code> <p>Testing target data.</p> required <code>feature_names</code> <code>Sequence[str]</code> <p>Names of the feature columns.</p> required <code>number_of_repetition</code> <code>int</code> <p>Number of iteration used when calculate feature importance (default 50).</p> <code>50</code> <code>random_state</code> <code>int</code> <p>random state for repeatability of results (Default 0).</p> <code>0</code> Return <p>A dataframe composed by features name and Importance value The resulted object with importance mean, importance std, and overall importance</p> <p>Raises:</p> Type Description <code>InvalidDatasetException</code> <p>When the dataset is None.</p> Source code in <code>eis_toolkit/exploratory_analyses/feature_importance.py</code> <pre><code>@beartype\ndef evaluate_feature_importance(\n    classifier: sklearn.base.BaseEstimator,\n    x_test: np.ndarray,\n    y_test: np.ndarray,\n    feature_names: Sequence[str],\n    number_of_repetition: int = 50,\n    random_state: int = 0,\n) -&gt; tuple[pd.DataFrame, dict]:\n    \"\"\"\n    Evaluate the feature importance of a sklearn classifier or linear model.\n\n    Parameters:\n        classifier: Trained classifier.\n        x_test: Testing feature data (X data need to be normalized / standardized).\n        y_test: Testing target data.\n        feature_names: Names of the feature columns.\n        number_of_repetition: Number of iteration used when calculate feature importance (default 50).\n        random_state: random state for repeatability of results (Default 0).\n\n    Return:\n        A dataframe composed by features name and Importance value\n        The resulted object with importance mean, importance std, and overall importance\n\n    Raises:\n        InvalidDatasetException: When the dataset is None.\n    \"\"\"\n\n    if x_test is None or y_test is None:\n        raise InvalidDatasetException\n\n    result = permutation_importance(\n        classifier, x_test, y_test.ravel(), n_repeats=number_of_repetition, random_state=random_state\n    )\n\n    feature_importance = pd.DataFrame({\"Feature\": feature_names, \"Importance\": result.importances_mean})\n\n    feature_importance[\"Importance\"] = feature_importance[\"Importance\"] * 100\n    feature_importance = feature_importance.sort_values(by=\"Importance\", ascending=False)\n\n    return feature_importance, result\n</code></pre>"},{"location":"exploratory_analyses/k_means_cluster/","title":"K-means clustering","text":""},{"location":"exploratory_analyses/k_means_cluster/#eis_toolkit.exploratory_analyses.k_means_cluster.k_means_clustering","title":"<code>k_means_clustering(data, number_of_clusters=None, random_state=None)</code>","text":"<p>Perform k-means clustering on the input data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>GeoDataFrame</code> <p>A GeoDataFrame containing the input data.</p> required <code>number_of_clusters</code> <code>Optional[int]</code> <p>The number of clusters (&gt;= 1) to form. Optional parameter. If not provided, optimal number of clusters is computed using the elbow method.</p> <code>None</code> <code>random_state</code> <code>Optional[int]</code> <p>A random number generation for centroid initialization to make the randomness deterministic. Optional parameter.</p> <code>None</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>GeoDataFrame containing assigned cluster labels.</p> <p>Raises:</p> Type Description <code>EmptyDataFrameException</code> <p>The input GeoDataFrame is empty.</p> <code>InvalidParameterException</code> <p>The number of clusters is less than one.</p> Source code in <code>eis_toolkit/exploratory_analyses/k_means_cluster.py</code> <pre><code>@beartype\ndef k_means_clustering(\n    data: gdp.GeoDataFrame, number_of_clusters: Optional[int] = None, random_state: Optional[int] = None\n) -&gt; gdp.GeoDataFrame:\n    \"\"\"\n    Perform k-means clustering on the input data.\n\n    Args:\n        data: A GeoDataFrame containing the input data.\n        number_of_clusters: The number of clusters (&gt;= 1) to form. Optional parameter. If not provided,\n            optimal number of clusters is computed using the elbow method.\n        random_state: A random number generation for centroid initialization to make\n            the randomness deterministic. Optional parameter.\n\n    Returns:\n        GeoDataFrame containing assigned cluster labels.\n\n    Raises:\n        EmptyDataFrameException: The input GeoDataFrame is empty.\n        InvalidParameterException: The number of clusters is less than one.\n    \"\"\"\n\n    if data.empty:\n        raise EmptyDataFrameException(\"The input GeoDataFrame is empty.\")\n\n    if number_of_clusters is not None and number_of_clusters &lt; 1:\n        raise InvalidParameterValueException(\"The input value for number of clusters must be at least one.\")\n\n    k_means_gdf = _k_means_clustering(data, number_of_clusters, random_state)\n\n    return k_means_gdf\n</code></pre>"},{"location":"exploratory_analyses/parallel_coordinates/","title":"Plot parallel coordinates","text":""},{"location":"exploratory_analyses/parallel_coordinates/#eis_toolkit.exploratory_analyses.parallel_coordinates.plot_parallel_coordinates","title":"<code>plot_parallel_coordinates(df, color_column_name, plot_title=None, palette_name=None, curved_lines=True)</code>","text":"<p>Plot a parallel coordinates plot.</p> <p>Automatically removes all rows containing null/nan values. Tries to convert columns to numeric to be able to plot them. If more than 8 columns are present (after numeric filtering), keeps only the first 8 to plot.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The DataFrame to plot.</p> required <code>color_column_name</code> <code>str</code> <p>The name of the column in df to use for color encoding.</p> required <code>plot_title</code> <code>Optional[str]</code> <p>The title for the plot. Default is None.</p> <code>None</code> <code>palette_name</code> <code>Optional[str]</code> <p>The name of the color palette to use. Default is None.</p> <code>None</code> <code>curved_lines</code> <code>bool</code> <p>If True, the plot will have curved instead of straight lines. Default is True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Figure</code> <p>A matplotlib figure containing the parallel coordinates plot.</p> <p>Raises:</p> Type Description <code>EmptyDataFrameException</code> <p>Raised when the DataFrame is empty.</p> <code>InvalidColumnException</code> <p>Raised when the color column is not found in the DataFrame.</p> <code>InconsistentDataTypesException</code> <p>Raised when the color column has multiple data types.</p> Source code in <code>eis_toolkit/exploratory_analyses/parallel_coordinates.py</code> <pre><code>@beartype\ndef plot_parallel_coordinates(\n    df: pd.DataFrame,\n    color_column_name: str,\n    plot_title: Optional[str] = None,\n    palette_name: Optional[str] = None,\n    curved_lines: bool = True,\n) -&gt; matplotlib.figure.Figure:\n    \"\"\"Plot a parallel coordinates plot.\n\n    Automatically removes all rows containing null/nan values. Tries to convert columns to numeric\n    to be able to plot them. If more than 8 columns are present (after numeric filtering), keeps only\n    the first 8 to plot.\n\n    Args:\n        df: The DataFrame to plot.\n        color_column_name: The name of the column in df to use for color encoding.\n        plot_title: The title for the plot. Default is None.\n        palette_name: The name of the color palette to use. Default is None.\n        curved_lines: If True, the plot will have curved instead of straight lines. Default is True.\n\n    Returns:\n        A matplotlib figure containing the parallel coordinates plot.\n\n    Raises:\n        EmptyDataFrameException: Raised when the DataFrame is empty.\n        InvalidColumnException: Raised when the color column is not found in the DataFrame.\n        InconsistentDataTypesException: Raised when the color column has multiple data types.\n    \"\"\"\n\n    if df.empty:\n        raise EmptyDataFrameException(\"The input DataFrame is empty.\")\n\n    if color_column_name not in df.columns:\n        raise InvalidColumnException(f\"The provided color column {color_column_name} is not found in the DataFrame.\")\n\n    df = df.convert_dtypes()\n    df = df.apply(pd.to_numeric, errors=\"ignore\")\n\n    color_data = df[color_column_name].to_numpy()\n    if len(set([type(elem) for elem in color_data])) != 1:\n        raise InconsistentDataTypesException(\n            \"The color column should have a consistent datatype. Multiple data types detected in the color column.\"\n        )\n\n    df = df.select_dtypes(include=np.number)\n\n    # Drop non-numeric columns and the column used for coloring\n    columns_to_drop = [color_column_name]\n    for column in df.columns.values:\n        if df[column].isnull().all():\n            columns_to_drop.append(column)\n    df = df.loc[:, ~df.columns.isin(columns_to_drop)]\n\n    # Keep only first 8 columns if more are still present\n    if len(df.columns.values) &gt; 8:\n        df = df.iloc[:, :8]\n\n    data_labels = df.columns.values\n    data = df.to_numpy()\n\n    fig = _plot_parallel_coordinates(\n        data=data,\n        data_labels=data_labels,\n        color_data=color_data,\n        color_column_name=color_column_name,\n        plot_title=plot_title,\n        palette_name=palette_name,\n        curved_lines=curved_lines,\n    )\n    return fig\n</code></pre>"},{"location":"exploratory_analyses/pca/","title":"PCA","text":""},{"location":"exploratory_analyses/pca/#eis_toolkit.exploratory_analyses.pca.compute_pca","title":"<code>compute_pca(data, number_of_components, columns=None, scaler_type='standard', nodata_handling='remove', nodata=None)</code>","text":"<p>Compute defined number of principal components for numeric input data.</p> <p>Before computation, data is scaled according to specified scaler and NaN values removed or replaced. Optionally, a nodata value can be given to handle similarly as NaN values.</p> <p>If input data is a Numpy array, interpretation of the data depends on its dimensions. If array is 3D, it is interpreted as a multiband raster/stacked rasters format (bands, rows, columns). If array is 2D, it is interpreted as table-like data, where each column represents a variable/raster band and each row a data point (similar to a Dataframe).</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[ndarray, DataFrame, GeoDataFrame]</code> <p>Input data for PCA.</p> required <code>number_of_components</code> <code>int</code> <p>The number of principal components to compute. Should be &gt;= 1 and at most the number of numeric columns if input is (Geo)Dataframe.</p> required <code>columns</code> <code>Optional[Sequence[str]]</code> <p>Select columns used for the PCA. Other columns are excluded from PCA, but added back to the result Dataframe intact. Only relevant if input is (Geo)Dataframe. Defaults to None.</p> <code>None</code> <code>scaler_type</code> <code>Literal[standard, min_max, robust]</code> <p>Transform data according to a specified Sklearn scaler. Options are \"standard\", \"min_max\" and \"robust\". Defaults to \"standard\".</p> <code>'standard'</code> <code>nodata_handling</code> <code>Literal[remove, replace]</code> <p>If observations with nodata (NaN and given <code>nodata</code>) should be removed for the time of PCA computation or replaced with column/band mean. Defaults to \"remove\".</p> <code>'remove'</code> <code>nodata</code> <code>Optional[Number]</code> <p>Define a nodata value to remove. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[ndarray, DataFrame, GeoDataFrame]</code> <p>The computed principal components in corresponding format as the input data and the</p> <code>ndarray</code> <p>explained variance ratios for each component.</p> <p>Raises:</p> Type Description <code>EmptyDataException</code> <p>The input is empty.</p> <code>InvalidColumnException</code> <p>Selected columns are not found in the input Dataframe.</p> <code>InvalidNumberOfPrincipalComponents</code> <p>The number of principal components is less than 1 or more than number of columns if input was (Geo)DataFrame.</p> <code>InvalidParameterValueException</code> <p>If value for <code>number_of_components</code> is invalid.</p> Source code in <code>eis_toolkit/exploratory_analyses/pca.py</code> <pre><code>@beartype\ndef compute_pca(\n    data: Union[np.ndarray, pd.DataFrame, gpd.GeoDataFrame],\n    number_of_components: int,\n    columns: Optional[Sequence[str]] = None,\n    scaler_type: Literal[\"standard\", \"min_max\", \"robust\"] = \"standard\",\n    nodata_handling: Literal[\"remove\", \"replace\"] = \"remove\",\n    nodata: Optional[Number] = None,\n) -&gt; Tuple[Union[np.ndarray, pd.DataFrame, gpd.GeoDataFrame], np.ndarray]:\n    \"\"\"\n    Compute defined number of principal components for numeric input data.\n\n    Before computation, data is scaled according to specified scaler and NaN values removed or replaced.\n    Optionally, a nodata value can be given to handle similarly as NaN values.\n\n    If input data is a Numpy array, interpretation of the data depends on its dimensions.\n    If array is 3D, it is interpreted as a multiband raster/stacked rasters format (bands, rows, columns).\n    If array is 2D, it is interpreted as table-like data, where each column represents a variable/raster band\n    and each row a data point (similar to a Dataframe).\n\n    Args:\n        data: Input data for PCA.\n        number_of_components: The number of principal components to compute. Should be &gt;= 1 and at most\n            the number of numeric columns if input is (Geo)Dataframe.\n        columns: Select columns used for the PCA. Other columns are excluded from PCA, but added back\n            to the result Dataframe intact. Only relevant if input is (Geo)Dataframe. Defaults to None.\n        scaler_type: Transform data according to a specified Sklearn scaler.\n            Options are \"standard\", \"min_max\" and \"robust\". Defaults to \"standard\".\n        nodata_handling: If observations with nodata (NaN and given `nodata`) should be removed for the time\n            of PCA computation or replaced with column/band mean. Defaults to \"remove\".\n        nodata: Define a nodata value to remove. Defaults to None.\n\n    Returns:\n        The computed principal components in corresponding format as the input data and the\n        explained variance ratios for each component.\n\n    Raises:\n        EmptyDataException: The input is empty.\n        InvalidColumnException: Selected columns are not found in the input Dataframe.\n        InvalidNumberOfPrincipalComponents: The number of principal components is less than 1 or more than\n            number of columns if input was (Geo)DataFrame.\n        InvalidParameterValueException: If value for `number_of_components` is invalid.\n    \"\"\"\n    if scaler_type not in SCALERS:\n        raise InvalidParameterValueException(f\"Invalid scaler. Choose from: {list(SCALERS.keys())}\")\n\n    if number_of_components &lt; 1:\n        raise InvalidParameterValueException(\"The number of principal components should be &gt;= 1.\")\n\n    # Get feature matrix (Numpy array) from various input types\n    if isinstance(data, np.ndarray):\n        feature_matrix = data\n        feature_matrix = feature_matrix.astype(float)\n        if feature_matrix.ndim == 2:  # Table-like data (assumme it is a DataFrame transformed to Numpy array)\n            feature_matrix, nan_mask = _prepare_array_data(\n                feature_matrix, nodata_handling=nodata_handling, nodata_value=nodata, reshape=False\n            )\n        elif feature_matrix.ndim == 3:  # Assume data represents multiband raster data\n            rows, cols = feature_matrix.shape[1], feature_matrix.shape[2]\n            feature_matrix, nan_mask = _prepare_array_data(\n                feature_matrix, nodata_handling=nodata_handling, nodata_value=nodata, reshape=True\n            )\n        else:\n            raise InvalidParameterValueException(\n                f\"Unsupported input data format. {feature_matrix.ndim} dimensions detected for given array.\"\n            )\n\n    elif isinstance(data, pd.DataFrame):\n        df = data.copy()\n        if df.empty:\n            raise EmptyDataException(\"Input DataFrame is empty.\")\n        if isinstance(data, gpd.GeoDataFrame):\n            geometries = data.geometry\n            crs = data.crs\n            df = df.drop(columns=[\"geometry\"])\n        if columns is not None and columns != []:\n            if not check_columns_valid(df, columns):\n                raise InvalidColumnException(\"All selected columns were not found in the input DataFrame.\")\n            df = df[columns]\n\n        df = df.convert_dtypes()\n        df = df.apply(pd.to_numeric, errors=\"ignore\")\n        df = df.select_dtypes(include=np.number)\n        df = df.astype(dtype=np.number)\n        feature_matrix = df.to_numpy()\n        feature_matrix = feature_matrix.astype(float)\n        feature_matrix, nan_mask = _handle_missing_values(feature_matrix, nodata_handling, nodata)\n\n    if number_of_components &gt; feature_matrix.shape[1]:\n        raise InvalidParameterValueException(\"The number of principal components is too high for the given input data.\")\n    # Core PCA computation\n    principal_components, explained_variances = _compute_pca(feature_matrix, number_of_components, scaler_type)\n\n    if nodata_handling == \"remove\" and nan_mask is not None:\n        principal_components_with_nans = np.full((nan_mask.size, principal_components.shape[1]), np.nan)\n        principal_components_with_nans[~nan_mask, :] = principal_components\n        principal_components = principal_components_with_nans\n\n    # Convert PCA output to proper format\n    if isinstance(data, np.ndarray):\n        if data.ndim == 3:\n            result_data = principal_components.reshape(rows, cols, -1).transpose(2, 0, 1)\n        else:\n            result_data = principal_components\n\n    elif isinstance(data, pd.DataFrame):\n        component_names = [f\"principal_component_{i+1}\" for i in range(number_of_components)]\n        result_data = pd.DataFrame(data=principal_components, columns=component_names)\n        if columns is not None:\n            old_columns = [column for column in data.columns if column not in columns]\n            for column in old_columns:\n                result_data[column] = data[column]\n        if isinstance(data, gpd.GeoDataFrame):\n            result_data = gpd.GeoDataFrame(result_data, geometry=geometries, crs=crs)\n\n    return result_data, explained_variances\n</code></pre>"},{"location":"exploratory_analyses/pca/#eis_toolkit.exploratory_analyses.pca.plot_pca","title":"<code>plot_pca(pca_df, explained_variances=None, color_column_name=None, save_path=None)</code>","text":"<p>Plot a scatter matrix of different principal component combinations.</p> <p>Automatically filters columns that do not start with \"principal_component\" for plotting. This tool is designed to work smoothly on <code>compute_pca</code> outputs.</p> <p>Parameters:</p> Name Type Description Default <code>pca_df</code> <code>DataFrame</code> <p>A DataFrame containing computed principal components.</p> required <code>explained_variances</code> <code>Optional[ndarray]</code> <p>The explained variance ratios for each principal component. Used for labeling axes in the plot. Optional parameter. Defaults to None.</p> <code>None</code> <code>color_column_name</code> <code>Optional[str]</code> <p>Name of the column that will be used for color-coding data points. Typically a categorical variable in the original data. Optional parameter, no colors if not provided. Defaults to None.</p> <code>None</code> <code>save_path</code> <code>Optional[str]</code> <p>The save path for the plot. Optional parameter, no saving if not provided. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>PairGrid</code> <p>A Seaborn pairgrid containing the PCA scatter matrix.</p> <p>Raises:</p> Type Description <code>InvalidColumnException</code> <p>DataFrame does not contain the given color column.</p> Source code in <code>eis_toolkit/exploratory_analyses/pca.py</code> <pre><code>@beartype\ndef plot_pca(\n    pca_df: pd.DataFrame,\n    explained_variances: Optional[np.ndarray] = None,\n    color_column_name: Optional[str] = None,\n    save_path: Optional[str] = None,\n) -&gt; sns.PairGrid:\n    \"\"\"\n    Plot a scatter matrix of different principal component combinations.\n\n    Automatically filters columns that do not start with \"principal_component\" for plotting.\n    This tool is designed to work smoothly on `compute_pca` outputs.\n\n    Args:\n        pca_df: A DataFrame containing computed principal components.\n        explained_variances: The explained variance ratios for each principal component. Used for labeling\n            axes in the plot. Optional parameter. Defaults to None.\n        color_column_name: Name of the column that will be used for color-coding data points. Typically a\n            categorical variable in the original data. Optional parameter, no colors if not provided.\n            Defaults to None.\n        save_path: The save path for the plot. Optional parameter, no saving if not provided. Defaults to None.\n\n    Returns:\n        A Seaborn pairgrid containing the PCA scatter matrix.\n\n    Raises:\n        InvalidColumnException: DataFrame does not contain the given color column.\n    \"\"\"\n\n    if color_column_name and color_column_name not in pca_df.columns:\n        raise InvalidColumnException(\"DataFrame does not contain the given color column.\")\n\n    filtered_df = pca_df.filter(regex=\"^principal_component\")\n    filtered_df = pd.concat([filtered_df, pca_df[[color_column_name]]], axis=1)\n\n    pair_grid = sns.pairplot(filtered_df, hue=color_column_name)\n\n    # Add explained variances to axis labels if provided\n    if explained_variances is not None:\n        labels = [f\"PC {i+1} ({var:.1f}%)\" for i, var in enumerate(explained_variances * 100)]\n    else:\n        labels = [f\"PC {i+1}\" for i in range(len(pair_grid.axes))]\n\n    # Iterate over axes objects and set the labels\n    for i, ax_row in enumerate(pair_grid.axes):\n        for j, ax in enumerate(ax_row):\n            if j == 0:  # Only the first column\n                ax.set_ylabel(labels[i], fontsize=\"large\")\n            if i == len(ax_row) - 1:  # Only the last row\n                ax.set_xlabel(labels[j], fontsize=\"large\")\n\n    if save_path is not None:\n        plt.savefig(save_path)\n\n    return pair_grid\n</code></pre>"},{"location":"exploratory_analyses/statistical_testing/","title":"Statistical (hypothesis) testing","text":""},{"location":"exploratory_analyses/statistical_testing/#eis_toolkit.exploratory_analyses.statistical_tests.chi_square_test","title":"<code>chi_square_test(data, target_column, columns=None)</code>","text":"<p>Compute Chi-square test for independence on the input data.</p> <p>It is assumed that the variables in the input data are independent and that they are categorical, i.e. strings, booleans or integers, but not floats.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Dataframe containing the input data</p> required <code>target_column</code> <code>str</code> <p>Variable against which independence of other variables is tested.</p> required <code>columns</code> <code>Optional[Sequence[str]]</code> <p>Variables that are tested against the variable in target_column. If None, every column is used.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>Test statistics for each variable (except target_column).</p> <p>Raises:</p> Type Description <code>EmptyDataFrameException</code> <p>The input Dataframe is empty.</p> <code>InvalidParameterValueException</code> <p>The target_column is not in input Dataframe or invalid column is provided.</p> Source code in <code>eis_toolkit/exploratory_analyses/statistical_tests.py</code> <pre><code>@beartype\ndef chi_square_test(data: pd.DataFrame, target_column: str, columns: Optional[Sequence[str]] = None) -&gt; dict:\n    \"\"\"Compute Chi-square test for independence on the input data.\n\n    It is assumed that the variables in the input data are independent and that they are categorical, i.e. strings,\n    booleans or integers, but not floats.\n\n    Args:\n        data: Dataframe containing the input data\n        target_column: Variable against which independence of other variables is tested.\n        columns: Variables that are tested against the variable in target_column. If None, every column is used.\n\n    Returns:\n        Test statistics for each variable (except target_column).\n\n    Raises:\n        EmptyDataFrameException: The input Dataframe is empty.\n        InvalidParameterValueException: The target_column is not in input Dataframe or invalid column is provided.\n    \"\"\"\n    if check_empty_dataframe(data):\n        raise EmptyDataFrameException(\"The input Dataframe is empty.\")\n\n    if not check_columns_valid(data, [target_column]):\n        raise InvalidParameterValueException(\"Target column not found in the Dataframe.\")\n\n    if columns is not None:\n        invalid_columns = [column for column in columns if column not in data.columns]\n        if any(invalid_columns):\n            raise InvalidParameterValueException(f\"The following variables are not in the dataframe: {invalid_columns}\")\n    else:\n        columns = data.columns\n\n    statistics = {}\n    for column in columns:\n        if column != target_column:\n            contingency_table = pd.crosstab(data[target_column], data[column])\n            chi_square, p_value, degrees_of_freedom, _ = chi2_contingency(contingency_table)\n            statistics[column] = (chi_square, p_value, degrees_of_freedom)\n\n    return statistics\n</code></pre>"},{"location":"exploratory_analyses/statistical_testing/#eis_toolkit.exploratory_analyses.statistical_tests.correlation_matrix","title":"<code>correlation_matrix(data, correlation_method='pearson', min_periods=None)</code>","text":"<p>Compute correlation matrix on the input data.</p> <p>It is assumed that the data is numeric, i.e. integers or floats. NaN values are excluded from the calculations.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Dataframe containing the input data.</p> required <code>correlation_method</code> <code>Literal[pearson, kendall, spearman]</code> <p>'pearson', 'kendall', or 'spearman'. Defaults to 'pearson'.</p> <code>'pearson'</code> <code>min_periods</code> <code>Optional[int]</code> <p>Minimum number of observations required per pair of columns to have valid result. Optional.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Dataframe containing matrix representing the correlation coefficient             between the corresponding pair of variables.</p> <p>Raises:</p> Type Description <code>EmptyDataFrameException</code> <p>The input Dataframe is empty.</p> <code>InvalidParameterValueException</code> <p>min_periods argument is used with method 'kendall'.</p> <code>NonNumericDataException</code> <p>The input data contain non-numeric data.</p> Source code in <code>eis_toolkit/exploratory_analyses/statistical_tests.py</code> <pre><code>@beartype\ndef correlation_matrix(\n    data: pd.DataFrame,\n    correlation_method: Literal[\"pearson\", \"kendall\", \"spearman\"] = \"pearson\",\n    min_periods: Optional[int] = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Compute correlation matrix on the input data.\n\n    It is assumed that the data is numeric, i.e. integers or floats. NaN values are excluded from the calculations.\n\n    Args:\n        data: Dataframe containing the input data.\n        correlation_method: 'pearson', 'kendall', or 'spearman'. Defaults to 'pearson'.\n        min_periods: Minimum number of observations required per pair of columns to have valid result. Optional.\n\n    Returns:\n        Dataframe containing matrix representing the correlation coefficient \\\n            between the corresponding pair of variables.\n\n    Raises:\n        EmptyDataFrameException: The input Dataframe is empty.\n        InvalidParameterValueException: min_periods argument is used with method 'kendall'.\n        NonNumericDataException: The input data contain non-numeric data.\n    \"\"\"\n    if check_empty_dataframe(data):\n        raise EmptyDataFrameException(\"The input Dataframe is empty.\")\n\n    if not check_columns_numeric(data, data.columns.to_list()):\n        raise NonNumericDataException(\"The input data contain non-numeric data.\")\n\n    if correlation_method == \"kendall\" and min_periods is not None:\n        raise InvalidParameterValueException(\n            \"The argument min_periods is available only with correlation methods 'pearson' and 'spearman'.\"\n        )\n\n    matrix = data.corr(method=correlation_method, min_periods=min_periods, numeric_only=True)\n\n    return matrix\n</code></pre>"},{"location":"exploratory_analyses/statistical_testing/#eis_toolkit.exploratory_analyses.statistical_tests.covariance_matrix","title":"<code>covariance_matrix(data, min_periods=None, delta_degrees_of_freedom=1)</code>","text":"<p>Compute covariance matrix on the input data.</p> <p>It is assumed that the data is numeric, i.e. integers or floats. NaN values are excluded from the calculations.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Dataframe containing the input data.</p> required <code>min_periods</code> <code>Optional[int]</code> <p>Minimum number of observations required per pair of columns to have valid result. Optional.</p> <code>None</code> <code>delta_degrees_of_freedom</code> <code>int</code> <p>Delta degrees of freedom used for computing covariance matrix. Defaults to 1.</p> <code>1</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Dataframe containing matrix representing the covariance between the corresponding pair of variables.</p> <p>Raises:</p> Type Description <code>EmptyDataFrameException</code> <p>The input Dataframe is empty.</p> <code>InvalidParameterValueException</code> <p>Provided value for delta_degrees_of_freedom or min_periods is negative.</p> <code>NonNumericDataException</code> <p>The input data contain non-numeric data.</p> Source code in <code>eis_toolkit/exploratory_analyses/statistical_tests.py</code> <pre><code>@beartype\ndef covariance_matrix(\n    data: pd.DataFrame, min_periods: Optional[int] = None, delta_degrees_of_freedom: int = 1\n) -&gt; pd.DataFrame:\n    \"\"\"Compute covariance matrix on the input data.\n\n    It is assumed that the data is numeric, i.e. integers or floats. NaN values are excluded from the calculations.\n\n    Args:\n        data: Dataframe containing the input data.\n        min_periods: Minimum number of observations required per pair of columns to have valid result. Optional.\n        delta_degrees_of_freedom: Delta degrees of freedom used for computing covariance matrix. Defaults to 1.\n\n    Returns:\n        Dataframe containing matrix representing the covariance between the corresponding pair of variables.\n\n    Raises:\n        EmptyDataFrameException: The input Dataframe is empty.\n        InvalidParameterValueException: Provided value for delta_degrees_of_freedom or min_periods is negative.\n        NonNumericDataException: The input data contain non-numeric data.\n    \"\"\"\n    if check_empty_dataframe(data):\n        raise EmptyDataFrameException(\"The input Dataframe is empty.\")\n\n    if not check_columns_numeric(data, data.columns.to_list()):\n        raise NonNumericDataException(\"The input data contain non-numeric data.\")\n\n    if delta_degrees_of_freedom &lt; 0:\n        raise InvalidParameterValueException(\"Delta degrees of freedom must be non-negative.\")\n\n    if min_periods and min_periods &lt; 0:\n        raise InvalidParameterValueException(\"Min perioids must be non-negative.\")\n\n    matrix = data.cov(min_periods=min_periods, ddof=delta_degrees_of_freedom)\n\n    return matrix\n</code></pre>"},{"location":"exploratory_analyses/statistical_testing/#eis_toolkit.exploratory_analyses.statistical_tests.normality_test","title":"<code>normality_test(data, columns=None)</code>","text":"<p>Compute Shapiro-Wilk test for normality on the input data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[DataFrame, ndarray]</code> <p>Dataframe or Numpy array containing the input data.</p> required <code>columns</code> <code>Optional[Sequence[str]]</code> <p>Optional columns to be used for testing.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[Dict[str, Tuple[float, float]], Tuple[float, float]]</code> <p>Test statistics for each variable, output differs based on input data type. Numpy array input returns a Tuple of statistic and p_value. Dataframe input returns a dictionary where keys are column names and values are tuples containing the statistic and p-value.</p> <p>Raises:</p> Type Description <code>EmptyDataException</code> <p>The input data is empty.</p> <code>InvalidColumnException</code> <p>All selected columns were not found in the input data.</p> <code>NonNumericDataException</code> <p>Selected data or columns contains non-numeric data.</p> <code>SampleSizeExceededException</code> <p>Input data exceeds the maximum of 5000 samples.</p> Source code in <code>eis_toolkit/exploratory_analyses/statistical_tests.py</code> <pre><code>@beartype\ndef normality_test(\n    data: Union[pd.DataFrame, np.ndarray], columns: Optional[Sequence[str]] = None\n) -&gt; Union[Dict[str, Tuple[float, float]], Tuple[float, float]]:\n    \"\"\"Compute Shapiro-Wilk test for normality on the input data.\n\n    Args:\n        data: Dataframe or Numpy array containing the input data.\n        columns: Optional columns to be used for testing.\n\n    Returns:\n        Test statistics for each variable, output differs based on input data type.\n            Numpy array input returns a Tuple of statistic and p_value.\n            Dataframe input returns a dictionary where keys are column names\n            and values are tuples containing the statistic and p-value.\n\n    Raises:\n        EmptyDataException: The input data is empty.\n        InvalidColumnException: All selected columns were not found in the input data.\n        NonNumericDataException: Selected data or columns contains non-numeric data.\n        SampleSizeExceededException: Input data exceeds the maximum of 5000 samples.\n    \"\"\"\n    statistics = {}\n    if isinstance(data, pd.DataFrame):\n        if check_empty_dataframe(data):\n            raise EmptyDataException(\"The input Dataframe is empty.\")\n\n        if columns is not None:\n            if not check_columns_valid(data, columns):\n                raise InvalidColumnException(\"All selected columns were not found in the input DataFrame.\")\n            if not check_columns_numeric(data, columns):\n                raise NonNumericDataException(\"The selected columns contain non-numeric data.\")\n\n            data = data[columns].dropna()\n\n        else:\n            if not check_columns_numeric(data, data.columns):\n                raise NonNumericDataException(\"The input data contain non-numeric data.\")\n            columns = data.columns\n\n        for column in columns:\n            if len(data[column]) &gt; 5000:\n                raise SampleSizeExceededException(f\"Sample size for '{column}' exceeds the limit of 5000 samples.\")\n            statistic, p_value = shapiro(data[column])\n            statistics[column] = (statistic, p_value)\n\n    else:\n        if data.size == 0:\n            raise EmptyDataException(\"The input numpy array is empty.\")\n        if len(data) &gt; 5000:\n            raise SampleSizeExceededException(\"Sample size exceeds the limit of 5000 samples.\")\n\n        nan_mask = np.isnan(data)\n        data = data[~nan_mask]\n\n        flattened_data = data.flatten()\n        statistic, p_value = shapiro(flattened_data)\n        statistics = (statistic, p_value)\n\n    return statistics\n</code></pre>"},{"location":"prediction/fuzzy_overlay/","title":"Fuzzy overlay","text":""},{"location":"prediction/fuzzy_overlay/#eis_toolkit.prediction.fuzzy_overlay.and_overlay","title":"<code>and_overlay(data)</code>","text":"<p>Compute an 'and' overlay operation with fuzzy logic.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>The input data as a 3D Numpy array. Each 2D array represents a raster band. Data points should be in the range [0, 1].</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>2D Numpy array with the result of the 'and' overlay operation. Values are in range [0, 1].</p> <p>Raises:</p> Type Description <code>InvalidParameterValueException</code> <p>If data values are not in range [0, 1].</p> Source code in <code>eis_toolkit/prediction/fuzzy_overlay.py</code> <pre><code>@beartype\ndef and_overlay(data: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Compute an 'and' overlay operation with fuzzy logic.\n\n    Args:\n        data: The input data as a 3D Numpy array. Each 2D array represents a raster band.\n            Data points should be in the range [0, 1].\n\n    Returns:\n        2D Numpy array with the result of the 'and' overlay operation. Values are in range [0, 1].\n\n    Raises:\n        InvalidParameterValueException: If data values are not in range [0, 1].\n    \"\"\"\n    _check_input_data(data=data)\n\n    return data.min(axis=0)\n</code></pre>"},{"location":"prediction/fuzzy_overlay/#eis_toolkit.prediction.fuzzy_overlay.gamma_overlay","title":"<code>gamma_overlay(data, gamma)</code>","text":"<p>Compute a 'gamma' overlay operation with fuzzy logic.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>The input data as a 3D Numpy array. Each 2D array represents a raster band. Data points should be in the range [0, 1].</p> required <code>gamma</code> <code>float</code> <p>The gamma parameter. With gamma value of 0, the result will be the same as 'product' overlay. When gamma is closer to 1, the weight of the 'sum' overlay is increased. Value must be in the range [0, 1].</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>2D Numpy array with the result of the 'gamma' overlay operation. Values are in range [0, 1].</p> <p>Raises:</p> Type Description <code>InvalidParameterValueException</code> <p>If data values or gamma are not in range [0, 1].</p> Source code in <code>eis_toolkit/prediction/fuzzy_overlay.py</code> <pre><code>@beartype\ndef gamma_overlay(data: np.ndarray, gamma: float) -&gt; np.ndarray:\n    \"\"\"Compute a 'gamma' overlay operation with fuzzy logic.\n\n    Args:\n        data: The input data as a 3D Numpy array. Each 2D array represents a raster band.\n            Data points should be in the range [0, 1].\n        gamma: The gamma parameter. With gamma value of 0, the result will be the same as 'product' overlay.\n            When gamma is closer to 1, the weight of the 'sum' overlay is increased.\n            Value must be in the range [0, 1].\n\n    Returns:\n        2D Numpy array with the result of the 'gamma' overlay operation. Values are in range [0, 1].\n\n    Raises:\n        InvalidParameterValueException: If data values or gamma are not in range [0, 1].\n    \"\"\"\n    if gamma &lt; 0 or gamma &gt; 1:\n        raise InvalidParameterValueException(\"The gamma parameter must be in range [0, 1]\")\n\n    sum = sum_overlay(data=data)\n    product = product_overlay(data=data)\n    return product ** (1 - gamma) * sum**gamma\n</code></pre>"},{"location":"prediction/fuzzy_overlay/#eis_toolkit.prediction.fuzzy_overlay.or_overlay","title":"<code>or_overlay(data)</code>","text":"<p>Compute an 'or' overlay operation with fuzzy logic.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>The input data as a 3D Numpy array. Each 2D array represents a raster band. Data points should be in the range [0, 1].</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>2D Numpy array with the result of the 'or' overlay operation. Values are in range [0, 1].</p> <p>Raises:</p> Type Description <code>InvalidParameterValueException</code> <p>If data values are not in range [0, 1].</p> Source code in <code>eis_toolkit/prediction/fuzzy_overlay.py</code> <pre><code>@beartype\ndef or_overlay(data: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Compute an 'or' overlay operation with fuzzy logic.\n\n    Args:\n        data: The input data as a 3D Numpy array. Each 2D array represents a raster band.\n            Data points should be in the range [0, 1].\n\n    Returns:\n        2D Numpy array with the result of the 'or' overlay operation. Values are in range [0, 1].\n\n    Raises:\n        InvalidParameterValueException: If data values are not in range [0, 1].\n    \"\"\"\n    _check_input_data(data=data)\n\n    return data.max(axis=0)\n</code></pre>"},{"location":"prediction/fuzzy_overlay/#eis_toolkit.prediction.fuzzy_overlay.product_overlay","title":"<code>product_overlay(data)</code>","text":"<p>Compute a 'product' overlay operation with fuzzy logic.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>The input data as a 3D Numpy array. Each 2D array represents a raster band. Data points should be in the range [0, 1].</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>2D Numpy array with the result of the 'product' overlay operation. Values are in range [0, 1].</p> <p>Raises:</p> Type Description <code>InvalidParameterValueException</code> <p>If data values are not in range [0, 1].</p> Source code in <code>eis_toolkit/prediction/fuzzy_overlay.py</code> <pre><code>@beartype\ndef product_overlay(data: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Compute a 'product' overlay operation with fuzzy logic.\n\n    Args:\n        data: The input data as a 3D Numpy array. Each 2D array represents a raster band.\n            Data points should be in the range [0, 1].\n\n    Returns:\n        2D Numpy array with the result of the 'product' overlay operation. Values are in range [0, 1].\n\n    Raises:\n        InvalidParameterValueException: If data values are not in range [0, 1].\n    \"\"\"\n    _check_input_data(data=data)\n\n    return np.prod(data, axis=0)\n</code></pre>"},{"location":"prediction/fuzzy_overlay/#eis_toolkit.prediction.fuzzy_overlay.sum_overlay","title":"<code>sum_overlay(data)</code>","text":"<p>Compute a 'sum' overlay operation with fuzzy logic.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>The input data as a 3D Numpy array. Each 2D array represents a raster band. Data points should be in the range [0, 1].</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>2D Numpy array with the result of the 'sum' overlay operation. Values are in range [0, 1].</p> <p>Raises:</p> Type Description <code>InvalidParameterValueException</code> <p>If data values are not in range [0, 1].</p> Source code in <code>eis_toolkit/prediction/fuzzy_overlay.py</code> <pre><code>@beartype\ndef sum_overlay(data: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Compute a 'sum' overlay operation with fuzzy logic.\n\n    Args:\n        data: The input data as a 3D Numpy array. Each 2D array represents a raster band.\n            Data points should be in the range [0, 1].\n\n    Returns:\n        2D Numpy array with the result of the 'sum' overlay operation. Values are in range [0, 1].\n\n    Raises:\n        InvalidParameterValueException: If data values are not in range [0, 1].\n    \"\"\"\n    _check_input_data(data=data)\n\n    return data.sum(axis=0) - np.prod(data, axis=0)\n</code></pre>"},{"location":"prediction/gradient_boosting/","title":"Gradient boosting","text":""},{"location":"prediction/gradient_boosting/#eis_toolkit.prediction.gradient_boosting.gradient_boosting_classifier_train","title":"<code>gradient_boosting_classifier_train(X, y, validation_method='split', metrics=['accuracy'], split_size=0.2, cv_folds=5, loss='log_loss', learning_rate=0.1, n_estimators=100, max_depth=3, subsample=1.0, verbose=0, random_state=None, **kwargs)</code>","text":"<p>Train and optionally validate a Gradient Boosting classifier model using Sklearn.</p> <p>Various options and configurations for model performance evaluation are available. No validation, split to train and validation parts, and cross-validation can be chosen. If validation is performed, metric(s) to calculate can be defined and validation process configured (cross-validation method, number of folds, size of the split). Depending on the details of the validation process, the output metrics dictionary can be empty, one-dimensional or nested.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Union[ndarray, DataFrame]</code> <p>Training data.</p> required <code>y</code> <code>Union[ndarray, Series]</code> <p>Target labels.</p> required <code>validation_method</code> <code>Literal[split, kfold_cv, skfold_cv, loo_cv, none]</code> <p>Validation method to use. \"split\" divides data into two parts, \"kfold_cv\" performs k-fold cross-validation, \"skfold_cv\" performs stratified k-fold cross-validation, \"loo_cv\" performs leave-one-out cross-validation and \"none\" will not validate model at all (in this case, all X and y will be used solely for training).</p> <code>'split'</code> <code>metrics</code> <code>Sequence[Literal[accuracy, precision, recall, f1, auc]]</code> <p>Metrics to use for scoring the model. Defaults to \"accuracy\".</p> <code>['accuracy']</code> <code>split_size</code> <code>float</code> <p>Fraction of the dataset to be used as validation data (rest is used for training). Used only when validation_method is \"split\". Defaults to 0.2.</p> <code>0.2</code> <code>cv_folds</code> <code>int</code> <p>Number of folds used in cross-validation. Used only when validation_method is \"kfold_cv\" or \"skfold_cv\". Defaults to 5.</p> <code>5</code> <code>loss</code> <code>Literal[log_loss, exponential]</code> <p>The loss function to be optimized. Defaults to \"log_loss\" (same as in logistic regression).</p> <code>'log_loss'</code> <code>learning_rate</code> <code>Number</code> <p>Shrinks the contribution of each tree. Values must be &gt;= 0. Defaults to 0.1.</p> <code>0.1</code> <code>n_estimators</code> <code>int</code> <p>The number of boosting stages to run. Gradient boosting is fairly robust to over-fitting so a large number can result in better performance. Values must be &gt;= 1. Defaults to 100.</p> <code>100</code> <code>max_depth</code> <code>Optional[int]</code> <p>Maximum depth of the individual regression estimators. The maximum depth limits the number of nodes in the tree. Values must be &gt;= 1 or None, in which case nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples. Defaults to 3.</p> <code>3</code> <code>subsample</code> <code>Number</code> <p>The fraction of samples to be used for fitting the individual base learners. If smaller than 1.0 this results in Stochastic Gradient Boosting. Subsample interacts with the parameter n_estimators. Choosing subsample &lt; 1.0 leads to a reduction of variance and an increase in bias. Values must be in the range 0.0 &lt; x &lt;= 1.0. Defaults to 1.0.</p> <code>1.0</code> <code>verbose</code> <code>int</code> <p>Specifies if modeling progress and performance should be printed. 0 doesn't print, 1 prints once in a while depending on the number of tress, 2 or above will print for every tree.</p> <code>0</code> <code>random_state</code> <code>Optional[int]</code> <p>Seed for random number generation. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional parameters for Sklearn's GradientBoostingClassifier.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tuple[GradientBoostingClassifier, dict]</code> <p>The trained GradientBoostingClassifier and metric scores as a dictionary.</p> <p>Raises:</p> Type Description <code>InvalidParameterValueException</code> <p>If some of the numeric parameters are given invalid input values.</p> <code>NonMatchingParameterLengthsException</code> <p>X and y have mismatching sizes.</p> Source code in <code>eis_toolkit/prediction/gradient_boosting.py</code> <pre><code>@beartype\ndef gradient_boosting_classifier_train(\n    X: Union[np.ndarray, pd.DataFrame],\n    y: Union[np.ndarray, pd.Series],\n    validation_method: Literal[\"split\", \"kfold_cv\", \"skfold_cv\", \"loo_cv\", \"none\"] = \"split\",\n    metrics: Sequence[Literal[\"accuracy\", \"precision\", \"recall\", \"f1\", \"auc\"]] = [\"accuracy\"],\n    split_size: float = 0.2,\n    cv_folds: int = 5,\n    loss: Literal[\"log_loss\", \"exponential\"] = \"log_loss\",\n    learning_rate: Number = 0.1,\n    n_estimators: int = 100,\n    max_depth: Optional[int] = 3,\n    subsample: Number = 1.0,\n    verbose: int = 0,\n    random_state: Optional[int] = None,\n    **kwargs,\n) -&gt; Tuple[GradientBoostingClassifier, dict]:\n    \"\"\"\n    Train and optionally validate a Gradient Boosting classifier model using Sklearn.\n\n    Various options and configurations for model performance evaluation are available. No validation,\n    split to train and validation parts, and cross-validation can be chosen. If validation is performed,\n    metric(s) to calculate can be defined and validation process configured (cross-validation method,\n    number of folds, size of the split). Depending on the details of the validation process,\n    the output metrics dictionary can be empty, one-dimensional or nested.\n\n    Args:\n        X: Training data.\n        y: Target labels.\n        validation_method: Validation method to use. \"split\" divides data into two parts, \"kfold_cv\"\n            performs k-fold cross-validation, \"skfold_cv\" performs stratified k-fold cross-validation,\n            \"loo_cv\" performs leave-one-out cross-validation and \"none\" will not validate model at all\n            (in this case, all X and y will be used solely for training).\n        metrics: Metrics to use for scoring the model. Defaults to \"accuracy\".\n        split_size: Fraction of the dataset to be used as validation data (rest is used for training).\n            Used only when validation_method is \"split\". Defaults to 0.2.\n        cv_folds: Number of folds used in cross-validation. Used only when validation_method is \"kfold_cv\"\n            or \"skfold_cv\". Defaults to 5.\n        loss: The loss function to be optimized. Defaults to \"log_loss\" (same as in logistic regression).\n        learning_rate: Shrinks the contribution of each tree. Values must be &gt;= 0. Defaults to 0.1.\n        n_estimators: The number of boosting stages to run. Gradient boosting is fairly robust to over-fitting\n            so a large number can result in better performance. Values must be &gt;= 1. Defaults to 100.\n        max_depth: Maximum depth of the individual regression estimators. The maximum depth limits the number\n            of nodes in the tree. Values must be &gt;= 1 or None, in which case nodes are expanded until all leaves\n            are pure or until all leaves contain less than min_samples_split samples. Defaults to 3.\n        subsample: The fraction of samples to be used for fitting the individual base learners.\n            If smaller than 1.0 this results in Stochastic Gradient Boosting. Subsample interacts with the\n            parameter n_estimators. Choosing subsample &lt; 1.0 leads to a reduction of variance and an increase in bias.\n            Values must be in the range 0.0 &lt; x &lt;= 1.0. Defaults to 1.0.\n        verbose: Specifies if modeling progress and performance should be printed. 0 doesn't print,\n            1 prints once in a while depending on the number of tress, 2 or above will print for every tree.\n        random_state: Seed for random number generation. Defaults to None.\n        **kwargs: Additional parameters for Sklearn's GradientBoostingClassifier.\n\n    Returns:\n        The trained GradientBoostingClassifier and metric scores as a dictionary.\n\n    Raises:\n        InvalidParameterValueException: If some of the numeric parameters are given invalid input values.\n        NonMatchingParameterLengthsException: X and y have mismatching sizes.\n    \"\"\"\n    if not learning_rate &gt;= 0:\n        raise InvalidParameterValueException(\"Learning rate must be non-negative.\")\n    if not n_estimators &gt;= 1:\n        raise InvalidParameterValueException(\"N-estimators must be at least 1.\")\n    if max_depth is not None and not max_depth &gt;= 1:\n        raise InvalidParameterValueException(\"Max depth must be at least 1 or None.\")\n    if not (0 &lt; subsample &lt;= 1):\n        raise InvalidParameterValueException(\"Subsample must be more than 0 and at most 1.\")\n    if verbose &lt; 0:\n        raise InvalidParameterValueException(\"Verbose must be a non-negative number.\")\n\n    model = GradientBoostingClassifier(\n        loss=loss,\n        learning_rate=learning_rate,\n        n_estimators=n_estimators,\n        max_depth=max_depth,\n        subsample=subsample,\n        random_state=random_state,\n        verbose=verbose,\n        **kwargs,\n    )\n\n    model, metrics = _train_and_validate_sklearn_model(\n        X=X,\n        y=y,\n        model=model,\n        validation_method=validation_method,\n        metrics=metrics,\n        split_size=split_size,\n        cv_folds=cv_folds,\n        random_state=random_state,\n    )\n\n    return model, metrics\n</code></pre>"},{"location":"prediction/gradient_boosting/#eis_toolkit.prediction.gradient_boosting.gradient_boosting_regressor_train","title":"<code>gradient_boosting_regressor_train(X, y, validation_method='split', metrics=['mse'], split_size=0.2, cv_folds=5, loss='squared_error', learning_rate=0.1, n_estimators=100, max_depth=3, subsample=1.0, verbose=0, random_state=None, **kwargs)</code>","text":"<p>Train and optionally validate a Gradient Boosting regressor model using Sklearn.</p> <p>Various options and configurations for model performance evaluation are available. No validation, split to train and validation parts, and cross-validation can be chosen. If validation is performed, metric(s) to calculate can be defined and validation process configured (cross-validation method, number of folds, size of the split). Depending on the details of the validation process, the output metrics dictionary can be empty, one-dimensional or nested.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Union[ndarray, DataFrame]</code> <p>Training data.</p> required <code>y</code> <code>Union[ndarray, Series]</code> <p>Target labels.</p> required <code>validation_method</code> <code>Literal[split, kfold_cv, skfold_cv, loo_cv, none]</code> <p>Validation method to use. \"split\" divides data into two parts, \"kfold_cv\" performs k-fold cross-validation, \"skfold_cv\" performs stratified k-fold cross-validation, \"loo_cv\" performs leave-one-out cross-validation and \"none\" will not validate model at all (in this case, all X and y will be used solely for training).</p> <code>'split'</code> <code>metrics</code> <code>Sequence[Literal[mse, rmse, mae, r2]]</code> <p>Metrics to use for scoring the model. Defaults to \"mse\".</p> <code>['mse']</code> <code>split_size</code> <code>float</code> <p>Fraction of the dataset to be used as validation data (rest is used for training). Used only when validation_method is \"split\". Defaults to 0.2.</p> <code>0.2</code> <code>cv_folds</code> <code>int</code> <p>Number of folds used in cross-validation. Used only when validation_method is \"kfold_cv\" or \"skfold_cv\". Defaults to 5.</p> <code>5</code> <code>loss</code> <code>Literal[squared_error, absolute_error, huber, quantile]</code> <p>The loss function to be optimized. Defaults to \"squared_error\".</p> <code>'squared_error'</code> <code>learning_rate</code> <code>Number</code> <p>Shrinks the contribution of each tree. Values must be &gt; 0. Defaults to 0.1.</p> <code>0.1</code> <code>n_estimators</code> <code>int</code> <p>The number of boosting stages to run. Gradient boosting is fairly robust to over-fitting so a large number can result in better performance. Values must be &gt;= 1. Defaults to 100.</p> <code>100</code> <code>max_depth</code> <code>Optional[int]</code> <p>Maximum depth of the individual regression estimators. The maximum depth limits the number of nodes in the tree. Values must be &gt;= 1 or None, in which case nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples. Defaults to 3.</p> <code>3</code> <code>subsample</code> <code>Number</code> <p>The fraction of samples to be used for fitting the individual base learners. If smaller than 1.0 this results in Stochastic Gradient Boosting. Subsample interacts with the parameter n_estimators. Choosing subsample &lt; 1.0 leads to a reduction of variance and an increase in bias. Values must be in the range 0.0 &lt; x &lt;= 1.0. Defaults to 1.</p> <code>1.0</code> <code>verbose</code> <code>int</code> <p>Specifies if modeling progress and performance should be printed. 0 doesn't print, 1 prints once in a while depending on the number of tress, 2 or above will print for every tree.</p> <code>0</code> <code>random_state</code> <code>Optional[int]</code> <p>Seed for random number generation. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional parameters for Sklearn's GradientBoostingRegressor.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tuple[GradientBoostingRegressor, dict]</code> <p>The trained GradientBoostingRegressor and metric scores as a dictionary.</p> <p>Raises:</p> Type Description <code>InvalidParameterValueException</code> <p>If some of the numeric parameters are given invalid input values.</p> <code>NonMatchingParameterLengthsException</code> <p>X and y have mismatching sizes.</p> Source code in <code>eis_toolkit/prediction/gradient_boosting.py</code> <pre><code>@beartype\ndef gradient_boosting_regressor_train(\n    X: Union[np.ndarray, pd.DataFrame],\n    y: Union[np.ndarray, pd.Series],\n    validation_method: Literal[\"split\", \"kfold_cv\", \"skfold_cv\", \"loo_cv\", \"none\"] = \"split\",\n    metrics: Sequence[Literal[\"mse\", \"rmse\", \"mae\", \"r2\"]] = [\"mse\"],\n    split_size: float = 0.2,\n    cv_folds: int = 5,\n    loss: Literal[\"squared_error\", \"absolute_error\", \"huber\", \"quantile\"] = \"squared_error\",\n    learning_rate: Number = 0.1,\n    n_estimators: int = 100,\n    max_depth: Optional[int] = 3,\n    subsample: Number = 1.0,\n    verbose: int = 0,\n    random_state: Optional[int] = None,\n    **kwargs,\n) -&gt; Tuple[GradientBoostingRegressor, dict]:\n    \"\"\"\n    Train and optionally validate a Gradient Boosting regressor model using Sklearn.\n\n    Various options and configurations for model performance evaluation are available. No validation,\n    split to train and validation parts, and cross-validation can be chosen. If validation is performed,\n    metric(s) to calculate can be defined and validation process configured (cross-validation method,\n    number of folds, size of the split). Depending on the details of the validation process,\n    the output metrics dictionary can be empty, one-dimensional or nested.\n\n    Args:\n        X: Training data.\n        y: Target labels.\n        validation_method: Validation method to use. \"split\" divides data into two parts, \"kfold_cv\"\n            performs k-fold cross-validation, \"skfold_cv\" performs stratified k-fold cross-validation,\n            \"loo_cv\" performs leave-one-out cross-validation and \"none\" will not validate model at all\n            (in this case, all X and y will be used solely for training).\n        metrics: Metrics to use for scoring the model. Defaults to \"mse\".\n        split_size: Fraction of the dataset to be used as validation data (rest is used for training).\n            Used only when validation_method is \"split\". Defaults to 0.2.\n        cv_folds: Number of folds used in cross-validation. Used only when validation_method is \"kfold_cv\"\n            or \"skfold_cv\". Defaults to 5.\n        loss: The loss function to be optimized. Defaults to \"squared_error\".\n        learning_rate: Shrinks the contribution of each tree. Values must be &gt; 0. Defaults to 0.1.\n        n_estimators: The number of boosting stages to run. Gradient boosting is fairly robust to over-fitting\n            so a large number can result in better performance. Values must be &gt;= 1. Defaults to 100.\n        max_depth: Maximum depth of the individual regression estimators. The maximum depth limits the number\n            of nodes in the tree. Values must be &gt;= 1 or None, in which case nodes are expanded until all leaves\n            are pure or until all leaves contain less than min_samples_split samples. Defaults to 3.\n        subsample: The fraction of samples to be used for fitting the individual base learners.\n            If smaller than 1.0 this results in Stochastic Gradient Boosting. Subsample interacts with the\n            parameter n_estimators. Choosing subsample &lt; 1.0 leads to a reduction of variance and an increase in bias.\n            Values must be in the range 0.0 &lt; x &lt;= 1.0. Defaults to 1.\n        verbose: Specifies if modeling progress and performance should be printed. 0 doesn't print,\n            1 prints once in a while depending on the number of tress, 2 or above will print for every tree.\n        random_state: Seed for random number generation. Defaults to None.\n        **kwargs: Additional parameters for Sklearn's GradientBoostingRegressor.\n\n    Returns:\n        The trained GradientBoostingRegressor and metric scores as a dictionary.\n\n    Raises:\n        InvalidParameterValueException: If some of the numeric parameters are given invalid input values.\n        NonMatchingParameterLengthsException: X and y have mismatching sizes.\n    \"\"\"\n    if not learning_rate &gt;= 0:\n        raise InvalidParameterValueException(\"Learning rate must be non-negative.\")\n    if not n_estimators &gt;= 1:\n        raise InvalidParameterValueException(\"N-estimators must be at least 1.\")\n    if max_depth is not None and not max_depth &gt;= 1:\n        raise InvalidParameterValueException(\"Max depth must be at least 1 or None.\")\n    if not (0 &lt; subsample &lt;= 1):\n        raise InvalidParameterValueException(\"Subsample must be more than 0 and at most 1.\")\n    if verbose &lt; 0:\n        raise InvalidParameterValueException(\"Verbose must be a non-negative number.\")\n\n    model = GradientBoostingRegressor(\n        loss=loss,\n        learning_rate=learning_rate,\n        n_estimators=n_estimators,\n        max_depth=max_depth,\n        subsample=subsample,\n        random_state=random_state,\n        verbose=verbose,\n        **kwargs,\n    )\n\n    model, metrics = _train_and_validate_sklearn_model(\n        X=X,\n        y=y,\n        model=model,\n        validation_method=validation_method,\n        metrics=metrics,\n        split_size=split_size,\n        cv_folds=cv_folds,\n        random_state=random_state,\n    )\n\n    return model, metrics\n</code></pre>"},{"location":"prediction/logistic_regression/","title":"Logistic regression","text":""},{"location":"prediction/logistic_regression/#eis_toolkit.prediction.logistic_regression.logistic_regression_train","title":"<code>logistic_regression_train(X, y, validation_method='split', metrics=['accuracy'], split_size=0.2, cv_folds=5, penalty='l2', max_iter=100, solver='lbfgs', verbose=0, random_state=None, **kwargs)</code>","text":"<p>Train and optionally validate a Logistic Regression classifier model using Sklearn.</p> <p>Various options and configurations for model performance evaluation are available. No validation, split to train and validation parts, and cross-validation can be chosen. If validation is performed, metric(s) to calculate can be defined and validation process configured (cross-validation method, number of folds, size of the split). Depending on the details of the validation process, the output metrics dictionary can be empty, one-dimensional or nested.</p> <p>The choice of the algorithm depends on the penalty chosen. Supported penalties by solver: 'lbfgs' - ['l2', None] 'liblinear' - ['l1', 'l2'] 'newton-cg' - ['l2', None] 'newton-cholesky' - ['l2', None] 'sag' - ['l2', None] 'saga' - ['elasticnet', 'l1', 'l2', None]</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Union[ndarray, DataFrame]</code> <p>Training data.</p> required <code>y</code> <code>Union[ndarray, Series]</code> <p>Target labels.</p> required <code>validation_method</code> <code>Literal[split, kfold_cv, skfold_cv, loo_cv, none]</code> <p>Validation method to use. \"split\" divides data into two parts, \"kfold_cv\" performs k-fold cross-validation, \"skfold_cv\" performs stratified k-fold cross-validation, \"loo_cv\" performs leave-one-out cross-validation and \"none\" will not validate model at all (in this case, all X and y will be used solely for training).</p> <code>'split'</code> <code>metrics</code> <code>Sequence[Literal[accuracy, precision, recall, f1, auc]]</code> <p>Metrics to use for scoring the model. Defaults to \"accuracy\".</p> <code>['accuracy']</code> <code>split_size</code> <code>float</code> <p>Fraction of the dataset to be used as validation data (rest is used for training). Used only when validation_method is \"split\". Defaults to 0.2.</p> <code>0.2</code> <code>cv_folds</code> <code>int</code> <p>Number of folds used in cross-validation. Used only when validation_method is \"kfold_cv\" or \"skfold_cv\". Defaults to 5.</p> <code>5</code> <code>penalty</code> <code>Literal[l1, l2, elasicnet, None]</code> <p>Specifies the norm of the penalty. Defaults to 'l2'.</p> <code>'l2'</code> <code>max_iter</code> <code>int</code> <p>Maximum number of iterations taken for the solvers to converge. Defaults to 100.</p> <code>100</code> <code>solver</code> <code>Literal[lbfgs, liblinear, newton - cg, newton - cholesky, sag, saga]</code> <p>Algorithm to use in the optimization problem. Defaults to 'lbfgs'.</p> <code>'lbfgs'</code> <code>verbose</code> <code>int</code> <p>Specifies if modeling progress and performance should be printed. 0 doesn't print, values 1 or above will produce prints.</p> <code>0</code> <code>random_state</code> <code>Optional[int]</code> <p>Seed for random number generation. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional parameters for Sklearn's LogisticRegression.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tuple[LogisticRegression, dict]</code> <p>The trained Logistric Regression classifier and metric scores as a dictionary.</p> <p>Raises:</p> Type Description <code>InvalidParameterValueException</code> <p>If some of the numeric parameters are given invalid input values.</p> <code>NonMatchingParameterLengthsException</code> <p>X and y have mismatching sizes.</p> Source code in <code>eis_toolkit/prediction/logistic_regression.py</code> <pre><code>@beartype\ndef logistic_regression_train(\n    X: Union[np.ndarray, pd.DataFrame],\n    y: Union[np.ndarray, pd.Series],\n    validation_method: Literal[\"split\", \"kfold_cv\", \"skfold_cv\", \"loo_cv\", \"none\"] = \"split\",\n    metrics: Sequence[Literal[\"accuracy\", \"precision\", \"recall\", \"f1\", \"auc\"]] = [\"accuracy\"],\n    split_size: float = 0.2,\n    cv_folds: int = 5,\n    penalty: Literal[\"l1\", \"l2\", \"elasicnet\", None] = \"l2\",\n    max_iter: int = 100,\n    solver: Literal[\"lbfgs\", \"liblinear\", \"newton-cg\", \"newton-cholesky\", \"sag\", \"saga\"] = \"lbfgs\",\n    verbose: int = 0,\n    random_state: Optional[int] = None,\n    **kwargs\n) -&gt; Tuple[LogisticRegression, dict]:\n    \"\"\"\n    Train and optionally validate a Logistic Regression classifier model using Sklearn.\n\n    Various options and configurations for model performance evaluation are available. No validation,\n    split to train and validation parts, and cross-validation can be chosen. If validation is performed,\n    metric(s) to calculate can be defined and validation process configured (cross-validation method,\n    number of folds, size of the split). Depending on the details of the validation process,\n    the output metrics dictionary can be empty, one-dimensional or nested.\n\n    The choice of the algorithm depends on the penalty chosen. Supported penalties by solver:\n    'lbfgs' - ['l2', None]\n    'liblinear' - ['l1', 'l2']\n    'newton-cg' - ['l2', None]\n    'newton-cholesky' - ['l2', None]\n    'sag' - ['l2', None]\n    'saga' - ['elasticnet', 'l1', 'l2', None]\n\n    Args:\n        X: Training data.\n        y: Target labels.\n        validation_method: Validation method to use. \"split\" divides data into two parts, \"kfold_cv\"\n            performs k-fold cross-validation, \"skfold_cv\" performs stratified k-fold cross-validation,\n            \"loo_cv\" performs leave-one-out cross-validation and \"none\" will not validate model at all\n            (in this case, all X and y will be used solely for training).\n        metrics: Metrics to use for scoring the model. Defaults to \"accuracy\".\n        split_size: Fraction of the dataset to be used as validation data (rest is used for training).\n            Used only when validation_method is \"split\". Defaults to 0.2.\n        cv_folds: Number of folds used in cross-validation. Used only when validation_method is \"kfold_cv\"\n            or \"skfold_cv\". Defaults to 5.\n        penalty: Specifies the norm of the penalty. Defaults to 'l2'.\n        max_iter: Maximum number of iterations taken for the solvers to converge. Defaults to 100.\n        solver: Algorithm to use in the optimization problem. Defaults to 'lbfgs'.\n        verbose: Specifies if modeling progress and performance should be printed. 0 doesn't print,\n            values 1 or above will produce prints.\n        random_state: Seed for random number generation. Defaults to None.\n        **kwargs: Additional parameters for Sklearn's LogisticRegression.\n\n    Returns:\n        The trained Logistric Regression classifier and metric scores as a dictionary.\n\n    Raises:\n        InvalidParameterValueException: If some of the numeric parameters are given invalid input values.\n        NonMatchingParameterLengthsException: X and y have mismatching sizes.\n    \"\"\"\n    if max_iter &lt; 1:\n        raise InvalidParameterValueException(\"Max iter must be &gt; 0.\")\n    if verbose &lt; 0:\n        raise InvalidParameterValueException(\"Verbose must be a non-negative number.\")\n\n    model = LogisticRegression(\n        penalty=penalty, max_iter=max_iter, random_state=random_state, solver=solver, verbose=verbose, **kwargs\n    )\n\n    model, metrics = _train_and_validate_sklearn_model(\n        X=X,\n        y=y,\n        model=model,\n        validation_method=validation_method,\n        metrics=metrics,\n        split_size=split_size,\n        cv_folds=cv_folds,\n        random_state=random_state,\n    )\n\n    return model, metrics\n</code></pre>"},{"location":"prediction/machine_learning_general/","title":"Logistic regression","text":""},{"location":"prediction/machine_learning_general/#eis_toolkit.prediction.machine_learning_general.evaluate_model","title":"<code>evaluate_model(X_test, y_test, model, metrics=None)</code>","text":"<p>Evaluate/score a trained model with test data.</p> <p>Predicts with the given test data and evaluates the predictions.</p> <p>Parameters:</p> Name Type Description Default <code>X_test</code> <code>Union[ndarray, DataFrame]</code> <p>Test data.</p> required <code>y_test</code> <code>Union[ndarray, Series]</code> <p>Target labels for test data.</p> required <code>model</code> <code>Union[BaseEstimator, Model]</code> <p>Trained Sklearn classifier or regressor.</p> required <code>metrics</code> <code>Optional[Sequence[Literal[mse, rmse, mae, r2, accuracy, precision, recall, f1]]]</code> <p>Metrics to use for scoring the model. Defaults to \"accuracy\" for a classifier and to \"mse\" for a regressor.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, Dict[str, Number]]</code> <p>Predictions and metric scores as a dictionary.</p> Source code in <code>eis_toolkit/prediction/machine_learning_general.py</code> <pre><code>@beartype\ndef evaluate_model(\n    X_test: Union[np.ndarray, pd.DataFrame],\n    y_test: Union[np.ndarray, pd.Series],\n    model: Union[BaseEstimator, keras.Model],\n    metrics: Optional[Sequence[Literal[\"mse\", \"rmse\", \"mae\", \"r2\", \"accuracy\", \"precision\", \"recall\", \"f1\"]]] = None,\n) -&gt; Tuple[np.ndarray, Dict[str, Number]]:\n    \"\"\"\n    Evaluate/score a trained model with test data.\n\n    Predicts with the given test data and evaluates the predictions.\n\n    Args:\n        X_test: Test data.\n        y_test: Target labels for test data.\n        model: Trained Sklearn classifier or regressor.\n        metrics: Metrics to use for scoring the model. Defaults to \"accuracy\" for a classifier\n            and to \"mse\" for a regressor.\n\n    Returns:\n        Predictions and metric scores as a dictionary.\n    \"\"\"\n    x_size = X_test.index.size if isinstance(X_test, pd.DataFrame) else X_test.shape[0]\n    if x_size != y_test.size:\n        raise NonMatchingParameterLengthsException(f\"X and y must have the length {x_size} != {y_test.size}.\")\n\n    if metrics is None:\n        metrics = [\"accuracy\"] if is_classifier(model) else [\"mse\"]\n\n    y_pred = model.predict(X_test)\n\n    out_metrics = {}\n    for metric in metrics:\n        score = _score_model(model, y_test, y_pred, metric)\n        out_metrics[metric] = score\n\n    return y_pred, out_metrics\n</code></pre>"},{"location":"prediction/machine_learning_general/#eis_toolkit.prediction.machine_learning_general.load_model","title":"<code>load_model(path)</code>","text":"<p>Load a Sklearn model from a .joblib file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Path from where the model should be loaded. Include the .joblib file extension.</p> required <p>Returns:</p> Type Description <code>BaseEstimator</code> <p>Loaded model.</p> Source code in <code>eis_toolkit/prediction/machine_learning_general.py</code> <pre><code>@beartype\ndef load_model(path: Path) -&gt; BaseEstimator:\n    \"\"\"\n    Load a Sklearn model from a .joblib file.\n\n    Args:\n        path: Path from where the model should be loaded. Include the .joblib file extension.\n\n    Returns:\n        Loaded model.\n    \"\"\"\n    return joblib.load(path)\n</code></pre>"},{"location":"prediction/machine_learning_general/#eis_toolkit.prediction.machine_learning_general.predict","title":"<code>predict(data, model)</code>","text":"<p>Predict with a trained model.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[ndarray, DataFrame]</code> <p>Data used to make predictions.</p> required <code>model</code> <code>Union[BaseEstimator, Model]</code> <p>Trained classifier or regressor. Can be any machine learning model trained with EIS Toolkit (Sklearn and Keras models).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Predictions.</p> Source code in <code>eis_toolkit/prediction/machine_learning_general.py</code> <pre><code>@beartype\ndef predict(data: Union[np.ndarray, pd.DataFrame], model: Union[BaseEstimator, keras.Model]) -&gt; np.ndarray:\n    \"\"\"\n    Predict with a trained model.\n\n    Args:\n        data: Data used to make predictions.\n        model: Trained classifier or regressor. Can be any machine learning model trained with\n            EIS Toolkit (Sklearn and Keras models).\n\n    Returns:\n        Predictions.\n    \"\"\"\n    result = model.predict(data)\n    return result\n</code></pre>"},{"location":"prediction/machine_learning_general/#eis_toolkit.prediction.machine_learning_general.prepare_data_for_ml","title":"<code>prepare_data_for_ml(training_raster_files, label_file=None)</code>","text":"<p>Prepare data ready for machine learning model training.</p> <p>Performs the following steps: - Read all bands of all training rasters into a stacked Numpy array - Read label data (and rasterize if a vector file is given) - Create a nodata mask using all training rasters and labels, and mask nodata cells out</p> <p>Parameters:</p> Name Type Description Default <code>training_raster_files</code> <code>Sequence[Union[str, PathLike]]</code> <p>List of filepaths of training rasters. Files should only include raster that have the same grid properties and extent.</p> required <code>label_file</code> <code>Optional[Union[str, PathLike]]</code> <p>Filepath to label (deposits) data. File can be either a vector file or raster file. If a vector file is provided, it will be rasterized into similar grid as training rasters. If a raster file is provided, it needs to have same grid properties and extent as training rasters. Optional parameter and can be omitted if preparing data for predicting. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, Optional[ndarray], Profile, Any]</code> <p>Training data (X) in prepared shape, target labels (y) in prepared shape (if <code>label_file</code> was given), refrence raster metadata and nodata mask applied to X and y.</p> Source code in <code>eis_toolkit/prediction/machine_learning_general.py</code> <pre><code>@beartype\ndef prepare_data_for_ml(\n    training_raster_files: Sequence[Union[str, os.PathLike]],\n    label_file: Optional[Union[str, os.PathLike]] = None,\n) -&gt; Tuple[np.ndarray, Optional[np.ndarray], rasterio.profiles.Profile, Any]:\n    \"\"\"\n    Prepare data ready for machine learning model training.\n\n    Performs the following steps:\n    - Read all bands of all training rasters into a stacked Numpy array\n    - Read label data (and rasterize if a vector file is given)\n    - Create a nodata mask using all training rasters and labels, and mask nodata cells out\n\n    Args:\n        training_raster_files: List of filepaths of training rasters. Files should only include\n            raster that have the same grid properties and extent.\n        label_file: Filepath to label (deposits) data. File can be either a vector file or raster file.\n            If a vector file is provided, it will be rasterized into similar grid as training rasters. If\n            a raster file is provided, it needs to have same grid properties and extent as training rasters.\n            Optional parameter and can be omitted if preparing data for predicting. Defaults to None.\n\n    Returns:\n        Training data (X) in prepared shape, target labels (y) in prepared shape (if `label_file` was given),\n            refrence raster metadata and nodata mask applied to X and y.\n    \"\"\"\n\n    def _read_and_stack_training_raster(filepath: Union[str, os.PathLike]) -&gt; Tuple[np.ndarray, dict]:\n        \"\"\"Read all bands of raster file with training data in a stack.\"\"\"\n        with rasterio.open(filepath) as src:\n            raster_data = np.stack([src.read(i) for i in range(1, src.count + 1)])\n            profile = src.profile\n        return raster_data, profile\n\n    # Read and stack training rasters\n    training_data, profiles = zip(*[_read_and_stack_training_raster(file) for file in training_raster_files])\n    # # TODO. Waiting for check_raster_grids input modification to profiles\n    # if not check_raster_grids(profiles, same_extent=True):\n    #     raise NonMatchingRasterGridException\n    reference_profile = profiles[0]\n    nodata_values = [profile[\"nodata\"] for profile in profiles]\n\n    # Reshape training data for ML and create mask\n    reshaped_data = []\n    nodata_mask = None\n\n    for raster, nodata in zip(training_data, nodata_values):\n        raster_reshaped = raster.reshape(raster.shape[0], -1).T\n        reshaped_data.append(raster_reshaped)\n\n        if nodata is not None:\n            raster_mask = (raster_reshaped == nodata).any(axis=1)\n            nodata_mask = raster_mask if nodata_mask is None else nodata_mask | raster_mask\n\n    X = np.concatenate(reshaped_data, axis=1)\n\n    if label_file is not None:\n        # Check label file type and process accordingly\n        file_extension = os.path.splitext(label_file)[1].lower()\n\n        # Labels/deposits in vector format\n        if file_extension in [\".shp\", \".geojson\", \".json\", \".gpkg\"]:\n            y, _ = rasterize_vector(geodataframe=gpd.read_file(label_file), base_raster_profile=reference_profile)\n\n        # Labels/deposits in raster format\n        else:\n            with rasterio.open(label_file) as label_raster:\n                y = label_raster.read(1)  # Assuming labels are in the first band\n                label_nodata = label_raster.nodata\n\n            label_nodata_mask = y == label_nodata\n\n            # Combine masks and apply to feature and label data\n            nodata_mask = nodata_mask | label_nodata_mask.ravel()\n\n        y = y.ravel()[~nodata_mask]\n\n    else:\n        y = None\n\n    X = X[~nodata_mask]\n\n    return X, y, reference_profile, nodata_mask\n</code></pre>"},{"location":"prediction/machine_learning_general/#eis_toolkit.prediction.machine_learning_general.reshape_predictions","title":"<code>reshape_predictions(predictions, height, width, nodata_mask=None)</code>","text":"<p>Reshape 1D prediction ouputs into 2D Numpy array.</p> <p>The output is ready to be visualized and saved as a raster.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>ndarray</code> <p>A 1D Numpy array with raw prediction data from <code>predict</code> function.</p> required <code>height</code> <code>int</code> <p>Height of the output array</p> required <code>width</code> <code>int</code> <p>Width of the output array</p> required <code>nodata_mask</code> <code>Optional[ndarray]</code> <p>Nodata mask used to reconstruct original shape of data. This is the same mask applied to data before predicting to remove nodata. If any nodata was removed before predicting, this mask is required to reconstruct the original shape of data. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Predictions as a 2D Numpy array in the original array shape.</p> Source code in <code>eis_toolkit/prediction/machine_learning_general.py</code> <pre><code>@beartype\ndef reshape_predictions(\n    predictions: np.ndarray, height: int, width: int, nodata_mask: Optional[np.ndarray] = None\n) -&gt; np.ndarray:\n    \"\"\"\n    Reshape 1D prediction ouputs into 2D Numpy array.\n\n    The output is ready to be visualized and saved as a raster.\n\n    Args:\n        predictions: A 1D Numpy array with raw prediction data from `predict` function.\n        height: Height of the output array\n        width: Width of the output array\n        nodata_mask: Nodata mask used to reconstruct original shape of data. This is the same mask\n            applied to data before predicting to remove nodata. If any nodata was removed\n            before predicting, this mask is required to reconstruct the original shape of data.\n            Defaults to None.\n\n    Returns:\n        Predictions as a 2D Numpy array in the original array shape.\n    \"\"\"\n    full_predictions_array = np.full(width * height, np.nan, dtype=predictions.dtype)\n    if nodata_mask is not None:\n        full_predictions_array[~nodata_mask.ravel()] = predictions\n    predictions_reshaped = full_predictions_array.reshape((height, width))\n    return predictions_reshaped\n</code></pre>"},{"location":"prediction/machine_learning_general/#eis_toolkit.prediction.machine_learning_general.save_model","title":"<code>save_model(model, path)</code>","text":"<p>Save a trained Sklearn model to a .joblib file.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>BaseEstimator</code> <p>Trained model.</p> required <code>path</code> <code>Path</code> <p>Path where the model should be saved. Include the .joblib file extension.</p> required Source code in <code>eis_toolkit/prediction/machine_learning_general.py</code> <pre><code>@beartype\ndef save_model(model: BaseEstimator, path: Path) -&gt; None:\n    \"\"\"\n    Save a trained Sklearn model to a .joblib file.\n\n    Args:\n        model: Trained model.\n        path: Path where the model should be saved. Include the .joblib file extension.\n    \"\"\"\n    joblib.dump(model, path)\n</code></pre>"},{"location":"prediction/machine_learning_general/#eis_toolkit.prediction.machine_learning_general.split_data","title":"<code>split_data(*data, split_size=0.2, random_state=None, shuffle=True)</code>","text":"<p>Split data into two parts. Can be used for train-test or train-validation splits.</p> <p>For more guidance, read documentation of sklearn.model_selection.train_test_split: (https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html).</p> <p>Parameters:</p> Name Type Description Default <code>*data</code> <code>Union[ndarray, DataFrame, csr_matrix, List[Number]]</code> <p>Data to be split. Multiple datasets can be given as input (for example X and y), but they need to have the same length. All datasets are split into two and the parts returned (for example X_train, X_test, y_train, y_test).</p> <code>()</code> <code>split_size</code> <code>float</code> <p>The proportion of the second part of the split. Typically this is the size of test/validation part. The first part will be complemental proportion. For example, if split_size = 0.2, the first part will have 80% of the data and the second part 20% of the data. Defaults to 0.2.</p> <code>0.2</code> <code>random_state</code> <code>Optional[int]</code> <p>Seed for random number generation. Defaults to None.</p> <code>None</code> <code>shuffle</code> <code>bool</code> <p>If data is shuffled before splitting. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>List[Union[ndarray, DataFrame, csr_matrix, List[Number]]]</code> <p>List containing splits of inputs (two outputs per input).</p> Source code in <code>eis_toolkit/prediction/machine_learning_general.py</code> <pre><code>@beartype\ndef split_data(\n    *data: Union[np.ndarray, pd.DataFrame, sparse._csr.csr_matrix, List[Number]],\n    split_size: float = 0.2,\n    random_state: Optional[int] = None,\n    shuffle: bool = True,\n) -&gt; List[Union[np.ndarray, pd.DataFrame, sparse._csr.csr_matrix, List[Number]]]:\n    \"\"\"\n    Split data into two parts. Can be used for train-test or train-validation splits.\n\n    For more guidance, read documentation of sklearn.model_selection.train_test_split:\n    (https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html).\n\n    Args:\n        *data: Data to be split. Multiple datasets can be given as input (for example X and y),\n            but they need to have the same length. All datasets are split into two and the parts returned\n            (for example X_train, X_test, y_train, y_test).\n        split_size: The proportion of the second part of the split. Typically this is the size of test/validation\n            part. The first part will be complemental proportion. For example, if split_size = 0.2, the first part\n            will have 80% of the data and the second part 20% of the data. Defaults to 0.2.\n        random_state: Seed for random number generation. Defaults to None.\n        shuffle: If data is shuffled before splitting. Defaults to True.\n\n    Returns:\n        List containing splits of inputs (two outputs per input).\n    \"\"\"\n\n    if not (0 &lt; split_size &lt; 1):\n        raise InvalidParameterValueException(\"Split size must be more than 0 and less than 1.\")\n\n    split_data = train_test_split(*data, test_size=split_size, random_state=random_state, shuffle=shuffle)\n\n    return split_data\n</code></pre>"},{"location":"prediction/mlp/","title":"MLP","text":""},{"location":"prediction/mlp/#eis_toolkit.prediction.mlp.train_MLP_classifier","title":"<code>train_MLP_classifier(X, y, neurons, validation_split=0.2, validation_data=None, activation='relu', output_neurons=1, last_activation='sigmoid', epochs=50, batch_size=32, optimizer='adam', learning_rate=0.001, loss_function='binary_crossentropy', dropout_rate=None, early_stopping=True, es_patience=5, metrics=['accuracy'], random_state=None)</code>","text":"<p>Train MLP (Multilayer Perceptron) using Keras.</p> <p>Creates a Sequential model with Dense NN layers. For each element in <code>neurons</code>, Dense layer with corresponding dimensionality/neurons is created with the specified activation function (<code>activation</code>). If <code>dropout_rate</code> is specified, a Dropout layer is added after each Dense layer.</p> <p>Parameters default to a binary classification model using sigmoid as last activation, binary crossentropy as loss function and 1 output neuron/unit.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Input data. Should be a 2-dimensional array where each row represents a sample and each column a feature. Features should ideally be normalized or standardized.</p> required <code>y</code> <code>ndarray</code> <p>Target labels. For binary classification, y should be a 1-dimensional array of binary labels (0 or 1). For multi-class classification, y should be a 2D array with one-hot encoded labels. The number of columns should match the number of classes.</p> required <code>neurons</code> <code>Sequence[int]</code> <p>Number of neurons in each hidden layer.</p> required <code>validation_split</code> <code>Optional[float]</code> <p>Fraction of data used for validation during training. Value must be &gt; 0 and &lt; 1 or None. Defaults to 0.2.</p> <code>0.2</code> <code>validation_data</code> <code>Optional[Tuple[ndarray, ndarray]]</code> <p>Separate dataset used for validation during training. Overrides validation_split if provided. Expected data form is (X_valid, y_valid). Defaults to None.</p> <code>None</code> <code>activation</code> <code>Literal[relu, linear, sigmoid, tanh]</code> <p>Activation function used in each hidden layer. Defaults to 'relu'.</p> <code>'relu'</code> <code>output_neurons</code> <code>int</code> <p>Number of neurons in the output layer. Defaults to 1.</p> <code>1</code> <code>last_activation</code> <code>Literal[sigmoid, softmax]</code> <p>Activation function used in the output layer. Defaults to 'sigmoid'.</p> <code>'sigmoid'</code> <code>epochs</code> <code>int</code> <p>Number of epochs to train the model. Defaults to 50.</p> <code>50</code> <code>batch_size</code> <code>int</code> <p>Number of samples per gradient update. Defaults to 32.</p> <code>32</code> <code>optimizer</code> <code>Literal[adam, adagrad, rmsprop, sdg]</code> <p>Optimizer to be used. Defaults to 'adam'.</p> <code>'adam'</code> <code>learning_rate</code> <code>Number</code> <p>Learning rate to be used in training. Value must be &gt; 0. Defalts to 0.001.</p> <code>0.001</code> <code>loss_function</code> <code>Literal[binary_crossentropy, categorical_crossentropy]</code> <p>Loss function to be used. Defaults to 'binary_crossentropy'.</p> <code>'binary_crossentropy'</code> <code>dropout_rate</code> <code>Optional[Number]</code> <p>Fraction of the input units to drop. Value must be &gt;= 0 and &lt;= 1. Defaults to None.</p> <code>None</code> <code>early_stopping</code> <code>bool</code> <p>Whether or not to use early stopping in training. Defaults to True.</p> <code>True</code> <code>es_patience</code> <code>int</code> <p>Number of epochs with no improvement after which training will be stopped. Defaults to 5.</p> <code>5</code> <code>metrics</code> <code>Optional[Sequence[Literal[accuracy, precision, recall, f1_score]]]</code> <p>Metrics to be evaluated by the model during training and testing. Defaults to ['accuracy'].</p> <code>['accuracy']</code> <code>random_state</code> <code>Optional[int]</code> <p>Seed for random number generation. Sets Python, Numpy and Tensorflow seeds to make program deterministic. Defaults to None (random state / seed).</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[Model, dict]</code> <p>Trained MLP model and training history.</p> <p>Raises:</p> Type Description <code>InvalidParameterValueException</code> <p>Some of the numeric parameters have invalid values.</p> <code>InvalidDataShapeException</code> <p>Shape of X or y is invalid.</p> Source code in <code>eis_toolkit/prediction/mlp.py</code> <pre><code>@beartype\ndef train_MLP_classifier(\n    X: np.ndarray,\n    y: np.ndarray,\n    neurons: Sequence[int],\n    validation_split: Optional[float] = 0.2,\n    validation_data: Optional[Tuple[np.ndarray, np.ndarray]] = None,\n    activation: Literal[\"relu\", \"linear\", \"sigmoid\", \"tanh\"] = \"relu\",\n    output_neurons: int = 1,\n    last_activation: Literal[\"sigmoid\", \"softmax\"] = \"sigmoid\",\n    epochs: int = 50,\n    batch_size: int = 32,\n    optimizer: Literal[\"adam\", \"adagrad\", \"rmsprop\", \"sdg\"] = \"adam\",\n    learning_rate: Number = 0.001,\n    loss_function: Literal[\"binary_crossentropy\", \"categorical_crossentropy\"] = \"binary_crossentropy\",\n    dropout_rate: Optional[Number] = None,\n    early_stopping: bool = True,\n    es_patience: int = 5,\n    metrics: Optional[Sequence[Literal[\"accuracy\", \"precision\", \"recall\", \"f1_score\"]]] = [\"accuracy\"],\n    random_state: Optional[int] = None,\n) -&gt; Tuple[keras.Model, dict]:\n    \"\"\"\n    Train MLP (Multilayer Perceptron) using Keras.\n\n    Creates a Sequential model with Dense NN layers. For each element in `neurons`, Dense layer with corresponding\n    dimensionality/neurons is created with the specified activation function (`activation`). If `dropout_rate` is\n    specified, a Dropout layer is added after each Dense layer.\n\n    Parameters default to a binary classification model using sigmoid as last activation, binary crossentropy as loss\n    function and 1 output neuron/unit.\n\n    Args:\n        X: Input data. Should be a 2-dimensional array where each row represents a sample and each column a\n            feature. Features should ideally be normalized or standardized.\n        y: Target labels. For binary classification, y should be a 1-dimensional array of binary labels (0 or 1).\n            For multi-class classification, y should be a 2D array with one-hot encoded labels. The number of columns\n            should match the number of classes.\n        neurons: Number of neurons in each hidden layer.\n        validation_split: Fraction of data used for validation during training. Value must be &gt; 0 and &lt; 1 or None.\n            Defaults to 0.2.\n        validation_data: Separate dataset used for validation during training. Overrides validation_split if\n            provided. Expected data form is (X_valid, y_valid). Defaults to None.\n        activation: Activation function used in each hidden layer. Defaults to 'relu'.\n        output_neurons: Number of neurons in the output layer. Defaults to 1.\n        last_activation: Activation function used in the output layer. Defaults to 'sigmoid'.\n        epochs: Number of epochs to train the model. Defaults to 50.\n        batch_size: Number of samples per gradient update. Defaults to 32.\n        optimizer: Optimizer to be used. Defaults to 'adam'.\n        learning_rate: Learning rate to be used in training. Value must be &gt; 0. Defalts to 0.001.\n        loss_function: Loss function to be used. Defaults to 'binary_crossentropy'.\n        dropout_rate: Fraction of the input units to drop. Value must be &gt;= 0 and &lt;= 1. Defaults to None.\n        early_stopping: Whether or not to use early stopping in training. Defaults to True.\n        es_patience: Number of epochs with no improvement after which training will be stopped. Defaults to 5.\n        metrics: Metrics to be evaluated by the model during training and testing. Defaults to ['accuracy'].\n        random_state: Seed for random number generation. Sets Python, Numpy and Tensorflow seeds to make\n            program deterministic. Defaults to None (random state / seed).\n\n    Returns:\n        Trained MLP model and training history.\n\n    Raises:\n        InvalidParameterValueException: Some of the numeric parameters have invalid values.\n        InvalidDataShapeException: Shape of X or y is invalid.\n    \"\"\"\n    # 1. Check input data\n    _check_ML_model_data_input(X=X, y=y)\n    _check_MLP_inputs(\n        neurons=neurons,\n        validation_split=validation_split,\n        learning_rate=learning_rate,\n        dropout_rate=dropout_rate,\n        es_patience=es_patience,\n        batch_size=batch_size,\n        epochs=epochs,\n        output_neurons=output_neurons,\n        loss_function=loss_function,\n    )\n\n    if random_state is not None:\n        keras.utils.set_random_seed(random_state)\n\n    # 2. Create and compile a sequential model\n    model = keras.Sequential()\n\n    model.add(keras.layers.Input(shape=(X.shape[1],)))\n\n    for neuron in neurons:\n        model.add(keras.layers.Dense(units=neuron, activation=activation))\n\n        if dropout_rate is not None:\n            model.add(keras.layers.Dropout(dropout_rate))\n\n    model.add(keras.layers.Dense(units=output_neurons, activation=last_activation))\n\n    model.compile(\n        optimizer=_keras_optimizer(optimizer, learning_rate=learning_rate), loss=loss_function, metrics=metrics\n    )\n\n    # 3. Train the model\n    # Early stopping callback\n    callbacks = [keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=es_patience)] if early_stopping else []\n\n    history = model.fit(\n        X,\n        y,\n        epochs=epochs,\n        validation_split=validation_split if validation_split else 0.0,\n        validation_data=validation_data,\n        batch_size=batch_size,\n        callbacks=callbacks,\n    )\n\n    return model, history.history\n</code></pre>"},{"location":"prediction/mlp/#eis_toolkit.prediction.mlp.train_MLP_regressor","title":"<code>train_MLP_regressor(X, y, neurons, validation_split=0.2, validation_data=None, activation='relu', output_neurons=1, last_activation='linear', epochs=50, batch_size=32, optimizer='adam', learning_rate=0.001, loss_function='mse', dropout_rate=None, early_stopping=True, es_patience=5, metrics=['mse'], random_state=None)</code>","text":"<p>Train MLP (Multilayer Perceptron) using Keras.</p> <p>Creates a Sequential model with Dense NN layers. For each element in <code>neurons</code>, Dense layer with corresponding dimensionality/neurons is created with the specified activation function (<code>activation</code>). If <code>dropout_rate</code> is specified, a Dropout layer is added after each Dense layer.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Input data. Should be a 2-dimensional array where each row represents a sample and each column a feature. Features should ideally be normalized or standardized.</p> required <code>y</code> <code>ndarray</code> <p>Target labels. Should be a 1-dimensional array where each entry corresponds to the continuous target value for the respective sample in X.</p> required <code>neurons</code> <code>Sequence[int]</code> <p>Number of neurons in each hidden layer.</p> required <code>validation_split</code> <code>Optional[float]</code> <p>Fraction of data used for validation during training. Value must be &gt; 0 and &lt; 1 or None. Defaults to 0.2.</p> <code>0.2</code> <code>validation_data</code> <code>Optional[Tuple[ndarray, ndarray]]</code> <p>Separate dataset used for validation during training. Overrides validation_split if provided. Expected data form is (X_valid, y_valid). Defaults to None.</p> <code>None</code> <code>activation</code> <code>Literal[relu, linear, sigmoid, tanh]</code> <p>Activation function used in each hidden layer. Defaults to 'relu'.</p> <code>'relu'</code> <code>output_neurons</code> <code>int</code> <p>Number of neurons in the output layer. Defaults to 1.</p> <code>1</code> <code>last_activation</code> <code>Literal[linear]</code> <p>Activation function used in the output layer. Defaults to 'linear'.</p> <code>'linear'</code> <code>epochs</code> <code>int</code> <p>Number of epochs to train the model. Defaults to 50.</p> <code>50</code> <code>batch_size</code> <code>int</code> <p>Number of samples per gradient update. Defaults to 32.</p> <code>32</code> <code>optimizer</code> <code>Literal[adam, adagrad, rmsprop, sdg]</code> <p>Optimizer to be used. Defaults to 'adam'.</p> <code>'adam'</code> <code>learning_rate</code> <code>Number</code> <p>Learning rate to be used in training. Value must be &gt; 0. Defalts to 0.001.</p> <code>0.001</code> <code>loss_function</code> <code>Literal[mse, mae, hinge, huber]</code> <p>Loss function to be used. Defaults to 'mse'.</p> <code>'mse'</code> <code>dropout_rate</code> <code>Optional[Number]</code> <p>Fraction of the input units to drop. Value must be &gt;= 0 and &lt;= 1. Defaults to None.</p> <code>None</code> <code>early_stopping</code> <code>bool</code> <p>Whether or not to use early stopping in training. Defaults to True.</p> <code>True</code> <code>es_patience</code> <code>int</code> <p>Number of epochs with no improvement after which training will be stopped. Defaults to 5.</p> <code>5</code> <code>metrics</code> <code>Optional[Sequence[Literal[mse, rmse, mae]]]</code> <p>Metrics to be evaluated by the model during training and testing. Defaults to ['mse'].</p> <code>['mse']</code> <code>random_state</code> <code>Optional[int]</code> <p>Seed for random number generation. Sets Python, Numpy and Tensorflow seeds to make program deterministic. Defaults to None (random state / seed).</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[Model, dict]</code> <p>Trained MLP model and training history.</p> <p>Raises:</p> Type Description <code>InvalidParameterValueException</code> <p>Some of the numeric parameters have invalid values.</p> <code>InvalidDataShapeException</code> <p>Shape of X or y is invalid.</p> Source code in <code>eis_toolkit/prediction/mlp.py</code> <pre><code>@beartype\ndef train_MLP_regressor(\n    X: np.ndarray,\n    y: np.ndarray,\n    neurons: Sequence[int],\n    validation_split: Optional[float] = 0.2,\n    validation_data: Optional[Tuple[np.ndarray, np.ndarray]] = None,\n    activation: Literal[\"relu\", \"linear\", \"sigmoid\", \"tanh\"] = \"relu\",\n    output_neurons: int = 1,\n    last_activation: Literal[\"linear\"] = \"linear\",\n    epochs: int = 50,\n    batch_size: int = 32,\n    optimizer: Literal[\"adam\", \"adagrad\", \"rmsprop\", \"sdg\"] = \"adam\",\n    learning_rate: Number = 0.001,\n    loss_function: Literal[\"mse\", \"mae\", \"hinge\", \"huber\"] = \"mse\",\n    dropout_rate: Optional[Number] = None,\n    early_stopping: bool = True,\n    es_patience: int = 5,\n    metrics: Optional[Sequence[Literal[\"mse\", \"rmse\", \"mae\"]]] = [\"mse\"],\n    random_state: Optional[int] = None,\n) -&gt; Tuple[keras.Model, dict]:\n    \"\"\"\n    Train MLP (Multilayer Perceptron) using Keras.\n\n    Creates a Sequential model with Dense NN layers. For each element in `neurons`, Dense layer with corresponding\n    dimensionality/neurons is created with the specified activation function (`activation`). If `dropout_rate` is\n    specified, a Dropout layer is added after each Dense layer.\n\n    Args:\n        X: Input data. Should be a 2-dimensional array where each row represents a sample and each column a\n            feature. Features should ideally be normalized or standardized.\n        y: Target labels. Should be a 1-dimensional array where each entry corresponds to the continuous\n            target value for the respective sample in X.\n        neurons: Number of neurons in each hidden layer.\n        validation_split: Fraction of data used for validation during training. Value must be &gt; 0 and &lt; 1 or None.\n            Defaults to 0.2.\n        validation_data: Separate dataset used for validation during training. Overrides validation_split if\n            provided. Expected data form is (X_valid, y_valid). Defaults to None.\n        activation: Activation function used in each hidden layer. Defaults to 'relu'.\n        output_neurons: Number of neurons in the output layer. Defaults to 1.\n        last_activation: Activation function used in the output layer. Defaults to 'linear'.\n        epochs: Number of epochs to train the model. Defaults to 50.\n        batch_size: Number of samples per gradient update. Defaults to 32.\n        optimizer: Optimizer to be used. Defaults to 'adam'.\n        learning_rate: Learning rate to be used in training. Value must be &gt; 0. Defalts to 0.001.\n        loss_function: Loss function to be used. Defaults to 'mse'.\n        dropout_rate: Fraction of the input units to drop. Value must be &gt;= 0 and &lt;= 1. Defaults to None.\n        early_stopping: Whether or not to use early stopping in training. Defaults to True.\n        es_patience: Number of epochs with no improvement after which training will be stopped. Defaults to 5.\n        metrics: Metrics to be evaluated by the model during training and testing. Defaults to ['mse'].\n        random_state: Seed for random number generation. Sets Python, Numpy and Tensorflow seeds to make\n            program deterministic. Defaults to None (random state / seed).\n\n    Returns:\n        Trained MLP model and training history.\n\n    Raises:\n        InvalidParameterValueException: Some of the numeric parameters have invalid values.\n        InvalidDataShapeException: Shape of X or y is invalid.\n    \"\"\"\n    # 1. Check input data\n    _check_ML_model_data_input(X=X, y=y)\n    _check_MLP_inputs(\n        neurons=neurons,\n        validation_split=validation_split,\n        learning_rate=learning_rate,\n        dropout_rate=dropout_rate,\n        es_patience=es_patience,\n        batch_size=batch_size,\n        epochs=epochs,\n        output_neurons=output_neurons,\n        loss_function=loss_function,\n    )\n\n    if random_state is not None:\n        keras.utils.set_random_seed(random_state)\n\n    # 2. Create and compile a sequential model\n    model = keras.Sequential()\n\n    model.add(keras.layers.Input(shape=(X.shape[1],)))\n\n    for neuron in neurons:\n        model.add(keras.layers.Dense(units=neuron, activation=activation))\n\n        if dropout_rate is not None:\n            model.add(keras.layers.Dropout(dropout_rate))\n\n    model.add(keras.layers.Dense(units=output_neurons, activation=last_activation))\n\n    model.compile(\n        optimizer=_keras_optimizer(optimizer, learning_rate=learning_rate), loss=loss_function, metrics=metrics\n    )\n\n    # 3. Train the model\n    # Early stopping callback\n    callbacks = [keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=es_patience)] if early_stopping else []\n\n    history = model.fit(\n        X,\n        y,\n        epochs=epochs,\n        validation_split=validation_split if validation_split else 0.0,\n        validation_data=validation_data,\n        batch_size=batch_size,\n        callbacks=callbacks,\n    )\n\n    return model, history.history\n</code></pre>"},{"location":"prediction/random_forests/","title":"Random forests","text":""},{"location":"prediction/random_forests/#eis_toolkit.prediction.random_forests.random_forest_classifier_train","title":"<code>random_forest_classifier_train(X, y, validation_method='split', metrics=['accuracy'], split_size=0.2, cv_folds=5, n_estimators=100, max_depth=None, verbose=0, random_state=None, **kwargs)</code>","text":"<p>Train and optionally validate a Random Forest classifier model using Sklearn.</p> <p>Various options and configurations for model performance evaluation are available. No validation, split to train and validation parts, and cross-validation can be chosen. If validation is performed, metric(s) to calculate can be defined and validation process configured (cross-validation method, number of folds, size of the split). Depending on the details of the validation process, the output metrics dictionary can be empty, one-dimensional or nested.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Union[ndarray, DataFrame]</code> <p>Training data.</p> required <code>y</code> <code>Union[ndarray, Series]</code> <p>Target labels.</p> required <code>validation_method</code> <code>Literal[split, kfold_cv, skfold_cv, loo_cv, none]</code> <p>Validation method to use. \"split\" divides data into two parts, \"kfold_cv\" performs k-fold cross-validation, \"skfold_cv\" performs stratified k-fold cross-validation, \"loo_cv\" performs leave-one-out cross-validation and \"none\" will not validate model at all (in this case, all X and y will be used solely for training).</p> <code>'split'</code> <code>metrics</code> <code>Sequence[Literal[accuracy, precision, recall, f1]]</code> <p>Metrics to use for scoring the model. Defaults to \"accuracy\".</p> <code>['accuracy']</code> <code>split_size</code> <code>float</code> <p>Fraction of the dataset to be used as validation data (rest is used for training). Used only when validation_method is \"split\". Defaults to 0.2.</p> <code>0.2</code> <code>cv_folds</code> <code>int</code> <p>Number of folds used in cross-validation. Used only when validation_method is \"kfold_cv\" or \"skfold_cv\". Defaults to 5.</p> <code>5</code> <code>n_estimators</code> <code>int</code> <p>The number of trees in the forest. Defaults to 100.</p> <code>100</code> <code>max_depth</code> <code>Optional[int]</code> <p>The maximum depth of the tree. Values must be &gt;= 1 or None, in which case nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples. Defaults to None.</p> <code>None</code> <code>verbose</code> <code>int</code> <p>Specifies if modeling progress and performance should be printed. 0 doesn't print, values 1 or above will produce prints.</p> <code>0</code> <code>random_state</code> <code>Optional[int]</code> <p>Seed for random number generation. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional parameters for Sklearn's RandomForestClassifier.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tuple[RandomForestClassifier, dict]</code> <p>The trained RandomForestClassifier and metric scores as a dictionary.</p> <p>Raises:</p> Type Description <code>InvalidParameterValueException</code> <p>If some of the numeric parameters are given invalid input values.</p> <code>NonMatchingParameterLengthsException</code> <p>X and y have mismatching sizes.</p> Source code in <code>eis_toolkit/prediction/random_forests.py</code> <pre><code>@beartype\ndef random_forest_classifier_train(\n    X: Union[np.ndarray, pd.DataFrame],\n    y: Union[np.ndarray, pd.Series],\n    validation_method: Literal[\"split\", \"kfold_cv\", \"skfold_cv\", \"loo_cv\", \"none\"] = \"split\",\n    metrics: Sequence[Literal[\"accuracy\", \"precision\", \"recall\", \"f1\"]] = [\"accuracy\"],\n    split_size: float = 0.2,\n    cv_folds: int = 5,\n    n_estimators: int = 100,\n    max_depth: Optional[int] = None,\n    verbose: int = 0,\n    random_state: Optional[int] = None,\n    **kwargs,\n) -&gt; Tuple[RandomForestClassifier, dict]:\n    \"\"\"\n    Train and optionally validate a Random Forest classifier model using Sklearn.\n\n    Various options and configurations for model performance evaluation are available. No validation,\n    split to train and validation parts, and cross-validation can be chosen. If validation is performed,\n    metric(s) to calculate can be defined and validation process configured (cross-validation method,\n    number of folds, size of the split). Depending on the details of the validation process,\n    the output metrics dictionary can be empty, one-dimensional or nested.\n\n    Args:\n        X: Training data.\n        y: Target labels.\n        validation_method: Validation method to use. \"split\" divides data into two parts, \"kfold_cv\"\n            performs k-fold cross-validation, \"skfold_cv\" performs stratified k-fold cross-validation,\n            \"loo_cv\" performs leave-one-out cross-validation and \"none\" will not validate model at all\n            (in this case, all X and y will be used solely for training).\n        metrics: Metrics to use for scoring the model. Defaults to \"accuracy\".\n        split_size: Fraction of the dataset to be used as validation data (rest is used for training).\n            Used only when validation_method is \"split\". Defaults to 0.2.\n        cv_folds: Number of folds used in cross-validation. Used only when validation_method is \"kfold_cv\"\n            or \"skfold_cv\". Defaults to 5.\n        n_estimators: The number of trees in the forest. Defaults to 100.\n        max_depth: The maximum depth of the tree. Values must be &gt;= 1 or None, in which case nodes are\n            expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n            Defaults to None.\n        verbose: Specifies if modeling progress and performance should be printed. 0 doesn't print,\n            values 1 or above will produce prints.\n        random_state: Seed for random number generation. Defaults to None.\n        **kwargs: Additional parameters for Sklearn's RandomForestClassifier.\n\n    Returns:\n        The trained RandomForestClassifier and metric scores as a dictionary.\n\n    Raises:\n        InvalidParameterValueException: If some of the numeric parameters are given invalid input values.\n        NonMatchingParameterLengthsException: X and y have mismatching sizes.\n    \"\"\"\n    if not n_estimators &gt;= 1:\n        raise InvalidParameterValueException(\"N-estimators must be at least 1.\")\n    if max_depth is not None and not max_depth &gt;= 1:\n        raise InvalidParameterValueException(\"Max depth must be at least 1 or None.\")\n    if verbose &lt; 0:\n        raise InvalidParameterValueException(\"Verbose must be a non-negative number.\")\n\n    model = RandomForestClassifier(\n        n_estimators=n_estimators, max_depth=max_depth, random_state=random_state, verbose=verbose, **kwargs\n    )\n\n    model, metrics = _train_and_validate_sklearn_model(\n        X=X,\n        y=y,\n        model=model,\n        validation_method=validation_method,\n        metrics=metrics,\n        split_size=split_size,\n        cv_folds=cv_folds,\n        random_state=random_state,\n    )\n\n    return model, metrics\n</code></pre>"},{"location":"prediction/random_forests/#eis_toolkit.prediction.random_forests.random_forest_regressor_train","title":"<code>random_forest_regressor_train(X, y, validation_method='split', metrics=['mse'], split_size=0.2, cv_folds=5, n_estimators=100, max_depth=None, verbose=0, random_state=None, **kwargs)</code>","text":"<p>Train and optionally validate a Random Forest regressor model using Sklearn.</p> <p>Various options and configurations for model performance evaluation are available. No validation, split to train and validation parts, and cross-validation can be chosen. If validation is performed, metric(s) to calculate can be defined and validation process configured (cross-validation method, number of folds, size of the split). Depending on the details of the validation process, the output metrics dictionary can be empty, one-dimensional or nested.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Union[ndarray, DataFrame]</code> <p>Training data.</p> required <code>y</code> <code>Union[ndarray, Series]</code> <p>Target labels.</p> required <code>validation_method</code> <code>Literal[split, kfold_cv, skfold_cv, loo_cv, none]</code> <p>Validation method to use. \"split\" divides data into two parts, \"kfold_cv\" performs k-fold cross-validation, \"skfold_cv\" performs stratified k-fold cross-validation, \"loo_cv\" performs leave-one-out cross-validation and \"none\" will not validate model at all (in this case, all X and y will be used solely for training).</p> <code>'split'</code> <code>metrics</code> <code>Sequence[Literal[mse, rmse, mae, r2]]</code> <p>Metrics to use for scoring the model. Defaults to \"mse\".</p> <code>['mse']</code> <code>split_size</code> <code>float</code> <p>Fraction of the dataset to be used as validation data (rest is used for training). Used only when validation_method is \"split\". Defaults to 0.2.</p> <code>0.2</code> <code>cv_folds</code> <code>int</code> <p>Number of folds used in cross-validation. Used only when validation_method is \"kfold_cv\" or \"skfold_cv\". Defaults to 5.</p> <code>5</code> <code>n_estimators</code> <code>int</code> <p>The number of trees in the forest. Defaults to 100.</p> <code>100</code> <code>max_depth</code> <code>Optional[int]</code> <p>The maximum depth of the tree. Values must be &gt;= 1 or None, in which case nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples. Defaults to None.</p> <code>None</code> <code>verbose</code> <code>int</code> <p>Specifies if modeling progress and performance should be printed. 0 doesn't print, values 1 or above will produce prints.</p> <code>0</code> <code>random_state</code> <code>Optional[int]</code> <p>Seed for random number generation. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional parameters for Sklearn's RandomForestRegressor.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tuple[RandomForestRegressor, dict]</code> <p>The trained RandomForestRegressor and metric scores as a dictionary.</p> <p>Raises:</p> Type Description <code>InvalidParameterValueException</code> <p>If some of the numeric parameters are given invalid input values.</p> <code>NonMatchingParameterLengthsException</code> <p>X and y have mismatching sizes.</p> Source code in <code>eis_toolkit/prediction/random_forests.py</code> <pre><code>@beartype\ndef random_forest_regressor_train(\n    X: Union[np.ndarray, pd.DataFrame],\n    y: Union[np.ndarray, pd.Series],\n    validation_method: Literal[\"split\", \"kfold_cv\", \"skfold_cv\", \"loo_cv\", \"none\"] = \"split\",\n    metrics: Sequence[Literal[\"mse\", \"rmse\", \"mae\", \"r2\"]] = [\"mse\"],\n    split_size: float = 0.2,\n    cv_folds: int = 5,\n    n_estimators: int = 100,\n    max_depth: Optional[int] = None,\n    verbose: int = 0,\n    random_state: Optional[int] = None,\n    **kwargs,\n) -&gt; Tuple[RandomForestRegressor, dict]:\n    \"\"\"\n    Train and optionally validate a Random Forest regressor model using Sklearn.\n\n    Various options and configurations for model performance evaluation are available. No validation,\n    split to train and validation parts, and cross-validation can be chosen. If validation is performed,\n    metric(s) to calculate can be defined and validation process configured (cross-validation method,\n    number of folds, size of the split). Depending on the details of the validation process,\n    the output metrics dictionary can be empty, one-dimensional or nested.\n\n    Args:\n        X: Training data.\n        y: Target labels.\n        validation_method: Validation method to use. \"split\" divides data into two parts, \"kfold_cv\"\n            performs k-fold cross-validation, \"skfold_cv\" performs stratified k-fold cross-validation,\n            \"loo_cv\" performs leave-one-out cross-validation and \"none\" will not validate model at all\n            (in this case, all X and y will be used solely for training).\n        metrics: Metrics to use for scoring the model. Defaults to \"mse\".\n        split_size: Fraction of the dataset to be used as validation data (rest is used for training).\n            Used only when validation_method is \"split\". Defaults to 0.2.\n        cv_folds: Number of folds used in cross-validation. Used only when validation_method is \"kfold_cv\"\n            or \"skfold_cv\". Defaults to 5.\n        n_estimators: The number of trees in the forest. Defaults to 100.\n        max_depth: The maximum depth of the tree. Values must be &gt;= 1 or None, in which case nodes are\n            expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n            Defaults to None.\n        verbose: Specifies if modeling progress and performance should be printed. 0 doesn't print,\n            values 1 or above will produce prints.\n        random_state: Seed for random number generation. Defaults to None.\n        **kwargs: Additional parameters for Sklearn's RandomForestRegressor.\n\n    Returns:\n        The trained RandomForestRegressor and metric scores as a dictionary.\n\n    Raises:\n        InvalidParameterValueException: If some of the numeric parameters are given invalid input values.\n        NonMatchingParameterLengthsException: X and y have mismatching sizes.\n    \"\"\"\n    if not n_estimators &gt;= 1:\n        raise InvalidParameterValueException(\"N-estimators must be at least 1.\")\n    if max_depth is not None and not max_depth &gt;= 1:\n        raise InvalidParameterValueException(\"Max depth must be at least 1 or None.\")\n    if verbose &lt; 0:\n        raise InvalidParameterValueException(\"Verbose must be a non-negative number.\")\n\n    model = RandomForestRegressor(\n        n_estimators=n_estimators, max_depth=max_depth, random_state=random_state, verbose=verbose, **kwargs\n    )\n\n    model, metrics = _train_and_validate_sklearn_model(\n        X=X,\n        y=y,\n        model=model,\n        validation_method=validation_method,\n        metrics=metrics,\n        split_size=split_size,\n        cv_folds=cv_folds,\n        random_state=random_state,\n    )\n\n    return model, metrics\n</code></pre>"},{"location":"prediction/weights_of_evidence/","title":"Weights of evidence","text":""},{"location":"prediction/weights_of_evidence/#eis_toolkit.prediction.weights_of_evidence.weights_of_evidence_calculate_responses","title":"<code>weights_of_evidence_calculate_responses(output_arrays, nr_of_deposits, nr_of_pixels)</code>","text":"<p>Calculate the posterior probabilities for the given generalized weight arrays.</p> <p>Parameters:</p> Name Type Description Default <code>output_arrays</code> <code>Sequence[Dict[str, ndarray]]</code> <p>List of output array dictionaries returned by weights of evidence calculations. For each dictionary, generalized weight and generalized standard deviation arrays are used and summed together pixel-wise to calculate the posterior probabilities. If generalized arrays are not found, the W+ and S_W+ arrays are used (so if outputs from unique weight calculations are used for this function).</p> required <code>nr_of_deposits</code> <code>int</code> <p>Number of deposit pixels in the input data for weights of evidence calculations.</p> required <code>nr_of_pixels</code> <code>int</code> <p>Number of evidence pixels in the input data for weights of evidence calculations.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Array of posterior probabilites.</p> <code>ndarray</code> <p>Array of standard deviations in the posterior probability calculations.</p> <code>ndarray</code> <p>Array of confidence of the prospectivity values obtained in the posterior probability array.</p> Source code in <code>eis_toolkit/prediction/weights_of_evidence.py</code> <pre><code>@beartype\ndef weights_of_evidence_calculate_responses(\n    output_arrays: Sequence[Dict[str, np.ndarray]], nr_of_deposits: int, nr_of_pixels: int\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Calculate the posterior probabilities for the given generalized weight arrays.\n\n    Args:\n        output_arrays: List of output array dictionaries returned by weights of evidence calculations.\n            For each dictionary, generalized weight and generalized standard deviation arrays are used and summed\n            together pixel-wise to calculate the posterior probabilities. If generalized arrays are not found,\n            the W+ and S_W+ arrays are used (so if outputs from unique weight calculations are used for this function).\n        nr_of_deposits: Number of deposit pixels in the input data for weights of evidence calculations.\n        nr_of_pixels: Number of evidence pixels in the input data for weights of evidence calculations.\n\n    Returns:\n        Array of posterior probabilites.\n        Array of standard deviations in the posterior probability calculations.\n        Array of confidence of the prospectivity values obtained in the posterior probability array.\n    \"\"\"\n    gen_weights_sum = sum(\n        [\n            item[GENERALIZED_WEIGHT_PLUS_COLUMN]\n            if GENERALIZED_WEIGHT_PLUS_COLUMN in item.keys()\n            else item[WEIGHT_PLUS_COLUMN]\n            for item in output_arrays\n        ]\n    )\n    gen_weights_variance_sum = sum(\n        [\n            np.square(item[GENERALIZED_S_WEIGHT_PLUS_COLUMN])\n            if GENERALIZED_S_WEIGHT_PLUS_COLUMN in item.keys()\n            else np.square(item[WEIGHT_S_PLUS_COLUMN])\n            for item in output_arrays\n        ]\n    )\n\n    prior_probabilities = nr_of_deposits / nr_of_pixels\n    prior_odds = np.log(prior_probabilities / (1 - prior_probabilities))\n    posterior_probabilities = np.exp(gen_weights_sum + prior_odds) / (1 + np.exp(gen_weights_sum + prior_odds))\n\n    posterior_probabilities_squared = np.square(posterior_probabilities)\n    posterior_probabilities_std = np.sqrt(\n        (1 / nr_of_deposits + gen_weights_variance_sum) * posterior_probabilities_squared\n    )\n\n    confidence_array = posterior_probabilities / posterior_probabilities_std\n    return posterior_probabilities, posterior_probabilities_std, confidence_array\n</code></pre>"},{"location":"prediction/weights_of_evidence/#eis_toolkit.prediction.weights_of_evidence.weights_of_evidence_calculate_weights","title":"<code>weights_of_evidence_calculate_weights(evidential_raster, deposits, raster_nodata=None, weights_type='unique', studentized_contrast_threshold=1, arrays_to_generate=None)</code>","text":"<p>Calculate weights of spatial associations.</p> <p>Parameters:</p> Name Type Description Default <code>evidential_raster</code> <code>DatasetReader</code> <p>The evidential raster.</p> required <code>deposits</code> <code>GeoDataFrame</code> <p>Vector data representing the mineral deposits or occurences point data.</p> required <code>raster_nodata</code> <code>Optional[Number]</code> <p>If nodata value of raster is wanted to specify manually. Optional parameter, defaults to None (nodata from raster metadata is used).</p> <code>None</code> <code>weights_type</code> <code>Literal[unique, categorical, ascending, descending]</code> <p>Accepted values are 'unique', 'categorical', 'ascending' and 'descending'. Unique weights does not create generalized classes and does not use a studentized contrast threshold value while categorical, cumulative ascending and cumulative descending do. Categorical weights are calculated so that all classes with studentized contrast below the defined threshold are grouped into one generalized class. Cumulative ascending and descending weights find the class with max contrast and group classes above/below into generalized classes. Generalized weights are also calculated for generalized classes.</p> <code>'unique'</code> <code>studentized_contrast_threshold</code> <code>Number</code> <p>Studentized contrast threshold value used with 'categorical', 'ascending' and 'descending' weight types. Used either as reclassification threshold directly (categorical) or to check that class with max contrast has studentized contrast value at least the defined value (cumulative). Defaults to 1.</p> <code>1</code> <code>arrays_to_generate</code> <code>Optional[Sequence[str]]</code> <p>Arrays to generate from the computed weight metrics. All column names in the produced weights_df are valid choices. Defaults to [\"Class\", \"W+\", \"S_W+] for \"unique\" weights_type and [\"Class\", \"W+\", \"S_W+\", \"Generalized W+\", \"Generalized S_W+\"] for the cumulative weight types.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Dataframe with weights of spatial association between the input data.</p> <code>dict</code> <p>Dictionary of arrays for specified metrics.</p> <code>dict</code> <p>Raster metadata.</p> <code>int</code> <p>Number of deposit pixels.</p> <code>int</code> <p>Number of all evidence pixels.</p> <p>Raises:</p> Type Description <code>ClassificationFailedException</code> <p>Unable to create generalized classes with the given studentized_contrast_threshold.</p> <code>InvalidColumnException</code> <p>Arrays to generate contains invalid column name(s).</p> <code>InvalidParameterValueException</code> <p>Input weights_type is not one of the accepted values.</p> Source code in <code>eis_toolkit/prediction/weights_of_evidence.py</code> <pre><code>@beartype\ndef weights_of_evidence_calculate_weights(\n    evidential_raster: rasterio.io.DatasetReader,\n    deposits: gpd.GeoDataFrame,\n    raster_nodata: Optional[Number] = None,\n    weights_type: Literal[\"unique\", \"categorical\", \"ascending\", \"descending\"] = \"unique\",\n    studentized_contrast_threshold: Number = 1,\n    arrays_to_generate: Optional[Sequence[str]] = None,\n) -&gt; Tuple[pd.DataFrame, dict, dict, int, int]:\n    \"\"\"\n    Calculate weights of spatial associations.\n\n    Args:\n        evidential_raster: The evidential raster.\n        deposits: Vector data representing the mineral deposits or occurences point data.\n        raster_nodata: If nodata value of raster is wanted to specify manually. Optional parameter, defaults to None\n            (nodata from raster metadata is used).\n        weights_type: Accepted values are 'unique', 'categorical', 'ascending' and 'descending'.\n            Unique weights does not create generalized classes and does not use a studentized contrast threshold value\n            while categorical, cumulative ascending and cumulative descending do. Categorical weights are calculated so\n            that all classes with studentized contrast below the defined threshold are grouped into one generalized\n            class. Cumulative ascending and descending weights find the class with max contrast and group classes\n            above/below into generalized classes. Generalized weights are also calculated for generalized classes.\n        studentized_contrast_threshold: Studentized contrast threshold value used with 'categorical', 'ascending' and\n            'descending' weight types. Used either as reclassification threshold directly (categorical) or to check\n            that class with max contrast has studentized contrast value at least the defined value (cumulative).\n            Defaults to 1.\n        arrays_to_generate: Arrays to generate from the computed weight metrics. All column names\n            in the produced weights_df are valid choices. Defaults to [\"Class\", \"W+\", \"S_W+]\n            for \"unique\" weights_type and [\"Class\", \"W+\", \"S_W+\", \"Generalized W+\", \"Generalized S_W+\"]\n            for the cumulative weight types.\n\n    Returns:\n        Dataframe with weights of spatial association between the input data.\n        Dictionary of arrays for specified metrics.\n        Raster metadata.\n        Number of deposit pixels.\n        Number of all evidence pixels.\n\n    Raises:\n        ClassificationFailedException: Unable to create generalized classes with the given\n            studentized_contrast_threshold.\n        InvalidColumnException: Arrays to generate contains invalid column name(s).\n        InvalidParameterValueException: Input weights_type is not one of the accepted values.\n    \"\"\"\n\n    if arrays_to_generate is None:\n        if weights_type == \"unique\":\n            metrics_to_arrays = DEFAULT_METRICS_UNIQUE\n        else:\n            metrics_to_arrays = DEFAULT_METRICS_CUMULATIVE\n    else:\n        for col_name in arrays_to_generate:\n            if col_name not in VALID_DF_COLUMNS:\n                raise InvalidColumnException(f\"Arrays to generate contains invalid metric / column name: {col_name}.\")\n        metrics_to_arrays = arrays_to_generate.copy()\n\n    # 1. Preprocess data\n    evidence_array = _read_and_preprocess_evidence(evidential_raster, raster_nodata)\n    raster_meta = evidential_raster.meta\n\n    # Rasterize deposits\n    deposit_array, _ = rasterize_vector(\n        geodataframe=deposits, default_value=1.0, base_raster_profile=raster_meta, fill_value=0.0\n    )\n\n    # Mask NaN out of the array\n    nodata_mask = np.isnan(evidence_array)\n    masked_evidence_array = evidence_array[~nodata_mask]\n    masked_deposit_array = deposit_array[~nodata_mask]\n\n    # 2. WofE calculations\n    if weights_type == \"unique\" or weights_type == \"categorical\":\n        wofe_weights = _unique_weights(masked_deposit_array, masked_evidence_array)\n    elif weights_type == \"ascending\":\n        wofe_weights = _cumulative_weights(masked_deposit_array, masked_evidence_array, ascending=True)\n    elif weights_type == \"descending\":\n        wofe_weights = _cumulative_weights(masked_deposit_array, masked_evidence_array, ascending=False)\n    else:\n        raise InvalidParameterValueException(\n            \"Expected weights_type to be one of unique, categorical, ascending or descending.\"\n        )\n\n    # 3. Create DataFrame based on calculated metrics\n    df_entries = []\n    for cls, metrics in wofe_weights.items():\n        metrics = [round(metric, 4) if isinstance(metric, np.floating) else metric for metric in metrics]\n        A, _, C, _, w_plus, s_w_plus, w_minus, s_w_minus, contrast, s_contrast, studentized_contrast = metrics\n        df_entries.append(\n            {\n                CLASS_COLUMN: cls,\n                PIXEL_COUNT_COLUMN: A + C,\n                DEPOSIT_COUNT_COLUMN: A,\n                WEIGHT_PLUS_COLUMN: w_plus,\n                WEIGHT_S_PLUS_COLUMN: s_w_plus,\n                WEIGHT_MINUS_COLUMN: w_minus,\n                WEIGHT_S_MINUS_COLUMN: s_w_minus,\n                CONTRAST_COLUMN: contrast,\n                S_CONTRAST_COLUMN: s_contrast,\n                STUDENTIZED_CONTRAST_COLUMN: studentized_contrast,\n            }\n        )\n    weights_df = pd.DataFrame(df_entries)\n\n    # 4. If we use cumulative weights type, calculate generalized classes and weights\n    if weights_type == \"categorical\":\n        weights_df = _generalized_classes_categorical(weights_df, studentized_contrast_threshold)\n        weights_df = _generalized_weights_categorical(weights_df, masked_deposit_array)\n    elif weights_type == \"ascending\" or weights_type == \"descending\":\n        weights_df = _generalized_classes_cumulative(weights_df, studentized_contrast_threshold)\n        weights_df = _generalized_weights_cumulative(weights_df, masked_deposit_array)\n\n    # 5. Generate arrays for desired metrics\n    arrays_dict = _generate_arrays_from_metrics(evidence_array, weights_df, metrics_to_arrays)\n\n    # Return nr. of deposit pixels  and nr. of all evidence pixels for to be used in calculate responses\n    nr_of_deposits = int(np.sum(masked_deposit_array == 1))\n    nr_of_pixels = int(np.size(masked_evidence_array))\n\n    return weights_df, arrays_dict, raster_meta, nr_of_deposits, nr_of_pixels\n</code></pre>"},{"location":"raster_processing/clipping/","title":"Clipping","text":""},{"location":"raster_processing/clipping/#eis_toolkit.raster_processing.clipping.clip_raster","title":"<code>clip_raster(raster, geodataframe)</code>","text":"<p>Clips a raster with polygon geometries.</p> <p>Parameters:</p> Name Type Description Default <code>raster</code> <code>DatasetReader</code> <p>The raster to be clipped.</p> required <code>geodataframe</code> <code>GeoDataFrame</code> <p>A geodataframe containing the geometries to do the clipping with. Should contain only polygon features.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>The clipped raster data.</p> <code>dict</code> <p>The updated metadata.</p> <p>Raises:</p> Type Description <code>NonMatchingCrsException</code> <p>The raster and geodataframe are not in the same CRS.</p> <code>GeometryTypeException</code> <p>The input geometries contain non-polygon features.</p> Source code in <code>eis_toolkit/raster_processing/clipping.py</code> <pre><code>@beartype\ndef clip_raster(raster: rasterio.io.DatasetReader, geodataframe: geopandas.GeoDataFrame) -&gt; Tuple[np.ndarray, dict]:\n    \"\"\"Clips a raster with polygon geometries.\n\n    Args:\n        raster: The raster to be clipped.\n        geodataframe: A geodataframe containing the geometries to do the clipping with.\n            Should contain only polygon features.\n\n    Returns:\n        The clipped raster data.\n        The updated metadata.\n\n    Raises:\n        NonMatchingCrsException: The raster and geodataframe are not in the same CRS.\n        GeometryTypeException: The input geometries contain non-polygon features.\n    \"\"\"\n    geometries = geodataframe[\"geometry\"]\n\n    if not check_matching_crs(\n        objects=[raster, geometries],\n    ):\n        raise NonMatchingCrsException(\"The raster and geodataframe are not in the same CRS.\")\n\n    if not check_geometry_types(\n        geometries=geometries,\n        allowed_types=[\"Polygon\", \"MultiPolygon\"],\n    ):\n        raise GeometryTypeException(\"The input geometries contain non-polygon features.\")\n\n    out_image, out_meta = _clip_raster(\n        raster=raster,\n        geometries=geometries,\n    )\n\n    return out_image, out_meta\n</code></pre>"},{"location":"raster_processing/create_constant_raster/","title":"Create constant raster","text":""},{"location":"raster_processing/create_constant_raster/#eis_toolkit.raster_processing.create_constant_raster.create_constant_raster","title":"<code>create_constant_raster(constant_value, template_raster=None, coord_west=None, coord_north=None, coord_east=None, coord_south=None, target_epsg=None, target_pixel_size=None, raster_width=None, raster_height=None, nodata_value=None)</code>","text":"<p>Create a constant raster based on a user-defined value.</p> <p>Provide 3 methods for raster creation: 1. Set extent and coordinate system based on a template raster. 2. Set extent from origin, based on the western and northern coordinates and the pixel size. 3. Set extent from bounds, based on western, northern, eastern and southern points.</p> <p>Always provide values for height and width for the last two options, which correspond to the desired number of pixels for rows and columns.</p> <p>Parameters:</p> Name Type Description Default <code>constant_value</code> <code>Number</code> <p>The constant value to use in the raster.</p> required <code>template_raster</code> <code>Optional[DatasetReader]</code> <p>An optional raster to use as a template for the output.</p> <code>None</code> <code>coord_west</code> <code>Optional[Number]</code> <p>The western coordinate of the output raster in [m].</p> <code>None</code> <code>coord_east</code> <code>Optional[Number]</code> <p>The eastern coordinate of the output raster in [m].</p> <code>None</code> <code>coord_south</code> <code>Optional[Number]</code> <p>The southern coordinate of the output raster in [m].</p> <code>None</code> <code>coord_north</code> <code>Optional[Number]</code> <p>The northern coordinate of the output raster in [m].</p> <code>None</code> <code>target_epsg</code> <code>Optional[int]</code> <p>The EPSG code for the output raster.</p> <code>None</code> <code>target_pixel_size</code> <code>Optional[int]</code> <p>The pixel size of the output raster.</p> <code>None</code> <code>raster_width</code> <code>Optional[int]</code> <p>The width of the output raster.</p> <code>None</code> <code>raster_height</code> <code>Optional[int]</code> <p>The height of the output raster.</p> <code>None</code> <code>nodata_value</code> <code>Optional[Number]</code> <p>The nodata value of the output raster.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, dict]</code> <p>A tuple containing the output raster as a NumPy array and updated metadata.</p> <p>Raises:</p> Type Description <code>InvalidParameterValueException</code> <p>Provide invalid input parameter.</p> Source code in <code>eis_toolkit/raster_processing/create_constant_raster.py</code> <pre><code>@beartype\ndef create_constant_raster(  # type: ignore[no-any-unimported]\n    constant_value: Number,\n    template_raster: Optional[rasterio.io.DatasetReader] = None,\n    coord_west: Optional[Number] = None,\n    coord_north: Optional[Number] = None,\n    coord_east: Optional[Number] = None,\n    coord_south: Optional[Number] = None,\n    target_epsg: Optional[int] = None,\n    target_pixel_size: Optional[int] = None,\n    raster_width: Optional[int] = None,\n    raster_height: Optional[int] = None,\n    nodata_value: Optional[Number] = None,\n) -&gt; Tuple[np.ndarray, dict]:\n    \"\"\"Create a constant raster based on a user-defined value.\n\n    Provide 3 methods for raster creation:\n    1. Set extent and coordinate system based on a template raster.\n    2. Set extent from origin, based on the western and northern coordinates and the pixel size.\n    3. Set extent from bounds, based on western, northern, eastern and southern points.\n\n    Always provide values for height and width for the last two options, which correspond to\n    the desired number of pixels for rows and columns.\n\n    Args:\n        constant_value: The constant value to use in the raster.\n        template_raster: An optional raster to use as a template for the output.\n        coord_west: The western coordinate of the output raster in [m].\n        coord_east: The eastern coordinate of the output raster in [m].\n        coord_south: The southern coordinate of the output raster in [m].\n        coord_north: The northern coordinate of the output raster in [m].\n        target_epsg: The EPSG code for the output raster.\n        target_pixel_size: The pixel size of the output raster.\n        raster_width: The width of the output raster.\n        raster_height: The height of the output raster.\n        nodata_value: The nodata value of the output raster.\n\n    Returns:\n        A tuple containing the output raster as a NumPy array and updated metadata.\n\n    Raises:\n        InvalidParameterValueException: Provide invalid input parameter.\n    \"\"\"\n\n    if template_raster is not None:\n        out_array, out_meta = _create_constant_raster_from_template(constant_value, template_raster, nodata_value)\n\n    elif all(coords is not None for coords in [coord_west, coord_east, coord_south, coord_north]):\n        if raster_height &lt;= 0 or raster_width &lt;= 0:\n            raise InvalidParameterValueException(\"Invalid raster extent provided.\")\n        if not check_minmax_position((coord_west, coord_east) or not check_minmax_position((coord_south, coord_north))):\n            raise InvalidParameterValueException(\"Invalid coordinate values provided.\")\n\n        out_array, out_meta = _create_constant_raster_from_bounds(\n            constant_value,\n            coord_west,\n            coord_north,\n            coord_east,\n            coord_south,\n            target_epsg,\n            raster_width,\n            raster_height,\n            nodata_value,\n        )\n\n    elif all(coords is not None for coords in [coord_west, coord_north]) and all(\n        coords is None for coords in [coord_east, coord_south]\n    ):\n        if raster_height &lt;= 0 or raster_width &lt;= 0:\n            raise InvalidParameterValueException(\"Invalid raster extent provided.\")\n        if target_pixel_size &lt;= 0:\n            raise InvalidParameterValueException(\"Invalid pixel size.\")\n\n        out_array, out_meta = _create_constant_raster_from_origin(\n            constant_value,\n            coord_west,\n            coord_north,\n            target_epsg,\n            target_pixel_size,\n            raster_width,\n            raster_height,\n            nodata_value,\n        )\n\n    else:\n        raise InvalidParameterValueException(\"Suitable parameter values were not provided for any of the 3 methods.\")\n\n    constant_value = cast_scalar_to_int(constant_value)\n    nodata_value = cast_scalar_to_int(out_meta[\"nodata\"])\n\n    if isinstance(constant_value, int) and isinstance(nodata_value, int):\n        target_dtype = np.result_type(get_min_int_type(constant_value), get_min_int_type(nodata_value))\n        out_array = out_array.astype(target_dtype)\n        out_meta[\"dtype\"] = out_array.dtype\n    elif isinstance(constant_value, int) and isinstance(nodata_value, float):\n        out_array = out_array.astype(get_min_int_type(constant_value))\n        out_meta[\"dtype\"] = np.float64.__name__\n    elif isinstance(constant_value, float):\n        out_array = out_array.astype(np.float64)\n        out_meta[\"dtype\"] = out_array.dtype\n\n    return out_array, out_meta\n</code></pre>"},{"location":"raster_processing/extract_values_from_raster/","title":"Extract values from raster","text":""},{"location":"raster_processing/extract_values_from_raster/#eis_toolkit.raster_processing.extract_values_from_raster.extract_values_from_raster","title":"<code>extract_values_from_raster(raster_list, geodataframe, raster_column_names=None)</code>","text":"<p>Extract raster values using point data to a DataFrame.</p> <p>If custom column names are not given, column names are file_name for singleband files    and file_name_bandnumber for multiband files. If custom column names are given, there    should be column names for each raster provided in the raster list.</p> <p>Parameters:</p> Name Type Description Default <code>raster_list</code> <code>Sequence[DatasetReader]</code> <p>List to extract values from.</p> required <code>geodataframe</code> <code>GeoDataFrame</code> <p>Object to extract values with.</p> required <code>raster_column_names</code> <code>Optional[Sequence[str]]</code> <p>List of optional column names for bands.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Dataframe with x &amp; y coordinates and the values from the raster file(s) as columns.</p> <p>Raises:</p> Type Description <code>NonMatchingParameterLengthsException</code> <p>raster_list and raster_columns_names have different lengths.</p> Source code in <code>eis_toolkit/raster_processing/extract_values_from_raster.py</code> <pre><code>@beartype\ndef extract_values_from_raster(\n    raster_list: Sequence[rasterio.io.DatasetReader],\n    geodataframe: gpd.GeoDataFrame,\n    raster_column_names: Optional[Sequence[str]] = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Extract raster values using point data to a DataFrame.\n\n       If custom column names are not given, column names are file_name for singleband files\n       and file_name_bandnumber for multiband files. If custom column names are given, there\n       should be column names for each raster provided in the raster list.\n\n    Args:\n        raster_list: List to extract values from.\n        geodataframe: Object to extract values with.\n        raster_column_names: List of optional column names for bands.\n\n    Returns:\n        Dataframe with x &amp; y coordinates and the values from the raster file(s) as columns.\n\n    Raises:\n        NonMatchingParameterLengthsException: raster_list and raster_columns_names have different lengths.\n    \"\"\"\n    if raster_column_names == []:\n        raster_column_names = None\n\n    if raster_column_names is not None and len(raster_list) != len(raster_column_names):\n        raise NonMatchingParameterLengthsException(\"Raster list and raster columns names have different lengths.\")\n\n    data_frame = _extract_values_from_raster(\n        raster_list=raster_list, geodataframe=geodataframe, raster_column_names=raster_column_names\n    )\n\n    return data_frame\n</code></pre>"},{"location":"raster_processing/reprojecting/","title":"Reprojecting","text":""},{"location":"raster_processing/reprojecting/#eis_toolkit.raster_processing.reprojecting.reproject_raster","title":"<code>reproject_raster(raster, target_crs, resampling_method=warp.Resampling.nearest)</code>","text":"<p>Reprojects raster to match given coordinate reference system (EPSG).</p> <p>Parameters:</p> Name Type Description Default <code>raster</code> <code>DatasetReader</code> <p>The raster to be reprojected.</p> required <code>target_crs</code> <code>int</code> <p>Target CRS as EPSG code.</p> required <code>resampling_method</code> <code>Resampling</code> <p>Resampling method. Most suitable method depends on the dataset and context. Nearest, bilinear and cubic are some common choices. This parameter defaults to nearest.</p> <code>nearest</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The reprojected raster data.</p> <code>dict</code> <p>The updated metadata.</p> <p>Raises:</p> Type Description <code>NonMatchinCrsException</code> <p>Raster is already in the target CRS.</p> Source code in <code>eis_toolkit/raster_processing/reprojecting.py</code> <pre><code>@beartype\ndef reproject_raster(\n    raster: rasterio.io.DatasetReader, target_crs: int, resampling_method: warp.Resampling = warp.Resampling.nearest\n) -&gt; Tuple[np.ndarray, dict]:\n    \"\"\"Reprojects raster to match given coordinate reference system (EPSG).\n\n    Args:\n        raster: The raster to be reprojected.\n        target_crs: Target CRS as EPSG code.\n        resampling_method: Resampling method. Most suitable method depends on the dataset and context.\n            Nearest, bilinear and cubic are some common choices. This parameter defaults to nearest.\n\n    Returns:\n        The reprojected raster data.\n        The updated metadata.\n\n    Raises:\n        NonMatchinCrsException: Raster is already in the target CRS.\n    \"\"\"\n    if target_crs == int(raster.crs.to_string()[5:]):\n        raise MatchingCrsException(\"Raster is already in the target CRS.\")\n\n    out_image, out_meta = _reproject_raster(raster, target_crs, resampling_method)\n\n    return out_image, out_meta\n</code></pre>"},{"location":"raster_processing/resampling/","title":"Resampling","text":""},{"location":"raster_processing/resampling/#eis_toolkit.raster_processing.resampling.resample","title":"<code>resample(raster, resolution, resampling_method=Resampling.bilinear)</code>","text":"<p>Resamples raster according to given resolution.</p> <p>Parameters:</p> Name Type Description Default <code>raster</code> <code>DatasetReader</code> <p>The raster to be resampled.</p> required <code>resolution</code> <code>Number</code> <p>Target resolution i.e. cell size of the output raster.</p> required <code>resampling_method</code> <code>Resampling</code> <p>Resampling method. Most suitable method depends on the dataset and context. Nearest, bilinear and cubic are some common choices. This parameter defaults to bilinear.</p> <code>bilinear</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The resampled raster data.</p> <code>dict</code> <p>The updated metadata.</p> <p>Raises:</p> Type Description <code>NumericValueSignException</code> <p>Resolution is not a positive value.</p> Source code in <code>eis_toolkit/raster_processing/resampling.py</code> <pre><code>@beartype\ndef resample(\n    raster: rasterio.io.DatasetReader,\n    resolution: Number,\n    resampling_method: Resampling = Resampling.bilinear,\n) -&gt; Tuple[np.ndarray, dict]:\n    \"\"\"Resamples raster according to given resolution.\n\n    Args:\n        raster: The raster to be resampled.\n        resolution: Target resolution i.e. cell size of the output raster.\n        resampling_method: Resampling method. Most suitable\n            method depends on the dataset and context. Nearest, bilinear and cubic are some\n            common choices. This parameter defaults to bilinear.\n\n    Returns:\n        The resampled raster data.\n        The updated metadata.\n\n    Raises:\n        NumericValueSignException: Resolution is not a positive value.\n    \"\"\"\n    if resolution &lt;= 0:\n        raise NumericValueSignException(f\"Expected a positive value for resolution: {resolution})\")\n\n    out_image, out_meta = _resample(raster, resolution, resampling_method)\n    return out_image, out_meta\n</code></pre>"},{"location":"raster_processing/snapping/","title":"Snapping","text":""},{"location":"raster_processing/snapping/#eis_toolkit.raster_processing.snapping.snap_with_raster","title":"<code>snap_with_raster(raster, snap_raster)</code>","text":"<p>Snaps/aligns raster to given snap raster.</p> <p>Raster is snapped from its left-bottom corner to nearest snap raster grid corner in left-bottom direction. If rasters are aligned, simply returns input raster data and metadata.</p> <p>Parameters:</p> Name Type Description Default <code>raster</code> <code>DatasetReader</code> <p>The raster to be clipped.</p> required <code>snap_raster</code> <code>DatasetReader</code> <p>The snap raster i.e. reference grid raster.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>The snapped raster data.</p> <code>dict</code> <p>The updated metadata.</p> <p>Raises:</p> Type Description <code>NonMatchingCrsException</code> <p>Raster and and snap raster are not in the same CRS.</p> <code>MatchingRasterGridException</code> <p>Raster grids are already aligned.</p> Source code in <code>eis_toolkit/raster_processing/snapping.py</code> <pre><code>@beartype\ndef snap_with_raster(raster: rasterio.DatasetReader, snap_raster: rasterio.DatasetReader) -&gt; Tuple[np.ndarray, dict]:\n    \"\"\"Snaps/aligns raster to given snap raster.\n\n    Raster is snapped from its left-bottom corner to nearest snap raster grid corner in left-bottom direction.\n    If rasters are aligned, simply returns input raster data and metadata.\n\n    Args:\n        raster: The raster to be clipped.\n        snap_raster: The snap raster i.e. reference grid raster.\n\n    Returns:\n        The snapped raster data.\n        The updated metadata.\n\n    Raises:\n        NonMatchingCrsException: Raster and and snap raster are not in the same CRS.\n        MatchingRasterGridException: Raster grids are already aligned.\n    \"\"\"\n\n    if not check_matching_crs(\n        objects=[raster, snap_raster],\n    ):\n        raise NonMatchingCrsException(\"Raster and and snap raster have different CRS.\")\n\n    if snap_raster.bounds.bottom == raster.bounds.bottom and snap_raster.bounds.left == raster.bounds.left:\n        raise MatchingRasterGridException(\"Raster grids are already aligned.\")\n\n    out_image, out_meta = _snap(raster, snap_raster)\n    return out_image, out_meta\n</code></pre>"},{"location":"raster_processing/unifying/","title":"Unifying","text":""},{"location":"raster_processing/unifying/#eis_toolkit.raster_processing.unifying.unify_raster_grids","title":"<code>unify_raster_grids(base_raster, rasters_to_unify, resampling_method=Resampling.nearest, same_extent=False)</code>","text":"<p>Unifies (reprojects, resamples, aligns and optionally clips) given rasters relative to base raster.</p> <p>Parameters:</p> Name Type Description Default <code>base_raster</code> <code>DatasetReader</code> <p>The base raster to determine target raster grid properties.</p> required <code>rasters_to_unify</code> <code>Sequence[DatasetReader]</code> <p>Rasters to be unified with the base raster.</p> required <code>resampling_method</code> <code>Resampling</code> <p>Resampling method. Most suitable method depends on the dataset and context. Nearest, bilinear and cubic are some common choices. This parameter defaults to nearest.</p> <code>nearest</code> <code>same_extent</code> <code>bool</code> <p>If the unified rasters will be forced to have the same extent/bounds as the base raster. Expands smaller rasters with nodata cells. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>List[Tuple[ndarray, dict]]</code> <p>List of unified rasters' data and metadata. First element is the base raster.</p> <p>Raises:</p> Type Description <code>InvalidParameterValueException</code> <p>Rasters to unify is empty.</p> Source code in <code>eis_toolkit/raster_processing/unifying.py</code> <pre><code>@beartype\ndef unify_raster_grids(\n    base_raster: rasterio.io.DatasetReader,\n    rasters_to_unify: Sequence[rasterio.io.DatasetReader],\n    resampling_method: Resampling = Resampling.nearest,\n    same_extent: bool = False,\n) -&gt; List[Tuple[np.ndarray, dict]]:\n    \"\"\"Unifies (reprojects, resamples, aligns and optionally clips) given rasters relative to base raster.\n\n    Args:\n        base_raster: The base raster to determine target raster grid properties.\n        rasters_to_unify: Rasters to be unified with the base raster.\n        resampling_method: Resampling method. Most suitable\n            method depends on the dataset and context. Nearest, bilinear and cubic are some\n            common choices. This parameter defaults to nearest.\n        same_extent: If the unified rasters will be forced to have the same extent/bounds\n            as the base raster. Expands smaller rasters with nodata cells. Defaults to False.\n\n    Returns:\n        List of unified rasters' data and metadata. First element is the base raster.\n\n    Raises:\n        InvalidParameterValueException: Rasters to unify is empty.\n    \"\"\"\n    if len(rasters_to_unify) == 0:\n        raise InvalidParameterValueException(\"Rasters to unify is empty.\")\n\n    out_rasters = _unify_raster_grids(base_raster, rasters_to_unify, resampling_method, same_extent)\n    return out_rasters\n</code></pre>"},{"location":"raster_processing/unique_combinations/","title":"Unique combinations in rasters","text":""},{"location":"raster_processing/unique_combinations/#eis_toolkit.raster_processing.unique_combinations.unique_combinations","title":"<code>unique_combinations(raster_list)</code>","text":"<p>Get combinations of raster values between rasters.</p> <p>All bands in all rasters are used for analysis. The first band of the first raster is used for reference when making the output.</p> <p>Parameters:</p> Name Type Description Default <code>raster_list</code> <code>Sequence[DatasetReader]</code> <p>Rasters to be used for finding combinations.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Combinations of rasters.</p> <code>dict</code> <p>The metadata of the first raster in raster_list.</p> <p>Raises:</p> Type Description <code>InvalidParameterValueException</code> <p>Input rasters don't have enough bands to perform the operation or input rasters are of different shape.</p> Source code in <code>eis_toolkit/raster_processing/unique_combinations.py</code> <pre><code>@beartype\ndef unique_combinations(  # type: ignore[no-any-unimported]\n    raster_list: Sequence[rasterio.io.DatasetReader],\n) -&gt; Tuple[np.ndarray, dict]:\n    \"\"\"Get combinations of raster values between rasters.\n\n    All bands in all rasters are used for analysis.\n    The first band of the first raster is used for reference when making the output.\n\n    Args:\n        raster_list: Rasters to be used for finding combinations.\n\n    Returns:\n        Combinations of rasters.\n        The metadata of the first raster in raster_list.\n\n    Raises:\n        InvalidParameterValueException: Input rasters don't have enough bands to perform\n            the operation or input rasters are of different shape.\n    \"\"\"\n    bands = []\n    out_meta = raster_list[0].meta\n    out_meta[\"count\"] = 1\n\n    raster_profiles = []\n    for raster in raster_list:\n        for band in range(1, raster.count + 1):\n            bands.append(raster.read(band))\n        raster_profiles.append(raster.profile)\n\n    if len(bands) == 1:\n        raise InvalidParameterValueException(\"Expected to have more bands than 1\")\n\n    if check_raster_grids(raster_profiles) is not True:\n        raise NonMatchingRasterMetadataException(\"Expected raster grids to be have the same grid properties.\")\n\n    out_image = _unique_combinations(bands)\n    return out_image, out_meta\n</code></pre>"},{"location":"raster_processing/windowing/","title":"Windowing","text":""},{"location":"raster_processing/windowing/#eis_toolkit.raster_processing.windowing.extract_window","title":"<code>extract_window(raster, center_coords, height, width)</code>","text":"<p>Extract window from raster.</p> <p>Center coordinate must be inside the raster but window can extent outside the raster in which case padding with    raster nodata value is used. Args:     raster: Source raster.     center_coords: Center coordinates for window in form (x, y). The coordinates should be in the raster's CRS.     height: Window height in pixels.     width: Window width in pixels.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>The extracted raster window.</p> <code>dict</code> <p>The updated metadata.</p> <p>Raises:</p> Type Description <code>InvalidParameterValueException</code> <p>Window size is too small.</p> <code>CoordinatesOutOfBoundException</code> <p>Window center coordinates are out of raster bounds.</p> Source code in <code>eis_toolkit/raster_processing/windowing.py</code> <pre><code>@beartype\ndef extract_window(\n    raster: rasterio.io.DatasetReader,\n    center_coords: Tuple[Number, Number],\n    height: int,\n    width: int,\n) -&gt; Tuple[np.ndarray, dict]:\n    \"\"\"Extract window from raster.\n\n       Center coordinate must be inside the raster but window can extent outside the raster in which case padding with\n       raster nodata value is used.\n    Args:\n        raster: Source raster.\n        center_coords: Center coordinates for window in form (x, y). The coordinates should be in the raster's CRS.\n        height: Window height in pixels.\n        width: Window width in pixels.\n\n    Returns:\n        The extracted raster window.\n        The updated metadata.\n\n    Raises:\n        InvalidParameterValueException: Window size is too small.\n        CoordinatesOutOfBoundException: Window center coordinates are out of raster bounds.\n    \"\"\"\n\n    if height &lt; 1 or width &lt; 1:\n        raise InvalidParameterValueException(f\"Window size is too small: {height}, {width}.\")\n\n    center_x = center_coords[0]\n    center_y = center_coords[1]\n\n    if (\n        center_x &lt; raster.bounds.left\n        or center_x &gt; raster.bounds.right\n        or center_y &lt; raster.bounds.bottom\n        or center_y &gt; raster.bounds.top\n    ):\n        raise CoordinatesOutOfBoundsException(\"Window center coordinates are out of raster bounds.\")\n\n    out_image, out_meta = _extract_window(raster, center_coords, height, width)\n\n    return out_image, out_meta\n</code></pre>"},{"location":"raster_processing/derivatives/classification/","title":"Classification","text":""},{"location":"raster_processing/derivatives/classification/#eis_toolkit.raster_processing.derivatives.classification.classify_aspect","title":"<code>classify_aspect(raster, unit='radians', num_classes=8)</code>","text":"<p>Classify an aspect raster data set.</p> <p>Can classify an aspect raster into 8 or 16 equally spaced directions with intervals of pi/4 and pi/8, respectively.</p> <p>Exemplary for 8 classes, the center of the intervall for North direction is 0\u00b0/360\u00b0 and edges are [337.5\u00b0, 22.5\u00b0], counting forward in clockwise direction. For 16 classes, the intervall-width is half with edges at [348,75\u00b0, 11,25\u00b0].</p> <p>Directions and interval for 8 classes: N: (337.5, 22.5), NE: (22.5, 67.5), E: (67.5, 112.5), SE: (112.5, 157.5), S: (157.5, 202.5), SW: (202.5, 247.5), W: (247.5, 292.5), NW: (292.5, 337.5)</p> <p>Directions and interval for 16 classes: N: (348.75, 11.25), NNE: (11.25, 33.75), NE: (33.75, 56.25), ENE: (56.25, 78.75), E: (78.75, 101.25), ESE: (101.25, 123.75), SE: (123.75, 146.25), SSE: (146.25, 168.75), S: (168.75, 191.25), SSW: (191.25, 213.75), SW: (213.75, 236.25), WSW: (236.25, 258.75), W: (258.75, 281.25), WNW: (281.25, 303.75), NW: (303.75, 326.25), NNW: (326.25, 348.75)</p> <p>Flat pixels (input value: -1) will be kept, the class is called ND (not defined).</p> <p>Parameters:</p> Name Type Description Default <code>raster</code> <code>DatasetReader</code> <p>The input raster data.</p> required <code>unit</code> <code>Literal[radians, degrees]</code> <p>The unit of the input raster. Either \"degrees\" or \"radians\"</p> <code>'radians'</code> <code>num_classes</code> <code>int</code> <p>The number of classes for discretization. Either 8 or 16 classes allowed.</p> <code>8</code> <p>Returns:</p> Type Description <code>tuple[ndarray, dict, dict]</code> <p>The classified aspect raster, a class mapping dictionary and the updated metadata.</p> <p>Raises:</p> Type Description <code>InvalidParameterValueException</code> <p>Invalid number of classes requested.</p> <code>InvalidRasterBandException</code> <p>Input raster has more than one band.</p> Source code in <code>eis_toolkit/raster_processing/derivatives/classification.py</code> <pre><code>@beartype\ndef classify_aspect(\n    raster: rasterio.io.DatasetReader,\n    unit: Literal[\"radians\", \"degrees\"] = \"radians\",\n    num_classes: int = 8,\n) -&gt; tuple[np.ndarray, dict, dict]:\n    \"\"\"\n    Classify an aspect raster data set.\n\n    Can classify an aspect raster into 8 or 16 equally spaced directions with\n    intervals of pi/4 and pi/8, respectively.\n\n    Exemplary for 8 classes, the center of the intervall for North direction is 0\u00b0/360\u00b0\n    and edges are [337.5\u00b0, 22.5\u00b0], counting forward in clockwise direction. For 16 classes,\n    the intervall-width is half with edges at [348,75\u00b0, 11,25\u00b0].\n\n    Directions and interval for 8 classes:\n    N: (337.5, 22.5), NE: (22.5, 67.5),\n    E: (67.5, 112.5), SE: (112.5, 157.5),\n    S: (157.5, 202.5), SW: (202.5, 247.5),\n    W: (247.5, 292.5), NW: (292.5, 337.5)\n\n    Directions and interval for 16 classes:\n    N: (348.75, 11.25), NNE: (11.25, 33.75), NE: (33.75, 56.25), ENE: (56.25, 78.75),\n    E: (78.75, 101.25), ESE: (101.25, 123.75), SE: (123.75, 146.25), SSE: (146.25, 168.75),\n    S: (168.75, 191.25), SSW: (191.25, 213.75), SW: (213.75, 236.25), WSW: (236.25, 258.75),\n    W: (258.75, 281.25), WNW: (281.25, 303.75), NW: (303.75, 326.25), NNW: (326.25, 348.75)\n\n    Flat pixels (input value: -1) will be kept, the class is called ND (not defined).\n\n    Args:\n        raster: The input raster data.\n        unit: The unit of the input raster. Either \"degrees\" or \"radians\"\n        num_classes: The number of classes for discretization. Either 8 or 16 classes allowed.\n\n    Returns:\n        The classified aspect raster, a class mapping dictionary and the updated metadata.\n\n    Raises:\n        InvalidParameterValueException: Invalid number of classes requested.\n        InvalidRasterBandException: Input raster has more than one band.\n    \"\"\"\n\n    if raster.count &gt; 1:\n        raise InvalidRasterBandException(\"Only one-band raster supported.\")\n\n    if num_classes != 8 and num_classes != 16:\n        raise InvalidParameterValueException(\"Only 8 or 16 classes allowed for classification!\")\n\n    return _classify_aspect(raster, unit, num_classes)\n</code></pre>"},{"location":"raster_processing/derivatives/parameters/","title":"Parameters","text":""},{"location":"raster_processing/derivatives/parameters/#eis_toolkit.raster_processing.derivatives.parameters.first_order","title":"<code>first_order(raster, parameters, scaling_factor=1, slope_tolerance=0, slope_gradient_unit='radians', slope_direction_unit='radians', method='Horn')</code>","text":"<p>Calculate the first order surface attributes.</p> <p>For compatibility for slope and aspect calculations with ArcGIS or QGIS, choose Method Horn (1981).</p> <p>Parameters:</p> Name Type Description Default <code>raster</code> <code>DatasetReader</code> <p>Input raster.</p> required <code>parameters</code> <code>Sequence[Literal[G, A]]</code> <p>List of surface parameters to be calculated.</p> required <code>scaling_factor</code> <code>Optional[Number]</code> <p>Scaling factor to be applied to the raster data set. Default to 1.</p> <code>1</code> <code>slope_tolerance</code> <code>Optional[Number]</code> <p>Tolerance value for flat pixels. Default to 0.</p> <code>0</code> <code>slope_gradient_unit</code> <code>Literal[degrees, radians, rise]</code> <p>Unit of the slope gradient parameter. Default to radians.</p> <code>'radians'</code> <code>slope_direction_unit</code> <code>Literal[degrees, radians]</code> <p>Unit of the slope direction parameter. Default to radians.</p> <code>'radians'</code> <code>method</code> <code>Literal[Horn, Evans, Young, Zevenbergen]</code> <p>Method for calculating the coefficients. Default to the Horn (1981) method.</p> <code>'Horn'</code> <p>Returns:</p> Type Description <code>dict</code> <p>Selected surface attributes and respective updated metadata.</p> <p>Raises:</p> Type Description <code>InvalidRasterBandException</code> <p>Raster has more than one band.</p> <code>NonSquarePixelSizeException</code> <p>Pixel dimensions do not have same length.</p> <code>InvalidParameterValueException</code> <p>Wrong input parameters provided.</p> Source code in <code>eis_toolkit/raster_processing/derivatives/parameters.py</code> <pre><code>@beartype\ndef first_order(\n    raster: rasterio.io.DatasetReader,\n    parameters: Sequence[Literal[\"G\", \"A\"]],\n    scaling_factor: Optional[Number] = 1,\n    slope_tolerance: Optional[Number] = 0,\n    slope_gradient_unit: Literal[\"degrees\", \"radians\", \"rise\"] = \"radians\",\n    slope_direction_unit: Literal[\"degrees\", \"radians\"] = \"radians\",\n    method: Literal[\"Horn\", \"Evans\", \"Young\", \"Zevenbergen\"] = \"Horn\",\n) -&gt; dict:\n    \"\"\"Calculate the first order surface attributes.\n\n    For compatibility for slope and aspect calculations with ArcGIS or QGIS, choose Method Horn (1981).\n\n    Args:\n        raster: Input raster.\n        parameters: List of surface parameters to be calculated.\n        scaling_factor: Scaling factor to be applied to the raster data set. Default to 1.\n        slope_tolerance: Tolerance value for flat pixels. Default to 0.\n        slope_gradient_unit: Unit of the slope gradient parameter. Default to radians.\n        slope_direction_unit: Unit of the slope direction parameter. Default to radians.\n        method: Method for calculating the coefficients. Default to the Horn (1981) method.\n\n    Returns:\n        Selected surface attributes and respective updated metadata.\n\n    Raises:\n        InvalidRasterBandException: Raster has more than one band.\n        NonSquarePixelSizeException: Pixel dimensions do not have same length.\n        InvalidParameterValueException: Wrong input parameters provided.\n    \"\"\"\n    if raster.count &gt; 1:\n        raise InvalidRasterBandException(\"Only one-band raster supported.\")\n\n    if check_quadratic_pixels(raster) is False:\n        raise NonSquarePixelSizeException(\"Processing requires quadratic pixel dimensions.\")\n\n    if scaling_factor &lt;= 0:\n        raise InvalidParameterValueException(\"Value must be greater than 0.\")\n\n    raster_array = raster.read()\n    raster_array = reduce_ndim(raster_array)\n    raster_array = nodata_to_nan(raster_array, nodata_value=raster.nodata)\n    raster_array = _scale_raster(raster_array, scaling_factor)\n\n    cellsize = raster.res[0]\n    p, q, *_ = _coefficients(raster_array, cellsize, method)\n    q = -q if method == \"Horn\" else q\n\n    slope_gradient = _first_order(\"G\", (p, q)) if slope_tolerance &gt; 0 else (p, q)\n\n    out_dict = {}\n    out_nodata = -9999\n    for parameter in parameters:\n        out_array = (\n            slope_gradient\n            if parameter == \"G\" and isinstance(slope_gradient, np.ndarray)\n            else _first_order(parameter, (p, q))\n        )\n\n        if (parameter == \"G\" and slope_gradient_unit == \"degrees\") or (\n            parameter == \"A\" and slope_direction_unit == \"degrees\"\n        ):\n            out_array = convert_rad_to_deg(out_array)\n        elif parameter == \"G\" and slope_gradient_unit == \"rise\":\n            out_array = convert_rad_to_rise(out_array)\n\n        out_array = (\n            _set_flat_pixels(out_array, slope_gradient, slope_tolerance, parameter) if parameter != \"G\" else out_array\n        )\n\n        out_array = nan_to_nodata(out_array, nodata_value=out_nodata).astype(np.float32)\n        out_meta = raster.meta.copy()\n        out_meta.update({\"dtype\": out_array.dtype.name, \"nodata\": out_nodata})\n        out_dict[parameter] = (out_array, out_meta)\n\n    return out_dict\n</code></pre>"},{"location":"raster_processing/derivatives/parameters/#eis_toolkit.raster_processing.derivatives.parameters.second_order_basic_set","title":"<code>second_order_basic_set(raster, parameters, scaling_factor=1, slope_tolerance=0, method='Young')</code>","text":"<p>Calculate the second order surface attributes.</p> References <p>Young, M., 1978: Terrain analysis program documentation. Report 5 on Grant DA-ERO-591-73-G0040, 'Statistical characterization of altitude matrices by computer'. Department of Geography, University of Durham, England: 27 pp.</p> <p>Zevenbergen, L.W. and Thorne, C.R., 1987: Quantitative analysis of land surface topography, Earth Surface Processes and Landforms, 12: 47-56.</p> <p>Wood, J., 1996: The Geomorphological Characterisation of Digital Elevation Models. Doctoral Thesis. Department of Geography, University of Leicester, England: 466 pp.</p> <p>Parameters longc and crosc from are referenced by Zevenbergen &amp; Thorne (1987) as profile and plan curvature. For compatibility with ArcGIS, choose Method Zevenbergen &amp; Thorne (1987) and parameters longc and crosc.</p> <p>Parameters:</p> Name Type Description Default <code>raster</code> <code>DatasetReader</code> <p>Input raster.</p> required <code>parameters</code> <code>Sequence[Literal[planc, profc, profc_min, profc_max, longc, crosc, rot, K, genc, tangc]]</code> <p>List of surface parameters to be calculated.</p> required <code>scaling_factor</code> <code>Optional[Number]</code> <p>Scaling factor to be applied to the raster data set. Default to 1.</p> <code>1</code> <code>slope_tolerance</code> <code>Optional[Number]</code> <p>Tolerance value for flat pixels. Default to 0.</p> <code>0</code> <code>method</code> <code>Literal[Evans, Young, Zevenbergen]</code> <p>Method for calculating the coefficients. Default to the Young (1978) method.</p> <code>'Young'</code> <p>Returns:</p> Type Description <code>dict</code> <p>Selected surface attributes and respective updated metadata.</p> <p>Raises:</p> Type Description <code>InvalidRasterBandException</code> <p>Raster has more than one band.</p> <code>NonSquarePixelSizeException</code> <p>Pixel dimensions do not have same length.</p> <code>InvalidParameterValueException</code> <p>Wrong input parameters provided.</p> Source code in <code>eis_toolkit/raster_processing/derivatives/parameters.py</code> <pre><code>@beartype\ndef second_order_basic_set(\n    raster: rasterio.io.DatasetReader,\n    parameters: Sequence[\n        Literal[\n            \"planc\",\n            \"profc\",\n            \"profc_min\",\n            \"profc_max\",\n            \"longc\",\n            \"crosc\",\n            \"rot\",\n            \"K\",\n            \"genc\",\n            \"tangc\",\n        ]\n    ],\n    scaling_factor: Optional[Number] = 1,\n    slope_tolerance: Optional[Number] = 0,\n    method: Literal[\"Evans\", \"Young\", \"Zevenbergen\"] = \"Young\",\n) -&gt; dict:\n    \"\"\"Calculate the second order surface attributes.\n\n    References:\n        Young, M., 1978: Terrain analysis program documentation. Report 5 on Grant DA-ERO-591-73-G0040,\n        'Statistical characterization of altitude matrices by computer'. Department of Geography,\n        University of Durham, England: 27 pp.\n\n        Zevenbergen, L.W. and Thorne, C.R., 1987: Quantitative analysis of land surface topography,\n        Earth Surface Processes and Landforms, 12: 47-56.\n\n        Wood, J., 1996: The Geomorphological Characterisation of Digital Elevation Models. Doctoral Thesis.\n        Department of Geography, University of Leicester, England: 466 pp.\n\n        Parameters longc and crosc from are referenced by Zevenbergen &amp; Thorne (1987) as profile and plan curvature.\n        For compatibility with ArcGIS, choose Method Zevenbergen &amp; Thorne (1987) and parameters longc and crosc.\n\n    Args:\n        raster: Input raster.\n        parameters: List of surface parameters to be calculated.\n        scaling_factor: Scaling factor to be applied to the raster data set. Default to 1.\n        slope_tolerance: Tolerance value for flat pixels. Default to 0.\n        method: Method for calculating the coefficients. Default to the Young (1978) method.\n\n    Returns:\n        Selected surface attributes and respective updated metadata.\n\n    Raises:\n        InvalidRasterBandException: Raster has more than one band.\n        NonSquarePixelSizeException: Pixel dimensions do not have same length.\n        InvalidParameterValueException: Wrong input parameters provided.\n    \"\"\"\n    if raster.count &gt; 1:\n        raise InvalidRasterBandException(\"Only one-band raster supported.\")\n\n    if check_quadratic_pixels(raster) is False:\n        raise NonSquarePixelSizeException(\"Processing requires quadratic pixel dimensions.\")\n\n    if scaling_factor &lt;= 0:\n        raise InvalidParameterValueException(\"Value must be greater than 0.\")\n\n    raster_array = raster.read()\n    raster_array = reduce_ndim(raster_array)\n    raster_array = nodata_to_nan(raster_array, nodata_value=raster.nodata)\n    raster_array = _scale_raster(raster_array, scaling_factor)\n\n    cellsize = raster.res[0]\n    p, q, r, s, t = _coefficients(raster_array, cellsize, method)\n    slope_gradient = _first_order(\"G\", (p, q)) if slope_tolerance &gt; 0 else (p, q)\n\n    out_dict = {}\n    out_nodata = -9999\n    for parameter in parameters:\n        out_array = _second_order_basic_set(parameter, (p, q, r, s, t))\n        out_array = _set_flat_pixels(out_array, slope_gradient, slope_tolerance, parameter)\n        out_array = nan_to_nodata(out_array, nodata_value=out_nodata).astype(np.float32)\n        out_meta = raster.meta.copy()\n        out_meta.update({\"dtype\": out_array.dtype.name, \"nodata\": out_nodata})\n        out_dict[parameter] = (out_array, out_meta)\n\n    return out_dict\n</code></pre>"},{"location":"raster_processing/derivatives/partial_derivatives/","title":"Partial derivatives","text":""},{"location":"raster_processing/derivatives/utilities/","title":"Utilities","text":""},{"location":"raster_processing/filters/focal/","title":"Focal","text":""},{"location":"raster_processing/filters/focal/#eis_toolkit.raster_processing.filters.focal.focal_filter","title":"<code>focal_filter(raster, method='mean', size=3, shape='circle')</code>","text":"<p>Apply a basic focal filter to the input raster.</p> <p>Parameters:</p> Name Type Description Default <code>raster</code> <code>DatasetReader</code> <p>The input raster dataset.</p> required <code>method</code> <code>Literal[mean, median]</code> <p>The method to use for filtering. Can be either \"mean\" or \"median\". Default to \"mean\".</p> <code>'mean'</code> <code>size</code> <code>int</code> <p>The size of the filter window. E.g., 3 means a 3x3 window. Default to 3.</p> <code>3</code> <code>shape</code> <code>Literal[square, circle]</code> <p>The shape of the filter window. Can be either \"square\" or \"circle\". Default to \"circle\".</p> <code>'circle'</code> <p>Returns:</p> Type Description <code>tuple[ndarray, dict]</code> <p>The filtered raster array.</p> <p>Raises:</p> Type Description <code>InvalidRasterBandException</code> <p>If the input raster has more than one band.</p> <code>InvalidParameterValueException</code> <p>If the filter size is smaller than 3. If the filter size is not an odd number. If the shape is not \"square\" or \"circle\".</p> Source code in <code>eis_toolkit/raster_processing/filters/focal.py</code> <pre><code>@beartype\ndef focal_filter(\n    raster: rasterio.io.DatasetReader,\n    method: Literal[\"mean\", \"median\"] = \"mean\",\n    size: int = 3,\n    shape: Literal[\"square\", \"circle\"] = \"circle\",\n) -&gt; tuple[np.ndarray, dict]:\n    \"\"\"\n    Apply a basic focal filter to the input raster.\n\n    Args:\n        raster: The input raster dataset.\n        method: The method to use for filtering. Can be either \"mean\" or \"median\". Default to \"mean\".\n        size: The size of the filter window. E.g., 3 means a 3x3 window. Default to 3.\n        shape: The shape of the filter window. Can be either \"square\" or \"circle\". Default to \"circle\".\n\n    Returns:\n        The filtered raster array.\n\n    Raises:\n        InvalidRasterBandException: If the input raster has more than one band.\n        InvalidParameterValueException: If the filter size is smaller than 3.\n            If the filter size is not an odd number.\n            If the shape is not \"square\" or \"circle\".\n    \"\"\"\n    _check_inputs(raster, size)\n\n    kernel = _basic_kernel(size, shape)\n\n    raster_array = raster.read()\n    raster_array = reduce_ndim(raster_array)\n    raster_array = nodata_to_nan(raster_array, raster.nodata)\n\n    if method == \"mean\":\n        out_array = _apply_correlated_filter(raster_array, kernel)\n    elif method == \"median\":\n        out_array = _apply_generic_filter(raster_array, _focal_median, kernel)\n\n    out_array = nan_to_nodata(out_array, raster.nodata)\n    out_array = cast_array_to_float(out_array, cast_float=True)\n    out_meta = raster.meta.copy()\n\n    return out_array, out_meta\n</code></pre>"},{"location":"raster_processing/filters/focal/#eis_toolkit.raster_processing.filters.focal.gaussian_filter","title":"<code>gaussian_filter(raster, sigma=1, truncate=4, size=None)</code>","text":"<p>Apply a gaussian filter to the input raster.</p> <p>Parameters:</p> Name Type Description Default <code>raster</code> <code>DatasetReader</code> <p>The input raster dataset.</p> required <code>sigma</code> <code>Number</code> <p>The standard deviation of the gaussian kernel.</p> <code>1</code> <code>truncate</code> <code>Number</code> <p>The truncation factor for the gaussian kernel based on the sigma value. Only if size is not given. Default to 4.0. E.g., for sigma = 1 and truncate = 4.0, the kernel size is 9x9.</p> <code>4</code> <code>size</code> <code>Optional[int]</code> <p>The size of the filter window. E.g., 3 means a 3x3 window. If size is not None, it overrides the dynamic size calculation based on sigma and truncate. Default to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[ndarray, dict]</code> <p>The filtered raster array.</p> <p>Raises:</p> Type Description <code>InvalidRasterBandException</code> <p>If the input raster has more than one band.</p> <code>InvalidParameterValueException</code> <p>If the filter size is smaller than 3. If the filter size is not an odd number. If the resulting radius is smaller than 1.</p> Source code in <code>eis_toolkit/raster_processing/filters/focal.py</code> <pre><code>@beartype\ndef gaussian_filter(\n    raster: rasterio.io.DatasetReader,\n    sigma: Number = 1,\n    truncate: Number = 4,\n    size: Optional[int] = None,\n) -&gt; tuple[np.ndarray, dict]:\n    \"\"\"\n    Apply a gaussian filter to the input raster.\n\n    Args:\n        raster: The input raster dataset.\n        sigma: The standard deviation of the gaussian kernel.\n        truncate: The truncation factor for the gaussian kernel based on the sigma value.\n            Only if size is not given. Default to 4.0.\n            E.g., for sigma = 1 and truncate = 4.0, the kernel size is 9x9.\n        size: The size of the filter window. E.g., 3 means a 3x3 window.\n            If size is not None, it overrides the dynamic size calculation based on sigma and truncate.\n            Default to None.\n\n    Returns:\n        The filtered raster array.\n\n    Raises:\n        InvalidRasterBandException: If the input raster has more than one band.\n        InvalidParameterValueException: If the filter size is smaller than 3.\n            If the filter size is not an odd number.\n            If the resulting radius is smaller than 1.\n    \"\"\"\n    _check_inputs(raster, size, sigma, truncate)\n\n    kernel = _gaussian_kernel(sigma, truncate, size)\n\n    raster_array = raster.read()\n    raster_array = reduce_ndim(raster_array)\n    raster_array = nodata_to_nan(raster_array, raster.nodata)\n\n    out_array = _apply_correlated_filter(raster_array, kernel)\n    out_array = nan_to_nodata(out_array, raster.nodata)\n\n    out_array = cast_array_to_float(out_array, cast_float=True)\n    out_meta = raster.meta.copy()\n\n    return out_array, out_meta\n</code></pre>"},{"location":"raster_processing/filters/focal/#eis_toolkit.raster_processing.filters.focal.mexican_hat_filter","title":"<code>mexican_hat_filter(raster, sigma=1, truncate=4, size=None, direction='circular')</code>","text":"<p>Apply a mexican hat filter to the input raster.</p> <p>Circular: Lowpass filter for smoothing. Rectangular: Highpass filter for edge detection. Results may need further normalization.</p> <p>Parameters:</p> Name Type Description Default <code>raster</code> <code>DatasetReader</code> <p>The input raster dataset.</p> required <code>sigma</code> <code>Number</code> <p>The standard deviation.</p> <code>1</code> <code>truncate</code> <code>Number</code> <p>The truncation factor. E.g., for sigma = 1 and truncate = 4.0, the kernel size is 9x9. Default to 4.0.</p> <code>4</code> <code>size</code> <code>Optional[int]</code> <p>The size of the filter window. E.g., 3 means a 3x3 window. Default to None.</p> <code>None</code> <code>direction</code> <code>Literal[rectangular, circular]</code> <p>The direction of calculating the kernel values. Can be either \"rectangular\" or \"circular\". Default to \"circular\".</p> <code>'circular'</code> <p>Returns:</p> Type Description <code>tuple[ndarray, dict]</code> <p>The filtered raster array.</p> <p>Raises:</p> Type Description <code>InvalidRasterBandException</code> <p>If the input raster has more than one band.</p> <code>InvalidParameterValueException</code> <p>If the filter size is smaller than 3. If the filter size is not an odd number. If the resulting radius is smaller than 1.</p> Source code in <code>eis_toolkit/raster_processing/filters/focal.py</code> <pre><code>@beartype\ndef mexican_hat_filter(\n    raster: rasterio.io.DatasetReader,\n    sigma: Number = 1,\n    truncate: Number = 4,\n    size: Optional[int] = None,\n    direction: Literal[\"rectangular\", \"circular\"] = \"circular\",\n) -&gt; tuple[np.ndarray, dict]:\n    \"\"\"\n    Apply a mexican hat filter to the input raster.\n\n    Circular: Lowpass filter for smoothing.\n    Rectangular: Highpass filter for edge detection. Results may need further normalization.\n\n    Args:\n        raster: The input raster dataset.\n        sigma: The standard deviation.\n        truncate: The truncation factor.\n            E.g., for sigma = 1 and truncate = 4.0, the kernel size is 9x9.\n            Default to 4.0.\n        size: The size of the filter window. E.g., 3 means a 3x3 window. Default to None.\n        direction: The direction of calculating the kernel values.\n            Can be either \"rectangular\" or \"circular\". Default to \"circular\".\n\n    Returns:\n       The filtered raster array.\n\n    Raises:\n        InvalidRasterBandException: If the input raster has more than one band.\n        InvalidParameterValueException: If the filter size is smaller than 3.\n            If the filter size is not an odd number.\n            If the resulting radius is smaller than 1.\n    \"\"\"\n    _check_inputs(raster, size, sigma, truncate)\n\n    kernel = _mexican_hat_kernel(sigma, truncate, size, direction)\n\n    raster_array = raster.read()\n    raster_array = reduce_ndim(raster_array)\n    raster_array = nodata_to_nan(raster_array, raster.nodata)\n\n    out_array = _apply_correlated_filter(raster_array, kernel)\n    out_array = nan_to_nodata(out_array, raster.nodata)\n\n    out_array = cast_array_to_float(out_array, cast_float=True)\n    out_meta = raster.meta.copy()\n\n    return out_array, out_meta\n</code></pre>"},{"location":"raster_processing/filters/kernels/","title":"Kernels","text":""},{"location":"raster_processing/filters/speckle/","title":"Speckle","text":""},{"location":"raster_processing/filters/speckle/#eis_toolkit.raster_processing.filters.speckle.frost_filter","title":"<code>frost_filter(raster, size=3, damping_factor=1)</code>","text":"<p>Apply a Frost filter to the input raster.</p> <p>Higher damping factor result in better edge preservation.</p> <p>Parameters:</p> Name Type Description Default <code>raster</code> <code>DatasetReader</code> <p>The input raster dataset.</p> required <code>size</code> <code>int</code> <p>The size of the filter window. E.g., 3 means a 3x3 window. Default to 3.</p> <code>3</code> <code>damping_factor</code> <code>Number</code> <p>Extent of exponential damping effect on filtering. Larger damping values preserve edges better but smooths less. Smaller values produce more smoothing. Default to 1.</p> <code>1</code> <p>Returns:</p> Type Description <code>tuple[ndarray, dict]</code> <p>The filtered raster array.</p> <p>Raises:</p> Type Description <code>InvalidRasterBandException</code> <p>If the input raster has more than one band.</p> <code>InvalidParameterValueException</code> <p>If the filter size is smaller than 3. If the filter size is not an odd number.</p> Source code in <code>eis_toolkit/raster_processing/filters/speckle.py</code> <pre><code>@beartype\ndef frost_filter(\n    raster: rasterio.io.DatasetReader,\n    size: int = 3,\n    damping_factor: Number = 1,\n) -&gt; tuple[np.ndarray, dict]:\n    \"\"\"\n    Apply a Frost filter to the input raster.\n\n    Higher damping factor result in better edge preservation.\n\n    Args:\n        raster: The input raster dataset.\n        size: The size of the filter window.\n            E.g., 3 means a 3x3 window. Default to 3.\n        damping_factor: Extent of exponential damping effect on filtering.\n            Larger damping values preserve edges better but smooths less.\n            Smaller values produce more smoothing.\n            Default to 1.\n\n    Returns:\n        The filtered raster array.\n\n    Raises:\n        InvalidRasterBandException: If the input raster has more than one band.\n        InvalidParameterValueException: If the filter size is smaller than 3.\n            If the filter size is not an odd number.\n    \"\"\"\n    _check_inputs(raster, size, damping_factor=damping_factor)\n\n    kernel = np.ones((size, size))\n\n    raster_array = raster.read()\n    raster_array = reduce_ndim(raster_array)\n    raster_array = nodata_to_nan(raster_array, raster.nodata)\n\n    out_array = _apply_generic_filter(raster_array, _frost, kernel, damping_factor)\n    out_array = nan_to_nodata(out_array, raster.nodata)\n\n    out_array = cast_array_to_float(out_array, cast_float=True)\n    out_meta = raster.meta.copy()\n\n    return out_array, out_meta\n</code></pre>"},{"location":"raster_processing/filters/speckle/#eis_toolkit.raster_processing.filters.speckle.gamma_filter","title":"<code>gamma_filter(raster, size=3, n_looks=1)</code>","text":"<p>Apply a Gamma filter to the input raster.</p> <p>Higher number of looks result in better edge preservation.</p> <p>Parameters:</p> Name Type Description Default <code>raster</code> <code>DatasetReader</code> <p>The input raster dataset.</p> required <code>size</code> <code>int</code> <p>The size of the filter window. E.g., 3 means a 3x3 window. Default to 3.</p> <code>3</code> <code>n_looks</code> <code>Number</code> <p>Number of looks to estimate the noise variation. Higher values result in higher smoothing. Low values may result in focal mean filtering. Default to 1.</p> <code>1</code> <p>Returns:</p> Type Description <code>tuple[ndarray, dict]</code> <p>The filtered raster array.</p> <p>Raises:</p> Type Description <code>InvalidRasterBandException</code> <p>If the input raster has more than one band.</p> <code>InvalidParameterValueException</code> <p>If the filter size is smaller than 3. If the filter size is not an odd number.</p> Source code in <code>eis_toolkit/raster_processing/filters/speckle.py</code> <pre><code>@beartype\ndef gamma_filter(\n    raster: rasterio.io.DatasetReader,\n    size: int = 3,\n    n_looks: Number = 1,\n) -&gt; tuple[np.ndarray, dict]:\n    \"\"\"\n    Apply a Gamma filter to the input raster.\n\n    Higher number of looks result in better edge preservation.\n\n    Args:\n        raster: The input raster dataset.\n        size: The size of the filter window.\n            E.g., 3 means a 3x3 window. Default to 3.\n        n_looks: Number of looks to estimate the noise variation.\n            Higher values result in higher smoothing.\n            Low values may result in focal mean filtering.\n            Default to 1.\n\n    Returns:\n        The filtered raster array.\n\n    Raises:\n        InvalidRasterBandException: If the input raster has more than one band.\n        InvalidParameterValueException: If the filter size is smaller than 3.\n            If the filter size is not an odd number.\n    \"\"\"\n    _check_inputs(raster, size, n_looks=n_looks)\n\n    kernel = np.ones((size, size))\n\n    raster_array = raster.read()\n    raster_array = reduce_ndim(raster_array)\n    raster_array = nodata_to_nan(raster_array, raster.nodata)\n\n    out_array = _apply_generic_filter(raster_array, _gamma, kernel, n_looks)\n    out_array = nan_to_nodata(out_array, raster.nodata)\n\n    out_array = cast_array_to_float(out_array, cast_float=True)\n    out_meta = raster.meta.copy()\n\n    return out_array, out_meta\n</code></pre>"},{"location":"raster_processing/filters/speckle/#eis_toolkit.raster_processing.filters.speckle.kuan_filter","title":"<code>kuan_filter(raster, size=3, n_looks=1)</code>","text":"<p>Apply a Kuan filter to the input raster.</p> <p>Higher number of looks result in better edge preservation.</p> <p>Parameters:</p> Name Type Description Default <code>raster</code> <code>DatasetReader</code> <p>The input raster dataset.</p> required <code>size</code> <code>int</code> <p>The size of the filter window. E.g., 3 means a 3x3 window. Default to 3.</p> <code>3</code> <code>n_looks</code> <code>Number</code> <p>Number of looks to estimate the noise variation. Higher values result in higher smoothing. Low values may result in focal mean filtering. Default to 1.</p> <code>1</code> <p>Returns:</p> Type Description <code>tuple[ndarray, dict]</code> <p>The filtered raster array.</p> <p>Raises:</p> Type Description <code>InvalidRasterBandException</code> <p>If the input raster has more than one band.</p> <code>InvalidParameterValueException</code> <p>If the filter size is smaller than 3. If the filter size is not an odd number.</p> Source code in <code>eis_toolkit/raster_processing/filters/speckle.py</code> <pre><code>@beartype\ndef kuan_filter(\n    raster: rasterio.io.DatasetReader,\n    size: int = 3,\n    n_looks: Number = 1,\n) -&gt; tuple[np.ndarray, dict]:\n    \"\"\"\n    Apply a Kuan filter to the input raster.\n\n    Higher number of looks result in better edge preservation.\n\n    Args:\n        raster: The input raster dataset.\n        size: The size of the filter window.\n            E.g., 3 means a 3x3 window. Default to 3.\n        n_looks: Number of looks to estimate the noise variation.\n            Higher values result in higher smoothing.\n            Low values may result in focal mean filtering.\n            Default to 1.\n\n    Returns:\n        The filtered raster array.\n\n    Raises:\n        InvalidRasterBandException: If the input raster has more than one band.\n        InvalidParameterValueException: If the filter size is smaller than 3.\n            If the filter size is not an odd number.\n    \"\"\"\n    _check_inputs(raster, size, n_looks=n_looks)\n\n    kernel = np.ones((size, size))\n\n    raster_array = raster.read()\n    raster_array = reduce_ndim(raster_array)\n    raster_array = nodata_to_nan(raster_array, raster.nodata)\n\n    out_array = _apply_generic_filter(raster_array, _kuan, kernel, n_looks)\n    out_array = nan_to_nodata(out_array, raster.nodata)\n\n    out_array = cast_array_to_float(out_array, cast_float=True)\n    out_meta = raster.meta.copy()\n\n    return out_array, out_meta\n</code></pre>"},{"location":"raster_processing/filters/speckle/#eis_toolkit.raster_processing.filters.speckle.lee_additive_multiplicative_noise_filter","title":"<code>lee_additive_multiplicative_noise_filter(raster, size=3, add_noise_var=0.25, add_noise_mean=0, mult_noise_mean=1)</code>","text":"<p>Apply a Lee filter considering both additive and multiplicative noise components in the input raster.</p> <p>Lower noise values result in better edge preservation.</p> <p>Parameters:</p> Name Type Description Default <code>raster</code> <code>DatasetReader</code> <p>The input raster dataset.</p> required <code>size</code> <code>int</code> <p>The size of the filter window. E.g., 3 means a 3x3 window. Default to 3.</p> <code>3</code> <code>add_noise_var</code> <code>Number</code> <p>The additive noise variation. Default to 0.25.</p> <code>0.25</code> <code>add_noise_mean</code> <code>Number</code> <p>The additive noise mean. Default to 0.</p> <code>0</code> <code>mult_noise_mean</code> <code>Number</code> <p>The multiplative noise mean. Default to 1.</p> <code>1</code> <p>Returns:</p> Type Description <code>tuple[ndarray, dict]</code> <p>The filtered raster array.</p> <p>Raises:</p> Type Description <code>InvalidRasterBandException</code> <p>If the input raster has more than one band.</p> <code>InvalidParameterValueException</code> <p>If the filter size is smaller than 3. If the filter size is not an odd number.</p> Source code in <code>eis_toolkit/raster_processing/filters/speckle.py</code> <pre><code>@beartype\ndef lee_additive_multiplicative_noise_filter(\n    raster: rasterio.io.DatasetReader,\n    size: int = 3,\n    add_noise_var: Number = 0.25,\n    add_noise_mean: Number = 0,\n    mult_noise_mean: Number = 1,\n) -&gt; tuple[np.ndarray, dict]:\n    \"\"\"\n    Apply a Lee filter considering both additive and multiplicative noise components in the input raster.\n\n    Lower noise values result in better edge preservation.\n\n    Args:\n        raster: The input raster dataset.\n        size: The size of the filter window.\n            E.g., 3 means a 3x3 window. Default to 3.\n        add_noise_var: The additive noise variation. Default to 0.25.\n        add_noise_mean: The additive noise mean. Default to 0.\n        mult_noise_mean: The multiplative noise mean. Default to 1.\n\n    Returns:\n        The filtered raster array.\n\n    Raises:\n        InvalidRasterBandException: If the input raster has more than one band.\n        InvalidParameterValueException: If the filter size is smaller than 3.\n            If the filter size is not an odd number.\n    \"\"\"\n    _check_inputs(raster, size)\n\n    kernel = np.ones((size, size))\n\n    raster_array = raster.read()\n    raster_array = reduce_ndim(raster_array)\n    raster_array = nodata_to_nan(raster_array, raster.nodata)\n\n    out_array = _apply_generic_filter(\n        raster_array, _lee_additive_multiplicative_noise, kernel, add_noise_var, add_noise_mean, mult_noise_mean\n    )\n\n    out_array = nan_to_nodata(out_array, raster.nodata)\n    out_array = cast_array_to_float(out_array, cast_float=True)\n    out_meta = raster.meta.copy()\n\n    return out_array, out_meta\n</code></pre>"},{"location":"raster_processing/filters/speckle/#eis_toolkit.raster_processing.filters.speckle.lee_additive_noise_filter","title":"<code>lee_additive_noise_filter(raster, size=3, add_noise_var=0.25)</code>","text":"<p>Apply a Lee filter considering additive noise components in the input raster.</p> <p>Lower noise values result in better edge preservation.</p> <p>Parameters:</p> Name Type Description Default <code>raster</code> <code>DatasetReader</code> <p>The input raster dataset.</p> required <code>size</code> <code>int</code> <p>The size of the filter window. E.g., 3 means a 3x3 window. Default to 3.</p> <code>3</code> <code>add_noise_var</code> <code>Number</code> <p>The additive noise variation. Default to 0.25.</p> <code>0.25</code> <p>Returns:</p> Type Description <code>tuple[ndarray, dict]</code> <p>The filtered raster array.</p> <p>Raises:</p> Type Description <code>InvalidRasterBandException</code> <p>If the input raster has more than one band.</p> <code>InvalidParameterValueException</code> <p>If the filter size is smaller than 3. If the filter size is not an odd number.</p> Source code in <code>eis_toolkit/raster_processing/filters/speckle.py</code> <pre><code>@beartype\ndef lee_additive_noise_filter(\n    raster: rasterio.io.DatasetReader,\n    size: int = 3,\n    add_noise_var: Number = 0.25,\n) -&gt; tuple[np.ndarray, dict]:\n    \"\"\"\n    Apply a Lee filter considering additive noise components in the input raster.\n\n    Lower noise values result in better edge preservation.\n\n    Args:\n        raster: The input raster dataset.\n        size: The size of the filter window.\n            E.g., 3 means a 3x3 window. Default to 3.\n        add_noise_var: The additive noise variation. Default to 0.25.\n\n    Returns:\n        The filtered raster array.\n\n    Raises:\n        InvalidRasterBandException: If the input raster has more than one band.\n        InvalidParameterValueException: If the filter size is smaller than 3.\n            If the filter size is not an odd number.\n    \"\"\"\n    _check_inputs(raster, size)\n\n    kernel = np.ones((size, size))\n\n    raster_array = raster.read()\n    raster_array = reduce_ndim(raster_array)\n\n    raster_array = nodata_to_nan(raster_array, raster.nodata)\n    out_array = _apply_generic_filter(raster_array, _lee_additive_noise, kernel, add_noise_var)\n    out_array = nan_to_nodata(out_array, raster.nodata)\n\n    out_array = cast_array_to_float(out_array, cast_float=True)\n    out_meta = raster.meta.copy()\n\n    return out_array, out_meta\n</code></pre>"},{"location":"raster_processing/filters/speckle/#eis_toolkit.raster_processing.filters.speckle.lee_enhanced_filter","title":"<code>lee_enhanced_filter(raster, size=3, n_looks=1, damping_factor=1)</code>","text":"<p>Apply an enhanced Lee filter to the input raster.</p> <p>Higher number of looks and damping factor result in better edge preservation.</p> <p>Parameters:</p> Name Type Description Default <code>raster</code> <code>DatasetReader</code> <p>The input raster dataset.</p> required <code>size</code> <code>int</code> <p>The size of the filter window. E.g., 3 means a 3x3 window. Default to 3.</p> <code>3</code> <code>n_looks</code> <code>Number</code> <p>Number of looks to estimate the noise variation. Higher values result in higher smoothing. Low values may result in focal mean filtering. Default to 1.</p> <code>1</code> <code>damping_factor</code> <code>Number</code> <p>Extent of exponential damping effect on filtering. Larger damping values preserve edges better but smooths less. Smaller values produce more smoothing. Default to 1.</p> <code>1</code> <p>Returns:</p> Type Description <code>tuple[ndarray, dict]</code> <p>The filtered raster array.</p> <p>Raises:</p> Type Description <code>InvalidRasterBandException</code> <p>If the input raster has more than one band.</p> <code>InvalidParameterValueException</code> <p>If the filter size is smaller than 3. If the filter size is not an odd number.</p> Source code in <code>eis_toolkit/raster_processing/filters/speckle.py</code> <pre><code>@beartype\ndef lee_enhanced_filter(\n    raster: rasterio.io.DatasetReader,\n    size: int = 3,\n    n_looks: Number = 1,\n    damping_factor: Number = 1,\n) -&gt; tuple[np.ndarray, dict]:\n    \"\"\"\n    Apply an enhanced Lee filter to the input raster.\n\n    Higher number of looks and damping factor result in better edge preservation.\n\n    Args:\n        raster: The input raster dataset.\n        size: The size of the filter window.\n            E.g., 3 means a 3x3 window. Default to 3.\n        n_looks: Number of looks to estimate the noise variation.\n            Higher values result in higher smoothing.\n            Low values may result in focal mean filtering.\n            Default to 1.\n        damping_factor: Extent of exponential damping effect on filtering.\n            Larger damping values preserve edges better but smooths less.\n            Smaller values produce more smoothing.\n            Default to 1.\n\n    Returns:\n        The filtered raster array.\n\n    Raises:\n        InvalidRasterBandException: If the input raster has more than one band.\n        InvalidParameterValueException: If the filter size is smaller than 3.\n            If the filter size is not an odd number.\n    \"\"\"\n    _check_inputs(raster, size, n_looks=n_looks, damping_factor=damping_factor)\n\n    kernel = np.ones((size, size))\n\n    raster_array = raster.read()\n    raster_array = reduce_ndim(raster_array)\n    raster_array = nodata_to_nan(raster_array, raster.nodata)\n\n    out_array = _apply_generic_filter(raster_array, _lee_enhanced, kernel, n_looks, damping_factor)\n    out_array = nan_to_nodata(out_array, raster.nodata)\n\n    out_array = cast_array_to_float(out_array, cast_float=True)\n    out_meta = raster.meta.copy()\n\n    return out_array, out_meta\n</code></pre>"},{"location":"raster_processing/filters/speckle/#eis_toolkit.raster_processing.filters.speckle.lee_multiplicative_noise_filter","title":"<code>lee_multiplicative_noise_filter(raster, size=3, mult_noise_mean=1, n_looks=1)</code>","text":"<p>Apply a Lee filter considering multiplicative noise components in the input raster.</p> <p>Higher number of looks result in better edge preservation.</p> <p>Parameters:</p> Name Type Description Default <code>raster</code> <code>DatasetReader</code> <p>The input raster dataset.</p> required <code>size</code> <code>int</code> <p>The size of the filter window. E.g., 3 means a 3x3 window. Default to 3.</p> <code>3</code> <code>mult_noise_mean</code> <code>Number</code> <p>The multiplative noise mean. Default to 1.</p> <code>1</code> <code>n_looks</code> <code>int</code> <p>Number of looks to estimate the noise variation. Higher values result in higher smoothing. Default to 1.</p> <code>1</code> <p>Returns:</p> Type Description <code>tuple[ndarray, dict]</code> <p>The filtered raster array.</p> <p>Raises:</p> Type Description <code>InvalidRasterBandException</code> <p>If the input raster has more than one band.</p> <code>InvalidParameterValueException</code> <p>If the filter size is smaller than 3. If the filter size is not an odd number.</p> Source code in <code>eis_toolkit/raster_processing/filters/speckle.py</code> <pre><code>@beartype\ndef lee_multiplicative_noise_filter(\n    raster: rasterio.io.DatasetReader,\n    size: int = 3,\n    mult_noise_mean: Number = 1,\n    n_looks: int = 1,\n) -&gt; tuple[np.ndarray, dict]:\n    \"\"\"\n    Apply a Lee filter considering multiplicative noise components in the input raster.\n\n    Higher number of looks result in better edge preservation.\n\n    Args:\n        raster: The input raster dataset.\n        size: The size of the filter window.\n            E.g., 3 means a 3x3 window. Default to 3.\n        mult_noise_mean: The multiplative noise mean. Default to 1.\n        n_looks: Number of looks to estimate the noise variation.\n            Higher values result in higher smoothing. Default to 1.\n\n    Returns:\n        The filtered raster array.\n\n    Raises:\n        InvalidRasterBandException: If the input raster has more than one band.\n        InvalidParameterValueException: If the filter size is smaller than 3.\n            If the filter size is not an odd number.\n    \"\"\"\n    _check_inputs(raster, size, n_looks=n_looks)\n\n    kernel = np.ones((size, size))\n\n    raster_array = raster.read()\n    raster_array = reduce_ndim(raster_array)\n    raster_array = nodata_to_nan(raster_array, raster.nodata)\n\n    out_array = _apply_generic_filter(raster_array, _lee_multiplicative_noise, kernel, mult_noise_mean, n_looks)\n    out_array = nan_to_nodata(out_array, raster.nodata)\n\n    out_array = cast_array_to_float(out_array, cast_float=True)\n    out_meta = raster.meta.copy()\n\n    return out_array, out_meta\n</code></pre>"},{"location":"raster_processing/filters/utilities/","title":"Utilities","text":""},{"location":"training_data_tools/class_balancing/","title":"Class balancing","text":""},{"location":"training_data_tools/class_balancing/#eis_toolkit.training_data_tools.class_balancing.balance_SMOTETomek","title":"<code>balance_SMOTETomek(X, y, sampling_strategy='auto', random_state=None)</code>","text":"<p>Balances the classes of input dataset using SMOTETomek resampling method.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Union[DataFrame, ndarray]</code> <p>The feature matrix (input data as a DataFrame).</p> required <code>y</code> <code>Union[Series, ndarray]</code> <p>The target labels corresponding to the feature matrix.</p> required <code>sampling_strategy</code> <code>Union[float, str, dict]</code> <p>Parameter controlling how to perform the resampling. If float, specifies the ratio of samples in minority class to samples of majority class, if str, specifies classes to be resampled (\"minority\", \"not minority\", \"not majority\", \"all\", \"auto\"), if dict, the keys should be targeted classes and values the desired number of samples for the class. Defaults to \"auto\", which will resample all classes except the majority class.</p> <code>'auto'</code> <code>random_state</code> <code>Optional[int]</code> <p>Parameter controlling randomization of the algorithm. Can be given a seed (number). Defaults to None, which randomizes the seed.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[Union[DataFrame, ndarray], Union[Series, ndarray]]</code> <p>Resampled feature matrix and target labels.</p> <p>Raises:</p> Type Description <code>NonMatchingParameterLengthsException</code> <p>If X and y have different length.</p> Source code in <code>eis_toolkit/training_data_tools/class_balancing.py</code> <pre><code>@beartype\ndef balance_SMOTETomek(\n    X: Union[pd.DataFrame, np.ndarray],\n    y: Union[pd.Series, np.ndarray],\n    sampling_strategy: Union[float, str, dict] = \"auto\",\n    random_state: Optional[int] = None,\n) -&gt; tuple[Union[pd.DataFrame, np.ndarray], Union[pd.Series, np.ndarray]]:\n    \"\"\"Balances the classes of input dataset using SMOTETomek resampling method.\n\n    Args:\n        X: The feature matrix (input data as a DataFrame).\n        y: The target labels corresponding to the feature matrix.\n        sampling_strategy: Parameter controlling how to perform the resampling.\n            If float, specifies the ratio of samples in minority class to samples of majority class,\n            if str, specifies classes to be resampled (\"minority\", \"not minority\", \"not majority\", \"all\", \"auto\"),\n            if dict, the keys should be targeted classes and values the desired number of samples for the class.\n            Defaults to \"auto\", which will resample all classes except the majority class.\n        random_state: Parameter controlling randomization of the algorithm. Can be given a seed (number).\n            Defaults to None, which randomizes the seed.\n\n    Returns:\n        Resampled feature matrix and target labels.\n\n    Raises:\n        NonMatchingParameterLengthsException: If X and y have different length.\n    \"\"\"\n\n    if len(X) != len(y):\n        raise NonMatchingParameterLengthsException(\"Feature matrix X and target labels y must have the same length.\")\n\n    X_res, y_res = SMOTETomek(sampling_strategy=sampling_strategy, random_state=random_state).fit_resample(X, y)\n    return X_res, y_res\n</code></pre>"},{"location":"transformations/binarize/","title":"Binarize","text":""},{"location":"transformations/binarize/#eis_toolkit.transformations.binarize.binarize","title":"<code>binarize(raster, bands=None, thresholds=[Number], nodata=None)</code>","text":"<p>Binarize data based on a given threshold.</p> <p>Replaces values less or equal threshold with 0. Replaces values greater than the threshold with 1.</p> <p>Takes one nodata value which will be re-written after transformation.</p> <p>If no band/column selection specified, all bands/columns will be used. If a parameter contains only 1 entry, it will be applied for all bands. The threshold can be set for each band individually.</p> <p>Parameters:</p> Name Type Description Default <code>raster</code> <code>DatasetReader</code> <p>Data object to be transformed.</p> required <code>bands</code> <code>Optional[Sequence[int]]</code> <p>Selection of bands to be transformed.</p> <code>None</code> <code>thresholds</code> <code>Sequence[Number]</code> <p>Threshold values for transformation.</p> <code>[Number]</code> <code>nodata</code> <code>Optional[Number]</code> <p>Nodata value to be considered.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>out_array</code> <code>ndarray</code> <p>The transformed data.</p> <code>out_meta</code> <code>dict</code> <p>Updated metadata.</p> <code>out_settings</code> <code>dict</code> <p>Log of input settings and calculated statistics if available.</p> <p>Raises:</p> Type Description <code>InvalidRasterBandException</code> <p>The input contains invalid band numbers.</p> <code>NonMatchingParameterLengthsException</code> <p>The input does not match the number of selected bands.</p> Source code in <code>eis_toolkit/transformations/binarize.py</code> <pre><code>@beartype\ndef binarize(  # type: ignore[no-any-unimported]\n    raster: rasterio.io.DatasetReader,\n    bands: Optional[Sequence[int]] = None,\n    thresholds: Sequence[Number] = [Number],\n    nodata: Optional[Number] = None,\n) -&gt; Tuple[np.ndarray, dict, dict]:\n    \"\"\"\n    Binarize data based on a given threshold.\n\n    Replaces values less or equal threshold with 0.\n    Replaces values greater than the threshold with 1.\n\n    Takes one nodata value which will be re-written after transformation.\n\n    If no band/column selection specified, all bands/columns will be used.\n    If a parameter contains only 1 entry, it will be applied for all bands.\n    The threshold can be set for each band individually.\n\n    Args:\n        raster: Data object to be transformed.\n        bands: Selection of bands to be transformed.\n        thresholds: Threshold values for transformation.\n        nodata: Nodata value to be considered.\n\n    Returns:\n        out_array: The transformed data.\n        out_meta: Updated metadata.\n        out_settings: Log of input settings and calculated statistics if available.\n\n    Raises:\n        InvalidRasterBandException: The input contains invalid band numbers.\n        NonMatchingParameterLengthsException: The input does not match the number of selected bands.\n    \"\"\"\n    bands = list(range(1, raster.count + 1)) if bands is None else bands\n    nodata = cast_scalar_to_int(raster.nodata if nodata is None else nodata)\n\n    if check_raster_bands(raster, bands) is False:\n        raise InvalidRasterBandException(\"Invalid band selection.\")\n\n    if check_parameter_length(bands, thresholds) is False:\n        raise NonMatchingParameterLengthsException(\"Invalid threshold length.\")\n\n    expanded_args = expand_and_zip(bands, thresholds)\n    thresholds = [element[1] for element in expanded_args]\n\n    out_settings = {}\n\n    for i in range(0, len(bands)):\n        band_array = raster.read(bands[i])\n        inital_dtype = band_array.dtype\n\n        band_mask = np.isin(band_array, nodata)\n        band_array = _binarize(band_array, threshold=thresholds[i])\n        band_array = np.where(band_mask, nodata, band_array)\n\n        if not check_dtype_for_int(nodata):\n            band_array = band_array.astype(inital_dtype)\n        else:\n            band_array = band_array.astype(np.min_scalar_type(nodata))\n\n        band_array = np.expand_dims(band_array, axis=0)\n\n        if i == 0:\n            out_array = band_array.copy()\n        else:\n            out_array = np.vstack((out_array, band_array))\n\n        current_transform = f\"transformation {i + 1}\"\n        current_settings = {\n            \"band_origin\": bands[i],\n            \"threshold\": thresholds[i],\n            \"nodata\": nodata,\n        }\n\n        out_settings[current_transform] = current_settings\n\n    out_meta = raster.meta.copy()\n    out_meta.update({\"count\": len(bands), \"nodata\": nodata, \"dtype\": out_array.dtype.name})\n\n    return out_array, out_meta, out_settings\n</code></pre>"},{"location":"transformations/clip/","title":"Clip","text":""},{"location":"transformations/clip/#eis_toolkit.transformations.clip.clip_transform","title":"<code>clip_transform(raster, limits, bands=None, nodata=None)</code>","text":"<p>Clips data based on specified upper and lower limits.</p> <p>Takes one nodata value that will be ignored in calculations. Replaces values below the lower limit and above the upper limit with provided values, respecively. Works both one-sided and two-sided but raises error if no limits provided.</p> <p>If no band/column selection specified, all bands/columns will be used. If a parameter contains only 1 entry, it will be applied for all bands. The limits can be set for each band individually.</p> <p>Parameters:</p> Name Type Description Default <code>raster</code> <code>DatasetReader</code> <p>Data object to be transformed.</p> required <code>bands</code> <code>Optional[Sequence[int]]</code> <p>Selection of bands to be transformed.</p> <code>None</code> <code>limits</code> <code>Sequence[Tuple[Optional[Number], Optional[Number]]]</code> <p>Lower and upper limits (lower, upper) as real values.</p> required <code>nodata</code> <code>Optional[Number]</code> <p>Nodata value to be considered.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>out_array</code> <code>ndarray</code> <p>The transformed data.</p> <code>out_meta</code> <code>dict</code> <p>Updated metadata.</p> <code>out_settings</code> <code>dict</code> <p>Log of input settings and calculated statistics if available.</p> <p>Raises:</p> Type Description <code>InvalidRasterBandException</code> <p>The input contains invalid band numbers.</p> <code>NonMatchingParameterLengthsException</code> <p>The input does not match the number of selected bands.</p> <code>InvalidParameterValueException</code> <p>The input does not match the requirements (values, order of values).</p> Source code in <code>eis_toolkit/transformations/clip.py</code> <pre><code>@beartype\ndef clip_transform(  # type: ignore[no-any-unimported]\n    raster: rasterio.io.DatasetReader,\n    limits: Sequence[Tuple[Optional[Number], Optional[Number]]],\n    bands: Optional[Sequence[int]] = None,\n    nodata: Optional[Number] = None,\n) -&gt; Tuple[np.ndarray, dict, dict]:\n    \"\"\"\n    Clips data based on specified upper and lower limits.\n\n    Takes one nodata value that will be ignored in calculations.\n    Replaces values below the lower limit and above the upper limit with provided values, respecively.\n    Works both one-sided and two-sided but raises error if no limits provided.\n\n    If no band/column selection specified, all bands/columns will be used.\n    If a parameter contains only 1 entry, it will be applied for all bands.\n    The limits can be set for each band individually.\n\n    Args:\n        raster: Data object to be transformed.\n        bands: Selection of bands to be transformed.\n        limits: Lower and upper limits (lower, upper) as real values.\n        nodata: Nodata value to be considered.\n\n    Returns:\n        out_array: The transformed data.\n        out_meta: Updated metadata.\n        out_settings: Log of input settings and calculated statistics if available.\n\n    Raises:\n        InvalidRasterBandException: The input contains invalid band numbers.\n        NonMatchingParameterLengthsException: The input does not match the number of selected bands.\n        InvalidParameterValueException: The input does not match the requirements (values, order of values).\n    \"\"\"\n    bands = list(range(1, raster.count + 1)) if bands is None else bands\n    nodata = raster.nodata if nodata is None else nodata\n\n    if check_raster_bands(raster, bands) is False:\n        raise InvalidRasterBandException(\"Invalid band selection\")\n\n    if check_parameter_length(bands, limits) is False:\n        raise NonMatchingParameterLengthsException(\"Invalid limit length.\")\n\n    for item in limits:\n        if item.count(None) == len(item):\n            raise InvalidParameterValueException(f\"Limit values all None: {item}.\")\n\n        if not check_minmax_position(item):\n            raise InvalidParameterValueException(f\"Invalid min-max values provided: {item}.\")\n\n    expanded_args = expand_and_zip(bands, limits)\n    limits = [element[1] for element in expanded_args]\n\n    out_settings = {}\n\n    for i in range(0, len(bands)):\n        band_array = raster.read(bands[i])\n        inital_dtype = band_array.dtype\n\n        band_array = cast_array_to_float(band_array, cast_int=True)\n        band_array = nodata_to_nan(band_array, nodata_value=nodata)\n\n        band_array = _clip_transform(band_array, limits=limits[i])\n\n        band_array = nan_to_nodata(band_array, nodata_value=nodata)\n        band_array = cast_array_to_int(band_array, scalar=nodata, initial_dtype=inital_dtype)\n\n        band_array = np.expand_dims(band_array, axis=0)\n\n        if i == 0:\n            out_array = band_array.copy()\n        else:\n            out_array = np.vstack((out_array, band_array))\n\n        current_transform = f\"transformation {i + 1}\"\n        current_settings = {\n            \"band_origin\": bands[i],\n            \"limit_lower\": cast_scalar_to_int(limits[i][0]),\n            \"limit_upper\": cast_scalar_to_int(limits[i][1]),\n            \"nodata\": cast_scalar_to_int(nodata),\n        }\n\n        out_settings[current_transform] = current_settings\n\n    out_meta = raster.meta.copy()\n    out_meta.update({\"count\": len(bands), \"nodata\": nodata, \"dtype\": out_array.dtype.name})\n\n    return out_array, out_meta, out_settings\n</code></pre>"},{"location":"transformations/linear/","title":"Linear","text":""},{"location":"transformations/linear/#eis_toolkit.transformations.linear.min_max_scaling","title":"<code>min_max_scaling(raster, bands=None, new_range=[(0, 1)], nodata=None)</code>","text":"<p>Normalize data based on a specified new range.</p> <p>Uses the provided new minimum and maximum to transform data into the new interval. Takes one nodata value that will be ignored in calculations.</p> <p>If no band/column selection specified, all bands/columns will be used. The new_range can be set for each band individually. If a parameter contains only 1 entry, it will be applied for all bands.</p> <p>Parameters:</p> Name Type Description Default <code>raster</code> <code>DatasetReader</code> <p>Data object to be transformed.</p> required <code>bands</code> <code>Optional[Sequence[int]]</code> <p>Selection of bands to be transformed.</p> <code>None</code> <code>new_range</code> <code>Sequence[Tuple[Number, Number]]</code> <p>The new interval data will be transformed into. First value corresponds to min, second to max.</p> <code>[(0, 1)]</code> <code>nodata</code> <code>Optional[Number]</code> <p>Nodata value to be considered.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>out_array</code> <code>ndarray</code> <p>The transformed data.</p> <code>out_meta</code> <code>dict</code> <p>Updated metadata.</p> <code>out_settings</code> <code>dict</code> <p>Log of input settings and calculated statistics if available.</p> <p>Raises:</p> Type Description <code>InvalidRasterBandException</code> <p>The input contains invalid band numbers.</p> <code>NonMatchingParameterLengthsException</code> <p>The input does not match the number of selected bands.</p> <code>InvalidParameterValueException</code> <p>The input does not match the requirements (values, order of values).</p> Source code in <code>eis_toolkit/transformations/linear.py</code> <pre><code>@beartype\ndef min_max_scaling(  # type: ignore[no-any-unimported]\n    raster: rasterio.io.DatasetReader,\n    bands: Optional[Sequence[int]] = None,\n    new_range: Sequence[Tuple[Number, Number]] = [(0, 1)],\n    nodata: Optional[Number] = None,\n) -&gt; Tuple[np.ndarray, dict, dict]:\n    \"\"\"\n    Normalize data based on a specified new range.\n\n    Uses the provided new minimum and maximum to transform data into the new interval.\n    Takes one nodata value that will be ignored in calculations.\n\n    If no band/column selection specified, all bands/columns will be used.\n    The new_range can be set for each band individually.\n    If a parameter contains only 1 entry, it will be applied for all bands.\n\n    Args:\n        raster: Data object to be transformed.\n        bands: Selection of bands to be transformed.\n        new_range: The new interval data will be transformed into. First value corresponds to min, second to max.\n        nodata: Nodata value to be considered.\n\n    Returns:\n        out_array: The transformed data.\n        out_meta: Updated metadata.\n        out_settings: Log of input settings and calculated statistics if available.\n\n    Raises:\n        InvalidRasterBandException: The input contains invalid band numbers.\n        NonMatchingParameterLengthsException: The input does not match the number of selected bands.\n        InvalidParameterValueException: The input does not match the requirements (values, order of values).\n    \"\"\"\n    bands = list(range(1, raster.count + 1)) if bands is None else bands\n    nodata = raster.nodata if nodata is None else nodata\n\n    if check_raster_bands(raster, bands) is False:\n        raise InvalidRasterBandException(\"Invalid band selection\")\n\n    if check_parameter_length(bands, new_range) is False:\n        raise NonMatchingParameterLengthsException(\"Invalid new_range length\")\n\n    for item in new_range:\n        if not check_minmax_position(item):\n            raise InvalidParameterValueException(f\"Invalid min-max values provided: {item}\")\n\n    expanded_args = expand_and_zip(bands, new_range)\n    new_range = [element[1] for element in expanded_args]\n\n    out_settings = {}\n    out_decimals = set_max_precision()\n\n    for i in range(0, len(bands)):\n        band_array = raster.read(bands[i])\n        band_array = cast_array_to_float(band_array, cast_int=True)\n        band_array = replace_values(band_array, values_to_replace=[nodata, np.inf], replace_value=np.nan)\n\n        band_array = _min_max_scaling(band_array.astype(np.float64), new_range=new_range[i])\n\n        band_array = truncate_decimal_places(band_array, decimal_places=out_decimals)\n        band_array = nan_to_nodata(band_array, nodata_value=nodata)\n        band_array = cast_array_to_float(band_array, scalar=nodata, cast_float=True)\n\n        band_array = np.expand_dims(band_array, axis=0)\n\n        if i == 0:\n            out_array = band_array.copy()\n        else:\n            out_array = np.vstack((out_array, band_array))\n\n        current_transform = f\"transformation {i + 1}\"\n        current_settings = {\n            \"band_origin\": bands[i],\n            \"scaled_min\": new_range[i][0],\n            \"scaled_max\": new_range[i][1],\n            \"nodata\": nodata,\n            \"decimal_places\": out_decimals,\n        }\n\n        out_settings[current_transform] = current_settings\n\n    out_meta = raster.meta.copy()\n    out_meta.update({\"count\": len(bands), \"nodata\": nodata, \"dtype\": out_array.dtype.name})\n\n    return out_array, out_meta, out_settings\n</code></pre>"},{"location":"transformations/linear/#eis_toolkit.transformations.linear.z_score_normalization","title":"<code>z_score_normalization(raster, bands=None, nodata=None)</code>","text":"<p>Normalize data based on mean and standard deviation.</p> <p>Results will have a mean = 0 and standard deviation = 1. Takes one nodata value that will be ignored in calculations.</p> <p>If no band/column selection specified, all bands/columns will be used. If a parameter contains only 1 entry, it will be applied for all bands.</p> <p>Parameters:</p> Name Type Description Default <code>raster</code> <code>DatasetReader</code> <p>Data object to be transformed.</p> required <code>bands</code> <code>Optional[Sequence[int]]</code> <p>Selection of bands to be transformed.</p> <code>None</code> <code>nodata</code> <code>Optional[Number]</code> <p>Nodata value to be considered.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>out_array</code> <code>ndarray</code> <p>The transformed data.</p> <code>out_meta</code> <code>dict</code> <p>Updated metadata.</p> <code>out_settings</code> <code>dict</code> <p>Log of input settings and calculated statistics if available.</p> <p>Raises:</p> Type Description <code>InvalidRasterBandException</code> <p>The input contains invalid band numbers.</p> <code>NonMatchingParameterLengthsException</code> <p>The input does not match the number of selected bands.</p> Source code in <code>eis_toolkit/transformations/linear.py</code> <pre><code>@beartype\ndef z_score_normalization(  # type: ignore[no-any-unimported]\n    raster: rasterio.io.DatasetReader,\n    bands: Optional[Sequence[int]] = None,\n    nodata: Optional[Number] = None,\n) -&gt; Tuple[np.ndarray, dict, dict]:\n    \"\"\"\n    Normalize data based on mean and standard deviation.\n\n    Results will have a mean = 0 and standard deviation = 1.\n    Takes one nodata value that will be ignored in calculations.\n\n    If no band/column selection specified, all bands/columns will be used.\n    If a parameter contains only 1 entry, it will be applied for all bands.\n\n    Args:\n        raster: Data object to be transformed.\n        bands: Selection of bands to be transformed.\n        nodata: Nodata value to be considered.\n\n    Returns:\n        out_array: The transformed data.\n        out_meta: Updated metadata.\n        out_settings: Log of input settings and calculated statistics if available.\n\n    Raises:\n        InvalidRasterBandException: The input contains invalid band numbers.\n        NonMatchingParameterLengthsException: The input does not match the number of selected bands.\n    \"\"\"\n    bands = list(range(1, raster.count + 1)) if bands is None else bands\n    nodata = raster.nodata if nodata is None else nodata\n\n    if check_raster_bands(raster, bands) is False:\n        raise InvalidRasterBandException(\"Invalid band selection.\")\n\n    out_settings = {}\n    out_decimals = set_max_precision()\n\n    for i in range(0, len(bands)):\n        band_array = raster.read(bands[i])\n        band_array = cast_array_to_float(band_array, cast_int=True)\n        band_array = replace_values(band_array, values_to_replace=[nodata, np.inf], replace_value=np.nan)\n\n        band_array, mean_array, sd_array = _z_score_normalization(band_array.astype(np.float64))\n\n        band_array = truncate_decimal_places(band_array, decimal_places=out_decimals)\n        band_array = nan_to_nodata(band_array, nodata_value=nodata)\n        band_array = cast_array_to_float(band_array, scalar=nodata, cast_float=True)\n\n        band_array = np.expand_dims(band_array, axis=0)\n\n        if i == 0:\n            out_array = band_array.copy()\n        else:\n            out_array = np.vstack((out_array, band_array))\n\n        current_transform = f\"transformation {i + 1}\"\n        current_settings = {\n            \"band_origin\": bands[i],\n            \"original_mean\": truncate_decimal_places(mean_array, decimal_places=out_decimals),\n            \"original_sd\": truncate_decimal_places(sd_array, decimal_places=out_decimals),\n            \"nodata\": nodata,\n            \"decimal_places\": out_decimals,\n        }\n\n        out_settings[current_transform] = current_settings\n\n    out_meta = raster.meta.copy()\n    out_meta.update({\"count\": len(bands), \"nodata\": nodata, \"dtype\": out_array.dtype.name})\n\n    return out_array, out_meta, out_settings\n</code></pre>"},{"location":"transformations/logarithmic/","title":"Logarithmic","text":""},{"location":"transformations/logarithmic/#eis_toolkit.transformations.logarithmic.log_transform","title":"<code>log_transform(raster, bands=None, log_transform=['log2'], nodata=None)</code>","text":"<p>Perform a logarithmic transformation on the provided data.</p> <p>Takes one nodata value that will be ignored in calculations. Negative values will not be considered for transformation and replaced by the specific nodata value.</p> <p>If no band/column selection specified, all bands/columns will be used. If a parameter contains only 1 entry, it will be applied for all bands. The log_transform can be set for each band individually.</p> <p>Parameters:</p> Name Type Description Default <code>raster</code> <code>DatasetReader</code> <p>Data object to be transformed.</p> required <code>bands</code> <code>Optional[Sequence[int]]</code> <p>Selection of bands to be transformed.</p> <code>None</code> <code>log_transform</code> <code>Sequence[str]</code> <p>The base for logarithmic transformation. Valid values 'ln', 'log2' and 'log10'.</p> <code>['log2']</code> <code>nodata</code> <code>Optional[Number]</code> <p>Nodata value to be considered.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>out_array</code> <code>ndarray</code> <p>The transformed data.</p> <code>out_meta</code> <code>dict</code> <p>Updated metadata.</p> <code>out_settings</code> <code>dict</code> <p>Log of input settings and calculated statistics if available.</p> <p>Raises:</p> Type Description <code>InvalidRasterBandException</code> <p>The input contains invalid band numbers.</p> <code>NonMatchingParameterLengthsException</code> <p>The input does not match the number of selected bands</p> <code>InvalidParameterValueException</code> <p>The input does not match the requirements (values, order of values)</p> Source code in <code>eis_toolkit/transformations/logarithmic.py</code> <pre><code>@beartype\ndef log_transform(  # type: ignore[no-any-unimported]\n    raster: rasterio.io.DatasetReader,\n    bands: Optional[Sequence[int]] = None,\n    log_transform: Sequence[str] = [\"log2\"],\n    nodata: Optional[Number] = None,\n) -&gt; Tuple[np.ndarray, dict, dict]:\n    \"\"\"\n    Perform a logarithmic transformation on the provided data.\n\n    Takes one nodata value that will be ignored in calculations.\n    Negative values will not be considered for transformation and replaced by the specific nodata value.\n\n    If no band/column selection specified, all bands/columns will be used.\n    If a parameter contains only 1 entry, it will be applied for all bands.\n    The log_transform can be set for each band individually.\n\n    Args:\n        raster: Data object to be transformed.\n        bands: Selection of bands to be transformed.\n        log_transform: The base for logarithmic transformation. Valid values 'ln', 'log2' and 'log10'.\n        nodata: Nodata value to be considered.\n\n    Returns:\n        out_array: The transformed data.\n        out_meta: Updated metadata.\n        out_settings: Log of input settings and calculated statistics if available.\n\n    Raises:\n        InvalidRasterBandException: The input contains invalid band numbers.\n        NonMatchingParameterLengthsException: The input does not match the number of selected bands\n        InvalidParameterValueException: The input does not match the requirements (values, order of values)\n    \"\"\"\n    bands = list(range(1, raster.count + 1)) if bands is None else bands\n    nodata = raster.nodata if nodata is None else nodata\n\n    if check_raster_bands(raster, bands) is False:\n        raise InvalidRasterBandException(\"Invalid band selection\")\n\n    if check_parameter_length(bands, log_transform) is False:\n        raise NonMatchingParameterLengthsException(\"Invalid length for log-base values.\")\n\n    for item in log_transform:\n        if not (item == \"ln\" or item == \"log2\" or item == \"log10\"):\n            raise InvalidParameterValueException(f\"Invalid method: {item}.\")\n\n    expanded_args = expand_and_zip(bands, log_transform)\n    log_transform = [element[1] for element in expanded_args]\n\n    out_settings = {}\n    out_decimals = set_max_precision()\n\n    for i in range(0, len(bands)):\n        band_array = raster.read(bands[i])\n        band_array = cast_array_to_float(band_array, cast_int=True)\n        band_array = replace_values(band_array, values_to_replace=[nodata, np.inf], replace_value=np.nan)\n        band_array[band_array &lt;= 0] = np.nan\n\n        if log_transform[i] == \"ln\":\n            band_array = _log_transform_ln(band_array.astype(np.float64))\n        elif log_transform[i] == \"log2\":\n            band_array = _log_transform_log2(band_array.astype(np.float64))\n        elif log_transform[i] == \"log10\":\n            band_array = _log_transform_log10(band_array.astype(np.float64))\n\n        band_array = truncate_decimal_places(band_array, decimal_places=out_decimals)\n        band_array = nan_to_nodata(band_array, nodata_value=nodata)\n        band_array = cast_array_to_float(band_array, scalar=nodata, cast_float=True)\n\n        band_array = np.expand_dims(band_array, axis=0)\n\n        if i == 0:\n            out_array = band_array.copy()\n        else:\n            out_array = np.vstack((out_array, band_array))\n\n        current_transform = f\"transformation {i + 1}\"\n        current_settings = {\n            \"band_origin\": bands[i],\n            \"log_transform\": log_transform[i],\n            \"nodata\": nodata,\n            \"decimal_places\": out_decimals,\n        }\n\n        out_settings[current_transform] = current_settings\n\n    out_meta = raster.meta.copy()\n    out_meta.update({\"count\": len(bands), \"nodata\": nodata, \"dtype\": out_array.dtype.name})\n\n    return out_array, out_meta, out_settings\n</code></pre>"},{"location":"transformations/one_hot_encoding/","title":"One-hot encoding","text":""},{"location":"transformations/one_hot_encoding/#eis_toolkit.transformations.one_hot_encoding.one_hot_encode","title":"<code>one_hot_encode(data, columns=None, drop_original_columns=True, drop_category=None, sparse_output=True, out_dtype=int, handle_unknown='infrequent_if_exist', min_frequency=None, max_categories=None)</code>","text":"<p>Perform one-hot (or one-of-K or dummy) encoding on categorical data in a DataFrame or NumPy array.</p> <p>This function converts categorical variables into a form that could be provided to machine learning algorithms for better prediction. For each unique category in the feature, a new binary column is created.</p> <p>Continuous data should not be given to this function to avoid excessive amounts of binary features. If input is a DataFrame, continuous data can be excluded from encoding by specifying columns to encode.</p> <p>The function allows control over aspects like handling unknown categories, controlling sparsity of the output, and setting data type of the encoded columns.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[DataFrame, ndarray]</code> <p>Input data as a DataFrame or Numpy array. If a DataFrame is provided, the operation can be restricted to specified columns.</p> required <code>columns</code> <code>Optional[Sequence[str]]</code> <p>Specifies the columns to encode if 'data' is a DataFrame. If None, all columns are considered for encoding. Ignored if 'data' is a Numpy array. Defaults to None.</p> <code>None</code> <code>drop_original_columns</code> <code>bool</code> <p>If True and 'data' is a DataFrame, the original columns being encoded will be dropped from the output. Defaults to True.</p> <code>True</code> <code>drop_category</code> <code>Optional[Literal[first, if_binary]]</code> <p>Specifies a method to drop one of the categories to avoid multicollinearity. 'first' drops the first category, 'if_binary' drops one category only if the feature is binary. If None, no category is dropped. Defaults to None.</p> <code>None</code> <code>sparse_output</code> <code>bool</code> <p>Determines whether the output matrix is sparse or dense. Defaults to True (sparse).</p> <code>True</code> <code>out_dtype</code> <code>Union[type, dtype]</code> <p>Numeric data type of the output. Defaults to int.</p> <code>int</code> <code>handle_unknown</code> <code>Literal[error, ignore, infrequent_if_exist]</code> <p>Specifies how to handle unknown categories encountered during transform. 'error' raises an error, 'ignore' ignores unknown categories, and 'infrequent_if_exist' treats them as infrequent. Defaults to 'infrequent_if_exist'.</p> <code>'infrequent_if_exist'</code> <code>min_frequency</code> <code>Optional[Number]</code> <p>The minimum frequency (as a float or an int) needed to include a category in encoding. Optional parameter. Defaults to None.</p> <code>None</code> <code>max_categories</code> <code>Optional[int]</code> <p>The maximum number of categories to include in encoding. Optional parameter. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[DataFrame, ndarray, csr_matrix]</code> <p>Encoded data as a DataFrame if input was a DataFrame, or as a Numpy array (dense or sparse) if input was a Numpy array.</p> <p>Raises:</p> Type Description <code>EmptyDataFrameException</code> <p>If the input DataFrame is empty.</p> <code>InvalidDatasetException</code> <p>If the input Numpy array is empty.</p> <code>InvalidColumnException</code> <p>If any specified column to encode does not exist in the input DataFrame.</p> Source code in <code>eis_toolkit/transformations/one_hot_encoding.py</code> <pre><code>@beartype\ndef one_hot_encode(\n    data: Union[pd.DataFrame, np.ndarray],\n    columns: Optional[Sequence[str]] = None,\n    drop_original_columns: bool = True,\n    drop_category: Optional[Literal[\"first\", \"if_binary\"]] = None,\n    sparse_output: bool = True,\n    out_dtype: Union[type, np.dtype] = int,\n    handle_unknown: Literal[\"error\", \"ignore\", \"infrequent_if_exist\"] = \"infrequent_if_exist\",\n    min_frequency: Optional[Number] = None,\n    max_categories: Optional[int] = None,\n) -&gt; Union[pd.DataFrame, np.ndarray, sparse._csr.csr_matrix]:\n    \"\"\"\n    Perform one-hot (or one-of-K or dummy) encoding on categorical data in a DataFrame or NumPy array.\n\n    This function converts categorical variables into a form that could be provided to machine learning\n    algorithms for better prediction. For each unique category in the feature, a new binary column is created.\n\n    Continuous data should not be given to this function to avoid excessive amounts of binary features. If input\n    is a DataFrame, continuous data can be excluded from encoding by specifying columns to encode.\n\n    The function allows control over aspects like handling unknown categories, controlling sparsity of the output,\n    and setting data type of the encoded columns.\n\n    Args:\n        data: Input data as a DataFrame or Numpy array. If a DataFrame is provided, the operation can be\n            restricted to specified columns.\n        columns: Specifies the columns to encode if 'data' is a DataFrame. If None, all columns are\n            considered for encoding. Ignored if 'data' is a Numpy array. Defaults to None.\n        drop_original_columns: If True and 'data' is a DataFrame, the original columns being encoded will\n            be dropped from the output. Defaults to True.\n        drop_category: Specifies a method to drop one of the categories to avoid multicollinearity.\n            'first' drops the first category, 'if_binary' drops one category only if the feature is binary.\n            If None, no category is dropped. Defaults to None.\n        sparse_output: Determines whether the output matrix is sparse or dense. Defaults to True (sparse).\n        out_dtype: Numeric data type of the output. Defaults to int.\n        handle_unknown: Specifies how to handle unknown categories encountered during transform. 'error' raises\n            an error, 'ignore' ignores unknown categories, and 'infrequent_if_exist' treats them as infrequent.\n            Defaults to 'infrequent_if_exist'.\n        min_frequency: The minimum frequency (as a float or an int) needed to include a category in encoding.\n            Optional parameter. Defaults to None.\n        max_categories: The maximum number of categories to include in encoding. Optional parameter.\n            Defaults to None.\n\n    Returns:\n        Encoded data as a DataFrame if input was a DataFrame, or as a Numpy array (dense or sparse)\n            if input was a Numpy array.\n\n    Raises:\n        EmptyDataFrameException: If the input DataFrame is empty.\n        InvalidDatasetException: If the input Numpy array is empty.\n        InvalidColumnException: If any specified column to encode does not exist in the input DataFrame.\n    \"\"\"\n    is_dataframe = isinstance(data, pd.DataFrame)\n\n    if is_dataframe:\n        if data.empty:\n            raise EmptyDataFrameException(\"Input DataFrame is empty.\")\n        df = data.copy()\n\n        if columns is not None:\n            if not check_columns_valid(df, columns):\n                raise InvalidColumnException(\"All selected columns were not found in the input DataFrame.\")\n            transform_df = df[columns]\n        else:\n            transform_df = df\n    else:\n        if data.size == 0:\n            raise InvalidDatasetException(\"Input array is empty.\")\n        transform_df = pd.DataFrame(data)\n\n    encoder = OneHotEncoder(\n        drop=drop_category,\n        sparse_output=sparse_output,\n        dtype=out_dtype,\n        handle_unknown=handle_unknown,\n        min_frequency=min_frequency,\n        max_categories=max_categories,\n        feature_name_combiner=lambda feature, category: str(feature) + \"_\" + str(category),\n    )\n\n    # Transform selected columns\n    encoded_data = encoder.fit_transform(transform_df)\n    encoded_cols = encoder.get_feature_names_out(transform_df.columns)\n\n    # If input was a DataFrame, create output DataFrame\n    if is_dataframe:\n        if sparse_output:\n            encoded_df = pd.DataFrame.sparse.from_spmatrix(encoded_data, columns=encoded_cols, index=df.index)\n        else:\n            encoded_df = pd.DataFrame(encoded_data, columns=encoded_cols, index=df.index)\n\n        if drop_original_columns:\n            df = df.drop(transform_df.columns, axis=1)\n\n        encoded_data = pd.concat([df, encoded_df], axis=1)\n\n    return encoded_data\n</code></pre>"},{"location":"transformations/sigmoid/","title":"Sigmoid","text":""},{"location":"transformations/sigmoid/#eis_toolkit.transformations.sigmoid.sigmoid_transform","title":"<code>sigmoid_transform(raster, bands=None, bounds=[(0, 1)], slope=[1], center=True, nodata=None)</code>","text":"<p>Transform data into a sigmoid-shape based on a specified new range.</p> <p>Uses the provided new minimum and maximum, shift and slope parameters to transform the data. Takes one nodata value that will be ignored in calculations.</p> <p>If no band/column selection specified, all bands/columns will be used. If a parameter contains only 1 entry, it will be applied for all bands. The bounds and slope values can be set for each band individually.</p> <p>Parameters:</p> Name Type Description Default <code>raster</code> <code>DatasetReader</code> <p>Data object to be transformed.</p> required <code>bands</code> <code>Optional[Sequence[int]]</code> <p>Selection of bands to be transformed.</p> <code>None</code> <code>bounds</code> <code>Sequence[Tuple[Number, Number]]</code> <p>Boundaries for the calculation of the sigmoid function (lower, upper).</p> <code>[(0, 1)]</code> <code>slope</code> <code>Sequence[Number]</code> <p>Value which modifies the slope of the resulting sigmoid-curve.</p> <code>[1]</code> <code>center</code> <code>bool</code> <p>Center array values around mean = 0 before sigmoid transformation.</p> <code>True</code> <code>nodata</code> <code>Optional[Number]</code> <p>Nodata value to be considered.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>out_array</code> <code>ndarray</code> <p>The transformed data.</p> <code>out_meta</code> <code>dict</code> <p>Updated metadata.</p> <code>out_settings</code> <code>dict</code> <p>Log of input settings and calculated statistics if available.</p> <p>Raises:</p> Type Description <code>InvalidRasterBandException</code> <p>The input contains invalid band numbers.</p> <code>NonMatchingParameterLengthsException</code> <p>The input does not match the number of selected bands.</p> <code>InvalidParameterValueException</code> <p>The input does not match the requirements (values, order of values)</p> Source code in <code>eis_toolkit/transformations/sigmoid.py</code> <pre><code>@beartype\ndef sigmoid_transform(  # type: ignore[no-any-unimported]\n    raster: rasterio.io.DatasetReader,\n    bands: Optional[Sequence[int]] = None,\n    bounds: Sequence[Tuple[Number, Number]] = [(0, 1)],\n    slope: Sequence[Number] = [1],\n    center: bool = True,\n    nodata: Optional[Number] = None,\n) -&gt; Tuple[np.ndarray, dict, dict]:\n    \"\"\"\n    Transform data into a sigmoid-shape based on a specified new range.\n\n    Uses the provided new minimum and maximum, shift and slope parameters to transform the data.\n    Takes one nodata value that will be ignored in calculations.\n\n    If no band/column selection specified, all bands/columns will be used.\n    If a parameter contains only 1 entry, it will be applied for all bands.\n    The bounds and slope values can be set for each band individually.\n\n    Args:\n        raster: Data object to be transformed.\n        bands: Selection of bands to be transformed.\n        bounds: Boundaries for the calculation of the sigmoid function (lower, upper).\n        slope: Value which modifies the slope of the resulting sigmoid-curve.\n        center: Center array values around mean = 0 before sigmoid transformation.\n        nodata: Nodata value to be considered.\n\n    Returns:\n        out_array: The transformed data.\n        out_meta: Updated metadata.\n        out_settings: Log of input settings and calculated statistics if available.\n\n    Raises:\n        InvalidRasterBandException: The input contains invalid band numbers.\n        NonMatchingParameterLengthsException: The input does not match the number of selected bands.\n        InvalidParameterValueException: The input does not match the requirements (values, order of values)\n    \"\"\"\n    bands = list(range(1, raster.count + 1)) if bands is None else bands\n    nodata = raster.nodata if nodata is None else nodata\n\n    if check_raster_bands(raster, bands) is False:\n        raise InvalidRasterBandException(\"Invalid band selection\")\n\n    for parameter_name, parameter in [(\"bounds\", bounds), (\"slope\", slope)]:\n        if check_parameter_length(bands, parameter) is False:\n            raise NonMatchingParameterLengthsException(f\"Invalid length for {parameter_name}.\")\n\n    for item in bounds:\n        if check_minmax_position(item) is False:\n            raise InvalidParameterValueException(f\"Invalid min-max values provided: {item}.\")\n\n    expanded_args = expand_and_zip(bands, bounds, slope)\n    bounds = [element[1] for element in expanded_args]\n    slope = [element[2] for element in expanded_args]\n\n    out_settings = {}\n    out_decimals = set_max_precision()\n\n    for i in range(0, len(bands)):\n        band_array = raster.read(bands[i])\n        band_array = cast_array_to_float(band_array, cast_int=True)\n        band_array = replace_values(band_array, values_to_replace=[nodata, np.inf], replace_value=np.nan)\n\n        band_array = _sigmoid_transform(band_array.astype(np.float64), bounds=bounds[i], slope=slope[i], center=center)\n\n        band_array = truncate_decimal_places(band_array, decimal_places=out_decimals)\n        band_array = nan_to_nodata(band_array, nodata_value=nodata)\n        band_array = cast_array_to_float(band_array, scalar=nodata, cast_float=True)\n\n        band_array = np.expand_dims(band_array, axis=0)\n\n        if i == 0:\n            out_array = band_array.copy()\n        else:\n            out_array = np.vstack((out_array, band_array))\n\n        current_transform = f\"transformation {i + 1}\"\n        current_settings = {\n            \"band_origin\": bands[i],\n            \"bound_lower\": truncate_decimal_places(bounds[i][0], decimal_places=out_decimals),\n            \"bound_upper\": truncate_decimal_places(bounds[i][1], decimal_places=out_decimals),\n            \"slope\": slope[i],\n            \"center\": center,\n            \"nodata\": nodata,\n            \"decimal_places\": out_decimals,\n        }\n\n        out_settings[current_transform] = current_settings\n\n    out_meta = raster.meta.copy()\n    out_meta.update({\"count\": len(bands), \"nodata\": nodata, \"dtype\": out_array.dtype.name})\n\n    return out_array, out_meta, out_settings\n</code></pre>"},{"location":"transformations/winsorize/","title":"Winsorize","text":""},{"location":"transformations/winsorize/#eis_toolkit.transformations.winsorize.winsorize","title":"<code>winsorize(raster, percentiles, bands=None, inside=False, nodata=None)</code>","text":"<p>Winsorize data based on specified percentile values.</p> <p>Takes one nodata value that will be ignored in calculations. Replaces values between [minimum, lower percentile] and [upper percentile, maximum] if provided. Works both one-sided and two-sided but raises error if no percentile values provided.</p> <p>Percentiles are symmetrical, i.e. percentile_lower = 10 corresponds to the interval [min, 10%]. And percentile_upper = 10 corresponds to the intervall [90%, max]. I.e. percentile_lower = 0 refers to the minimum and percentile_upper = 0 to the data maximum.</p> <p>Calculation of percentiles is ambiguous. Users can choose whether to use the value for replacement from inside or outside of the respective interval. Example: Given the np.array[5 10 12 15 20 24 27 30 35] and percentiles(10, 10), the calculated percentiles are (5, 35) for inside and (10, 30) for outside. This results in [5 10 12 15 20 24 27 30 35] and [10 10 12 15 20 24 27 30 30], respectively.</p> <p>If no band/column selection specified, all bands/columns will be used. If a parameter contains only 1 entry, it will be applied for all bands. The percentiles can be set for each band individually, but inside parameter is same for all bands.</p> <p>Parameters:</p> Name Type Description Default <code>raster</code> <code>DatasetReader</code> <p>Data object to be transformed.</p> required <code>bands</code> <code>Optional[Sequence[int]]</code> <p>Selection of bands to be transformed.</p> <code>None</code> <code>percentiles</code> <code>Sequence[Tuple[Optional[Number], Optional[Number]]]</code> <p>Lower and upper percentile values (lower, upper) between [0, 100].</p> required <code>inside</code> <code>bool</code> <p>Whether to use the value for replacement from the left or right of the calculated percentile.</p> <code>False</code> <code>nodata</code> <code>Optional[Number]</code> <p>Nodata value to be considered.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>out_array</code> <code>ndarray</code> <p>The transformed data.</p> <code>out_meta</code> <code>dict</code> <p>Updated metadata.</p> <code>out_settings</code> <code>dict</code> <p>Log of input settings and calculated statistics if available.</p> <p>Raises:</p> Type Description <code>InvalidRasterBandException</code> <p>The input contains invalid band numbers.</p> <code>NonMatchingParameterLengthsException</code> <p>The input does not match the number of selected bands.</p> <code>InvalidParameterValueException</code> <p>The input does not match the requirements (values, order of values)</p> Source code in <code>eis_toolkit/transformations/winsorize.py</code> <pre><code>@beartype\ndef winsorize(  # type: ignore[no-any-unimported]\n    raster: rasterio.io.DatasetReader,\n    percentiles: Sequence[Tuple[Optional[Number], Optional[Number]]],\n    bands: Optional[Sequence[int]] = None,\n    inside: bool = False,\n    nodata: Optional[Number] = None,\n) -&gt; Tuple[np.ndarray, dict, dict]:\n    \"\"\"\n    Winsorize data based on specified percentile values.\n\n    Takes one nodata value that will be ignored in calculations.\n    Replaces values between [minimum, lower percentile] and [upper percentile, maximum] if provided.\n    Works both one-sided and two-sided but raises error if no percentile values provided.\n\n    Percentiles are symmetrical, i.e. percentile_lower = 10 corresponds to the interval [min, 10%].\n    And percentile_upper = 10 corresponds to the intervall [90%, max].\n    I.e. percentile_lower = 0 refers to the minimum and percentile_upper = 0 to the data maximum.\n\n    Calculation of percentiles is ambiguous. Users can choose whether to use the value\n    for replacement from inside or outside of the respective interval. Example:\n    Given the np.array[5 10 12 15 20 24 27 30 35] and percentiles(10, 10), the calculated\n    percentiles are (5, 35) for inside and (10, 30) for outside.\n    This results in [5 10 12 15 20 24 27 30 35] and [10 10 12 15 20 24 27 30 30], respectively.\n\n    If no band/column selection specified, all bands/columns will be used.\n    If a parameter contains only 1 entry, it will be applied for all bands.\n    The percentiles can be set for each band individually, but inside parameter is same for all bands.\n\n    Args:\n        raster: Data object to be transformed.\n        bands: Selection of bands to be transformed.\n        percentiles: Lower and upper percentile values (lower, upper) between [0, 100].\n        inside: Whether to use the value for replacement from the left or right of the calculated percentile.\n        nodata: Nodata value to be considered.\n\n    Returns:\n        out_array: The transformed data.\n        out_meta: Updated metadata.\n        out_settings: Log of input settings and calculated statistics if available.\n\n    Raises:\n        InvalidRasterBandException: The input contains invalid band numbers.\n        NonMatchingParameterLengthsException: The input does not match the number of selected bands.\n        InvalidParameterValueException: The input does not match the requirements (values, order of values)\n    \"\"\"\n    bands = list(range(1, raster.count + 1)) if bands is None else bands\n    nodata = raster.nodata if nodata is None else nodata\n\n    if check_raster_bands(raster, bands) is False:\n        raise InvalidRasterBandException(\"Invalid band selection\")\n\n    if check_parameter_length(bands, percentiles) is False:\n        raise NonMatchingParameterLengthsException(\"Invalid length for percentiles.\")\n\n    for item in percentiles:\n        if item.count(None) == len(item):\n            raise InvalidParameterValueException(f\"Percentile values all None: {item}.\")\n\n        if None not in item and sum(item) &gt;= 100:\n            raise InvalidParameterValueException(f\"Sum &gt;= 100: {item}.\")\n\n        if item[0] is not None and not (0 &lt; item[0] &lt; 100):\n            raise InvalidParameterValueException(f\"Invalid lower percentile value: {item}.\")\n\n        if item[1] is not None and not (0 &lt; item[1] &lt; 100):\n            raise InvalidParameterValueException(f\"Invalid upper percentile value: {item}.\")\n\n    expanded_args = expand_and_zip(bands, percentiles)\n    percentiles = [element[1] for element in expanded_args]\n\n    out_settings = {}\n\n    for i in range(0, len(bands)):\n        band_array = raster.read(bands[i])\n        inital_dtype = band_array.dtype\n\n        band_array = cast_array_to_float(band_array, cast_int=True)\n        band_array = nodata_to_nan(band_array, nodata_value=nodata)\n\n        band_array, calculated_lower, calculated_upper = _winsorize(\n            band_array, percentiles=percentiles[i], inside=inside\n        )\n\n        band_array = nan_to_nodata(band_array, nodata_value=nodata)\n        band_array = cast_array_to_int(band_array, scalar=nodata, initial_dtype=inital_dtype)\n\n        band_array = np.expand_dims(band_array, axis=0)\n\n        if i == 0:\n            out_array = band_array.copy()\n        else:\n            out_array = np.vstack((out_array, band_array))\n\n        current_transform = f\"transformation {i + 1}\"\n        current_settings = {\n            \"band_origin\": bands[i],\n            \"percentile_lower\": cast_scalar_to_int(percentiles[i][0]),\n            \"percentile_upper\": cast_scalar_to_int(percentiles[i][1]),\n            \"calculated_lower\": cast_scalar_to_int(calculated_lower),\n            \"calculated_upper\": cast_scalar_to_int(calculated_upper),\n            \"nodata\": cast_scalar_to_int(nodata),\n        }\n\n        out_settings[current_transform] = current_settings\n\n    out_meta = raster.meta.copy()\n    out_meta.update({\"count\": len(bands), \"nodata\": nodata, \"dtype\": out_array.dtype.name})\n\n    return out_array, out_meta, out_settings\n</code></pre>"},{"location":"transformations/coda/alr/","title":"Additive logratio transform","text":""},{"location":"transformations/coda/alr/#eis_toolkit.transformations.coda.alr.alr_transform","title":"<code>alr_transform(df, column=None, keep_denominator_column=False)</code>","text":"<p>Perform an additive logratio transformation on the data.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A dataframe of compositional data.</p> required <code>column</code> <code>Optional[str]</code> <p>The name of the column to be used as the denominator column.</p> <code>None</code> <code>keep_denominator_column</code> <code>bool</code> <p>Whether to include the denominator column in the result. If True, the returned dataframe retains its original shape.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A new dataframe containing the ALR transformed data.</p> <p>Raises:</p> Type Description <code>InvalidColumnException</code> <p>The input column isn't found in the dataframe.</p> <code>InvalidCompositionException</code> <p>Data is not normalized to the expected value.</p> <code>NumericValueSignException</code> <p>Data contains zeros or negative values.</p> Source code in <code>eis_toolkit/transformations/coda/alr.py</code> <pre><code>@beartype\ndef alr_transform(\n    df: pd.DataFrame, column: Optional[str] = None, keep_denominator_column: bool = False\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Perform an additive logratio transformation on the data.\n\n    Args:\n        df: A dataframe of compositional data.\n        column: The name of the column to be used as the denominator column.\n        keep_denominator_column: Whether to include the denominator column in the result. If True, the returned\n            dataframe retains its original shape.\n\n    Returns:\n        A new dataframe containing the ALR transformed data.\n\n    Raises:\n        InvalidColumnException: The input column isn't found in the dataframe.\n        InvalidCompositionException: Data is not normalized to the expected value.\n        NumericValueSignException: Data contains zeros or negative values.\n    \"\"\"\n    check_in_simplex_sample_space(df)\n\n    if column is not None and column not in df.columns:\n        raise InvalidColumnException(f\"The column {column} was not found in the dataframe.\")\n\n    column = column if column is not None else df.columns[-1]\n\n    columns = [col for col in df.columns]\n\n    if not keep_denominator_column and column in columns:\n        columns.remove(column)\n\n    return rename_columns_by_pattern(_alr_transform(df, columns, column))\n</code></pre>"},{"location":"transformations/coda/alr/#eis_toolkit.transformations.coda.alr.inverse_alr","title":"<code>inverse_alr(df, denominator_column, scale=1.0)</code>","text":"<p>Perform the inverse transformation for a set of ALR transformed data.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A dataframe of ALR transformed compositional data.</p> required <code>denominator_column</code> <code>str</code> <p>The name of the denominator column.</p> required <code>scale</code> <code>Number</code> <p>The value to which each composition should be normalized. Eg., if the composition is expressed as percentages, scale=100.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A dataframe containing the inverse transformed data.</p> <p>Raises:</p> Type Description <code>NumericValueSignException</code> <p>The input scale value is zero or less.</p> Source code in <code>eis_toolkit/transformations/coda/alr.py</code> <pre><code>@beartype\ndef inverse_alr(df: pd.DataFrame, denominator_column: str, scale: Number = 1.0) -&gt; pd.DataFrame:\n    \"\"\"\n    Perform the inverse transformation for a set of ALR transformed data.\n\n    Args:\n        df: A dataframe of ALR transformed compositional data.\n        denominator_column: The name of the denominator column.\n        scale: The value to which each composition should be normalized. Eg., if the composition is expressed\n            as percentages, scale=100.\n\n    Returns:\n        A dataframe containing the inverse transformed data.\n\n    Raises:\n        NumericValueSignException: The input scale value is zero or less.\n    \"\"\"\n    if scale &lt;= 0:\n        raise NumericValueSignException(\"The scale value should be positive.\")\n\n    return _inverse_alr(df, denominator_column, scale)\n</code></pre>"},{"location":"transformations/coda/clr/","title":"Centered logratio transform","text":""},{"location":"transformations/coda/clr/#eis_toolkit.transformations.coda.clr.clr_transform","title":"<code>clr_transform(df)</code>","text":"<p>Perform a centered logratio transformation on the data.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A dataframe of compositional data.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A new dataframe containing the CLR transformed data.</p> <p>Raises:</p> Type Description <code>InvalidCompositionException</code> <p>Data is not normalized to the expected value.</p> <code>NumericValueSignException</code> <p>Data contains zeros or negative values.</p> Source code in <code>eis_toolkit/transformations/coda/clr.py</code> <pre><code>@beartype\ndef clr_transform(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Perform a centered logratio transformation on the data.\n\n    Args:\n        df: A dataframe of compositional data.\n\n    Returns:\n        A new dataframe containing the CLR transformed data.\n\n    Raises:\n        InvalidCompositionException: Data is not normalized to the expected value.\n        NumericValueSignException: Data contains zeros or negative values.\n    \"\"\"\n    check_in_simplex_sample_space(df)\n    return rename_columns_by_pattern(_clr_transform(df))\n</code></pre>"},{"location":"transformations/coda/clr/#eis_toolkit.transformations.coda.clr.inverse_clr","title":"<code>inverse_clr(df, colnames=None, scale=1.0)</code>","text":"<p>Perform the inverse transformation for a set of CLR transformed data.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A dataframe of CLR transformed compositional data.</p> required <code>colnames</code> <code>Optional[Sequence[str]]</code> <p>List of column names to rename the columns to.</p> <code>None</code> <code>scale</code> <code>Number</code> <p>The value to which each composition should be normalized. Eg., if the composition is expressed as percentages, scale=100.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A dataframe containing the inverse transformed data.</p> <p>Raises:</p> Type Description <code>NumericValueSignException</code> <p>The input scale value is zero or less.</p> Source code in <code>eis_toolkit/transformations/coda/clr.py</code> <pre><code>@beartype\ndef inverse_clr(df: pd.DataFrame, colnames: Optional[Sequence[str]] = None, scale: Number = 1.0) -&gt; pd.DataFrame:\n    \"\"\"\n    Perform the inverse transformation for a set of CLR transformed data.\n\n    Args:\n        df: A dataframe of CLR transformed compositional data.\n        colnames: List of column names to rename the columns to.\n        scale: The value to which each composition should be normalized. Eg., if the composition is expressed\n            as percentages, scale=100.\n\n    Returns:\n        A dataframe containing the inverse transformed data.\n\n    Raises:\n        NumericValueSignException: The input scale value is zero or less.\n    \"\"\"\n    if scale &lt;= 0:\n        raise NumericValueSignException(\"The scale value should be positive.\")\n\n    return _inverse_clr(df, colnames, scale)\n</code></pre>"},{"location":"transformations/coda/ilr/","title":"Isometric logratio transform","text":""},{"location":"transformations/coda/ilr/#eis_toolkit.transformations.coda.ilr.single_ilr_transform","title":"<code>single_ilr_transform(df, subcomposition_1, subcomposition_2)</code>","text":"<p>Perform a single isometric logratio transformation on the provided subcompositions.</p> <p>Returns ILR balances. Column order matters.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A dataframe of shape [N, D] of compositional data.</p> required <code>subcomposition_1</code> <code>Sequence[str]</code> <p>Names of the columns in the numerator part of the ratio.</p> required <code>subcomposition_2</code> <code>Sequence[str]</code> <p>Names of the columns in the denominator part of the ratio.</p> required <p>Returns:</p> Type Description <code>Series</code> <p>A series of length N containing the transforms.</p> <p>Raises:</p> Type Description <code>InvalidColumnException</code> <p>One or more subcomposition columns are not found in the input dataframe.</p> <code>InvalidCompositionException</code> <p>Data is not normalized to the expected value or one or more columns are found in both subcompositions.</p> <code>InvalidParameterValueException</code> <p>At least one subcomposition provided was empty.</p> <code>NumericValueSignException</code> <p>Data contains zeros or negative values.</p> Source code in <code>eis_toolkit/transformations/coda/ilr.py</code> <pre><code>@beartype\ndef single_ilr_transform(\n    df: pd.DataFrame, subcomposition_1: Sequence[str], subcomposition_2: Sequence[str]\n) -&gt; pd.Series:\n    \"\"\"\n    Perform a single isometric logratio transformation on the provided subcompositions.\n\n    Returns ILR balances. Column order matters.\n\n    Args:\n        df: A dataframe of shape [N, D] of compositional data.\n        subcomposition_1: Names of the columns in the numerator part of the ratio.\n        subcomposition_2: Names of the columns in the denominator part of the ratio.\n\n    Returns:\n        A series of length N containing the transforms.\n\n    Raises:\n        InvalidColumnException: One or more subcomposition columns are not found in the input dataframe.\n        InvalidCompositionException: Data is not normalized to the expected value or\n            one or more columns are found in both subcompositions.\n        InvalidParameterValueException: At least one subcomposition provided was empty.\n        NumericValueSignException: Data contains zeros or negative values.\n    \"\"\"\n    check_in_simplex_sample_space(df)\n\n    if not (subcomposition_1 and subcomposition_2):\n        raise InvalidParameterValueException(\"A subcomposition should contain at least one column.\")\n\n    if not (check_columns_valid(df, subcomposition_1) and check_columns_valid(df, subcomposition_2)):\n        raise InvalidColumnException(\"Not all of the input columns were found in the input dataframe.\")\n\n    if check_lists_overlap(subcomposition_1, subcomposition_2):\n        raise InvalidCompositionException(\"The subcompositions overlap.\")\n\n    return _single_ilr_transform(df, subcomposition_1, subcomposition_2)\n</code></pre>"},{"location":"transformations/coda/pairwise/","title":"Pairwise logratio transform","text":""},{"location":"transformations/coda/pairwise/#eis_toolkit.transformations.coda.pairwise.pairwise_logratio","title":"<code>pairwise_logratio(df, numerator_column, denominator_column)</code>","text":"<p>Perform a pairwise logratio transformation on the given columns.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe containing the columns to use in the transformation.</p> required <code>numerator_column</code> <code>str</code> <p>The name of the column to use as the numerator column.</p> required <code>denominator_column</code> <code>str</code> <p>The name of the column to use as the denominator.</p> required <p>Returns:</p> Type Description <code>Series</code> <p>A series containing the transformed values.</p> <p>Raises:</p> Type Description <code>InvalidColumnException</code> <p>One or both of the input columns are not found in the dataframe.</p> <code>InvalidParameterValueException</code> <p>The input columns contain at least one zero value.</p> Source code in <code>eis_toolkit/transformations/coda/pairwise.py</code> <pre><code>@beartype\ndef pairwise_logratio(df: pd.DataFrame, numerator_column: str, denominator_column: str) -&gt; pd.Series:\n    \"\"\"\n    Perform a pairwise logratio transformation on the given columns.\n\n    Args:\n        df: The dataframe containing the columns to use in the transformation.\n        numerator_column: The name of the column to use as the numerator column.\n        denominator_column: The name of the column to use as the denominator.\n\n    Returns:\n        A series containing the transformed values.\n\n    Raises:\n        InvalidColumnException: One or both of the input columns are not found in the dataframe.\n        InvalidParameterValueException: The input columns contain at least one zero value.\n    \"\"\"\n    if numerator_column not in df.columns or denominator_column not in df.columns:\n        raise InvalidColumnException(\"At least one input column is not found in the dataframe.\")\n\n    if check_dataframe_contains_zeros(df[[numerator_column, denominator_column]]):\n        raise InvalidParameterValueException(\"The input columns contain at least one zero value.\")\n\n    return _pairwise_logratio(df, numerator_column, denominator_column)\n</code></pre>"},{"location":"transformations/coda/pairwise/#eis_toolkit.transformations.coda.pairwise.single_pairwise_logratio","title":"<code>single_pairwise_logratio(numerator, denominator)</code>","text":"<p>Perform a pairwise logratio transformation on the given values.</p> <p>Parameters:</p> Name Type Description Default <code>numerator</code> <code>Number</code> <p>The numerator in the ratio.</p> required <code>denominator</code> <code>Number</code> <p>The denominator in the ratio.</p> required <p>Returns:</p> Type Description <code>float64</code> <p>The transformed value.</p> <p>Raises:</p> Type Description <code>InvalidParameterValueException</code> <p>One or both input values are zero.</p> Source code in <code>eis_toolkit/transformations/coda/pairwise.py</code> <pre><code>@beartype\ndef single_pairwise_logratio(numerator: Number, denominator: Number) -&gt; np.float64:\n    \"\"\"\n    Perform a pairwise logratio transformation on the given values.\n\n    Args:\n        numerator: The numerator in the ratio.\n        denominator: The denominator in the ratio.\n\n    Returns:\n        The transformed value.\n\n    Raises:\n        InvalidParameterValueException: One or both input values are zero.\n    \"\"\"\n    if numerator == 0 or denominator == 0:\n        raise InvalidParameterValueException(\"Input values cannot be zero.\")\n\n    return _single_pairwise_logratio(numerator, denominator)\n</code></pre>"},{"location":"transformations/coda/plr/","title":"Pivot logratio transform","text":""},{"location":"transformations/coda/plr/#eis_toolkit.transformations.coda.plr.plr_transform","title":"<code>plr_transform(df)</code>","text":"<p>Perform a pivot logratio transformation on the dataframe, returning the full set of transforms.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A dataframe of shape [N, D] of compositional data.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A dataframe of shape [N, D-1] containing the set of PLR transformed data.</p> <p>Raises:</p> Type Description <code>InvalidColumnException</code> <p>The data contains one or more zeros.</p> <code>InvalidCompositionException</code> <p>Data is not normalized to the expected value.</p> <code>NumericValueSignException</code> <p>Data contains zeros or negative values.</p> Source code in <code>eis_toolkit/transformations/coda/plr.py</code> <pre><code>@beartype\ndef plr_transform(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Perform a pivot logratio transformation on the dataframe, returning the full set of transforms.\n\n    Args:\n        df: A dataframe of shape [N, D] of compositional data.\n\n    Returns:\n        A dataframe of shape [N, D-1] containing the set of PLR transformed data.\n\n    Raises:\n        InvalidColumnException: The data contains one or more zeros.\n        InvalidCompositionException: Data is not normalized to the expected value.\n        NumericValueSignException: Data contains zeros or negative values.\n    \"\"\"\n    check_in_simplex_sample_space(df)\n\n    return rename_columns_by_pattern(_plr_transform(df))\n</code></pre>"},{"location":"transformations/coda/plr/#eis_toolkit.transformations.coda.plr.single_plr_transform","title":"<code>single_plr_transform(df, column)</code>","text":"<p>Perform a pivot logratio transformation on the selected column.</p> <p>Pivot logratio is a special case of ILR, where the numerator in the ratio is always a single part and the denominator all of the parts to the right in the ordered list of parts.</p> <p>Column order matters.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A dataframe of shape [N, D] of compositional data.</p> required <code>column</code> <code>str</code> <p>The name of the numerator column to use for the transformation.</p> required <p>Returns:</p> Type Description <code>Series</code> <p>A series of length N containing the transforms.</p> <p>Raises:</p> Type Description <code>InvalidColumnException</code> <p>The input column isn't found in the dataframe, or there are no columns to the right of the given column.</p> <code>InvalidCompositionException</code> <p>Data is not normalized to the expected value.</p> <code>NumericValueSignException</code> <p>Data contains zeros or negative values.</p> Source code in <code>eis_toolkit/transformations/coda/plr.py</code> <pre><code>@beartype\ndef single_plr_transform(df: pd.DataFrame, column: str) -&gt; pd.Series:\n    \"\"\"\n    Perform a pivot logratio transformation on the selected column.\n\n    Pivot logratio is a special case of ILR, where the numerator in the ratio is always a single\n    part and the denominator all of the parts to the right in the ordered list of parts.\n\n    Column order matters.\n\n    Args:\n        df: A dataframe of shape [N, D] of compositional data.\n        column: The name of the numerator column to use for the transformation.\n\n    Returns:\n        A series of length N containing the transforms.\n\n    Raises:\n        InvalidColumnException: The input column isn't found in the dataframe, or there are no columns\n            to the right of the given column.\n        InvalidCompositionException: Data is not normalized to the expected value.\n        NumericValueSignException: Data contains zeros or negative values.\n    \"\"\"\n    check_in_simplex_sample_space(df)\n\n    if column not in df.columns:\n        raise InvalidColumnException(f\"The column {column} was not found in the dataframe.\")\n\n    idx = df.columns.get_loc(column)\n\n    if idx == len(df.columns) - 1:\n        raise InvalidColumnException()\n\n    return _single_plr_transform(df, column)\n</code></pre>"},{"location":"validation/calculate_auc/","title":"Calculate AUC","text":""},{"location":"validation/calculate_auc/#eis_toolkit.validation.calculate_auc.calculate_auc","title":"<code>calculate_auc(x_values, y_values)</code>","text":"<p>Calculate area under curve (AUC).</p> <p>Calculates AUC for curve. X-axis should be either proportion of area ore false positive rate. Y-axis should be always true positive rate. AUC is calculated with sklearn.metrics.auc which uses trapezoidal rule for calculation.</p> <p>Parameters:</p> Name Type Description Default <code>x_values</code> <code>Union[ndarray, Series]</code> <p>Either proportion of area or false positive rate values.</p> required <code>y_values</code> <code>Union[ndarray, Series]</code> <p>True positive rate values.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The area under curve.</p> <p>Raises:</p> Type Description <code>InvalidParameterValueException</code> <p>x_values or y_values are out of bounds.</p> Source code in <code>eis_toolkit/validation/calculate_auc.py</code> <pre><code>@beartype\ndef calculate_auc(x_values: Union[np.ndarray, pd.Series], y_values: Union[np.ndarray, pd.Series]) -&gt; float:\n    \"\"\"Calculate area under curve (AUC).\n\n    Calculates AUC for curve. X-axis should be either proportion of area ore false positive rate. Y-axis should be\n    always true positive rate. AUC is calculated with sklearn.metrics.auc which uses trapezoidal rule for calculation.\n\n    Args:\n        x_values: Either proportion of area or false positive rate values.\n        y_values: True positive rate values.\n\n    Returns:\n        The area under curve.\n\n    Raises:\n        InvalidParameterValueException: x_values or y_values are out of bounds.\n    \"\"\"\n    if x_values.max() &gt; 1 or x_values.min() &lt; 0:\n        raise InvalidParameterValueException(\"x_values should be within range 0-1\")\n\n    if y_values.max() &gt; 1 or y_values.min() &lt; 0:\n        raise InvalidParameterValueException(\"y_values should be within range 0-1\")\n\n    auc_value = _calculate_auc(x_values=x_values, y_values=y_values)\n    return auc_value\n</code></pre>"},{"location":"validation/calculate_base_metrics/","title":"Calculate base metrics","text":""},{"location":"validation/calculate_base_metrics/#eis_toolkit.validation.calculate_base_metrics.calculate_base_metrics","title":"<code>calculate_base_metrics(raster, deposits, band=1, negatives=None)</code>","text":"<p>Calculate true positive rate, proportion of area and false positive rate values for different thresholds.</p> <p>Function calculates true positive rate, proportion of area and false positive rate values for different thresholds which are determined from inputted deposit locations and mineral prospectivity map. Note that calculation of false positive rate is optional and is only done if negative point locations are provided.</p> <p>Parameters:</p> Name Type Description Default <code>raster</code> <code>DatasetReader</code> <p>Mineral prospectivity map or evidence layer.</p> required <code>deposits</code> <code>GeoDataFrame</code> <p>Mineral deposit locations as points.</p> required <code>band</code> <code>int</code> <p>Band index of the mineral prospectivity map. Defaults to 1.</p> <code>1</code> <code>negatives</code> <code>Optional[GeoDataFrame]</code> <p>Negative locations as points.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame containing true positive rate, proportion of area, threshold values and false positive rate (optional) values.</p> <p>Raises:</p> Type Description <code>NonMatchingCrsException</code> <p>The raster and point data are not in the same CRS.</p> <code>GeometryTypeException</code> <p>The input geometries contain non-point features.</p> Source code in <code>eis_toolkit/validation/calculate_base_metrics.py</code> <pre><code>@beartype\ndef calculate_base_metrics(\n    raster: rasterio.io.DatasetReader,\n    deposits: geopandas.GeoDataFrame,\n    band: int = 1,\n    negatives: Optional[geopandas.GeoDataFrame] = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Calculate true positive rate, proportion of area and false positive rate values for different thresholds.\n\n    Function calculates true positive rate, proportion of area and false positive rate values for different thresholds\n    which are determined from inputted deposit locations and mineral prospectivity map. Note that calculation of false\n    positive rate is optional and is only done if negative point locations are provided.\n\n    Args:\n        raster: Mineral prospectivity map or evidence layer.\n        deposits: Mineral deposit locations as points.\n        band: Band index of the mineral prospectivity map. Defaults to 1.\n        negatives: Negative locations as points.\n\n    Returns:\n        DataFrame containing true positive rate, proportion of area, threshold values and false positive\n            rate (optional) values.\n\n    Raises:\n        NonMatchingCrsException: The raster and point data are not in the same CRS.\n        GeometryTypeException: The input geometries contain non-point features.\n    \"\"\"\n    if negatives is not None:\n        geometries = pd.concat([deposits, negatives]).geometry\n    else:\n        geometries = deposits[\"geometry\"]\n\n    if not check_matching_crs(\n        objects=[raster, geometries],\n    ):\n        raise NonMatchingCrsException(\"The raster and deposits are not in the same CRS.\")\n\n    if not check_geometry_types(\n        geometries=geometries,\n        allowed_types=[\"Point\"],\n    ):\n        raise GeometryTypeException(\"The input geometries contain non-point features.\")\n\n    base_metrics = _calculate_base_metrics(raster=raster, deposits=deposits, band=band, negatives=negatives)\n\n    return base_metrics\n</code></pre>"},{"location":"validation/get_pa_intersection/","title":"Get P-A plot intersection point","text":""},{"location":"validation/get_pa_intersection/#eis_toolkit.validation.get_pa_intersection.get_pa_intersection","title":"<code>get_pa_intersection(true_positive_rate_values, proportion_of_area_values, threshold_values)</code>","text":"<p>Calculate the intersection point for prediction rate and area curves in (P-A plot).</p> <p>Threshold_values values act as x-axis for both curves. Prediction rate curve uses true positive rate for y-axis. Area curve uses inverted proportion of area as y-axis.</p> <p>Parameters:</p> Name Type Description Default <code>true_positive_rate_values</code> <code>Union[ndarray, Series]</code> <p>True positive rate values, values should be within range 0-1.</p> required <code>proportion_of_area_values</code> <code>Union[ndarray, Series]</code> <p>Proportion of area values, values should be within range 0-1.</p> required <code>threshold_values</code> <code>Union[ndarray, Series]</code> <p>Threshold values that were used to calculate true positive rate and proportion of area.</p> required <p>Returns:</p> Type Description <code>Tuple[float, float]</code> <p>X and y coordinates of the intersection point.</p> <p>Raises:</p> Type Description <code>InvalidParameterValueException</code> <p>true_positive_rate_values or proportion_of_area_values values are out of bounds.</p> Source code in <code>eis_toolkit/validation/get_pa_intersection.py</code> <pre><code>@beartype\ndef get_pa_intersection(\n    true_positive_rate_values: Union[np.ndarray, pd.Series],\n    proportion_of_area_values: Union[np.ndarray, pd.Series],\n    threshold_values: Union[np.ndarray, pd.Series],\n) -&gt; Tuple[float, float]:\n    \"\"\"Calculate the intersection point for prediction rate and area curves in (P-A plot).\n\n    Threshold_values values act as x-axis for both curves. Prediction rate curve uses true positive rate for y-axis.\n    Area curve uses inverted proportion of area as y-axis.\n\n    Args:\n        true_positive_rate_values: True positive rate values, values should be within range 0-1.\n        proportion_of_area_values: Proportion of area values, values should be within range 0-1.\n        threshold_values: Threshold values that were used to calculate true positive rate and proportion of area.\n\n    Returns:\n        X and y coordinates of the intersection point.\n\n    Raises:\n        InvalidParameterValueException: true_positive_rate_values or proportion_of_area_values values are out of bounds.\n    \"\"\"\n    if true_positive_rate_values.max() &gt; 1 or true_positive_rate_values.min() &lt; 0:\n        raise InvalidParameterValueException(\"true_positive_rate_values values should be within range 0-1\")\n\n    if proportion_of_area_values.max() &gt; 1 or proportion_of_area_values.min() &lt; 0:\n        raise InvalidParameterValueException(\"proportion_of_area_values values should be within range 0-1\")\n\n    intersection = _get_pa_intersection(\n        true_positive_rate_values=true_positive_rate_values,\n        proportion_of_area_values=proportion_of_area_values,\n        threshold_values=threshold_values,\n    )\n\n    return intersection.x, intersection.y\n</code></pre>"},{"location":"validation/plot_correlation_matrix/","title":"Plot correlation matrix","text":""},{"location":"validation/plot_correlation_matrix/#eis_toolkit.validation.plot_correlation_matrix.plot_correlation_matrix","title":"<code>plot_correlation_matrix(matrix, annotate=True, cmap=None, plot_title=None, **kwargs)</code>","text":"<p>Create a Seaborn heatmap to visualize correlation matrix.</p> <p>Parameters:</p> Name Type Description Default <code>matrix</code> <code>DataFrame</code> <p>Correlation matrix as a DataFrame.</p> required <code>annotate</code> <code>bool</code> <p>If plot squares should display the correlation values. Defaults to True.</p> <code>True</code> <code>cmap</code> <code>Optional[ListedColormap]</code> <p>Colormap for plotting. Optional parameter. Defaults to None, in which case a default colormap is used.</p> <code>None</code> <code>plot_title</code> <code>Optional[str]</code> <p>Title of the plot. Optional parameter, defaults to none (no title).</p> <code>None</code> <code>**kwargs</code> <code>dict</code> <p>Additional parameters to pass to Seaborn and matplotlib.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Axes</code> <p>Matplotlib axes object with the produced plot.</p> <p>Raises:</p> Type Description <code>EmptyDataFrameException</code> <p>Input matrix is empty.</p> Source code in <code>eis_toolkit/validation/plot_correlation_matrix.py</code> <pre><code>def plot_correlation_matrix(\n    matrix: pd.DataFrame,\n    annotate: bool = True,\n    cmap: Optional[matplotlib.colors.ListedColormap] = None,\n    plot_title: Optional[str] = None,\n    **kwargs: dict\n) -&gt; matplotlib.axes.Axes:\n    \"\"\"\n    Create a Seaborn heatmap to visualize correlation matrix.\n\n    Args:\n        matrix: Correlation matrix as a DataFrame.\n        annotate: If plot squares should display the correlation values. Defaults to True.\n        cmap: Colormap for plotting. Optional parameter. Defaults to None, in which\n            case a default colormap is used.\n        plot_title: Title of the plot. Optional parameter, defaults to none (no title).\n        **kwargs: Additional parameters to pass to Seaborn and matplotlib.\n\n    Returns:\n        Matplotlib axes object with the produced plot.\n\n    Raises:\n        EmptyDataFrameException: Input matrix is empty.\n    \"\"\"\n    if matrix.empty:\n        raise EmptyDataFrameException(\"Input matrix DataFrame is empty.\")\n\n    # Mask for the upper triangle of the heatmap\n    mask = np.triu(np.ones_like(matrix, dtype=bool))\n\n    if cmap is None:\n        # Generate a default diverging colormap\n        cmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n    ax = sns.heatmap(\n        matrix,\n        mask=mask,\n        cmap=cmap,\n        vmax=0.3,\n        center=0,\n        square=True,\n        linewidths=0.5,\n        annot=annotate,\n        cbar_kws={\"shrink\": 0.5},\n        **kwargs\n    )\n    if plot_title is not None:\n        ax.set_title(plot_title)\n\n    return ax\n</code></pre>"},{"location":"validation/plot_prediction_area_curves/","title":"Plot prediction-area (P-A) curves","text":""},{"location":"validation/plot_prediction_area_curves/#eis_toolkit.validation.plot_prediction_area_curves.plot_prediction_area_curves","title":"<code>plot_prediction_area_curves(true_positive_rate_values, proportion_of_area_values, threshold_values)</code>","text":"<p>Plot prediction-area (P-A) plot.</p> <p>Plots prediction area plot that can be used to evaluate mineral prospectivity maps and evidential layers. See e.g., Yousefi and Carranza (2015).</p> <p>Parameters:</p> Name Type Description Default <code>true_positive_rate_values</code> <code>Union[ndarray, Series]</code> <p>True positive rate values.</p> required <code>proportion_of_area_values</code> <code>Union[ndarray, Series]</code> <p>Proportion of area values.</p> required <code>threshold_values</code> <code>Union[ndarray, Series]</code> <p>Threshold values.</p> required <p>Returns:</p> Type Description <code>Figure</code> <p>P-A plot figure object.</p> <p>Raises:</p> Type Description <code>InvalidParameterValueException</code> <p>true_positive_rate_values or proportion_of_area_values values are out of bounds.</p> References <p>Yousefi, Mahyar, and Emmanuel John M. Carranza. \"Fuzzification of continuous-value spatial evidence for mineral prospectivity mapping.\" Computers &amp; Geosciences 74 (2015): 97-109.</p> Source code in <code>eis_toolkit/validation/plot_prediction_area_curves.py</code> <pre><code>@beartype\ndef plot_prediction_area_curves(\n    true_positive_rate_values: Union[np.ndarray, pd.Series],\n    proportion_of_area_values: Union[np.ndarray, pd.Series],\n    threshold_values: Union[np.ndarray, pd.Series],\n) -&gt; matplotlib.figure.Figure:\n    \"\"\"Plot prediction-area (P-A) plot.\n\n    Plots prediction area plot that can be used to evaluate mineral prospectivity maps and evidential layers. See e.g.,\n    Yousefi and Carranza (2015).\n\n    Args:\n        true_positive_rate_values: True positive rate values.\n        proportion_of_area_values: Proportion of area values.\n        threshold_values: Threshold values.\n\n    Returns:\n        P-A plot figure object.\n\n    Raises:\n        InvalidParameterValueException: true_positive_rate_values or proportion_of_area_values values are out of bounds.\n\n    References:\n        Yousefi, Mahyar, and Emmanuel John M. Carranza. \"Fuzzification of continuous-value spatial evidence for mineral\n        prospectivity mapping.\" Computers &amp; Geosciences 74 (2015): 97-109.\n    \"\"\"\n    if true_positive_rate_values.max() &gt; 1 or true_positive_rate_values.min() &lt; 0:\n        raise InvalidParameterValueException(\"true_positive_rate values should be within range 0-1\")\n\n    if proportion_of_area_values.max() &gt; 1 or proportion_of_area_values.min() &lt; 0:\n        raise InvalidParameterValueException(\"proportion_of_area values should be within range 0-1\")\n\n    fig = _plot_prediction_area_curves(\n        true_positive_rate_values=true_positive_rate_values,\n        proportion_of_area_values=proportion_of_area_values,\n        threshold_values=threshold_values,\n    )\n    return fig\n</code></pre>"},{"location":"validation/plot_rate_curve/","title":"Plot rate curve","text":""},{"location":"validation/plot_rate_curve/#eis_toolkit.validation.plot_rate_curve.plot_rate_curve","title":"<code>plot_rate_curve(x_values, y_values, plot_type='success_rate')</code>","text":"<p>Plot success rate, prediction rate or ROC curve.</p> <p>Plot type depends on plot_type argument. Y-axis is always true positive rate, while x-axis can be either false positive rate (roc) or proportion of area (success and prediction rate) depending on plot type.</p> <p>Parameters:</p> Name Type Description Default <code>x_values</code> <code>Union[ndarray, Series]</code> <p>False positive rate values or proportion of area values.</p> required <code>y_values</code> <code>Union[ndarray, Series]</code> <p>True positive rate values.</p> required <code>plot_type</code> <code>Literal[success_rate, prediction_rate, roc]</code> <p>Plot type. Can be either: \"success_rate\", \"prediction_rate\" or \"roc\".</p> <code>'success_rate'</code> <p>Returns:</p> Type Description <code>Figure</code> <p>Success rate, prediction rate or ROC plot figure object.</p> <p>Raises:</p> Type Description <code>InvalidParameterValueException</code> <p>Invalid plot type.</p> <code>InvalidParameterValueException</code> <p>x_values or y_values are out of bounds.</p> Source code in <code>eis_toolkit/validation/plot_rate_curve.py</code> <pre><code>@beartype\ndef plot_rate_curve(\n    x_values: Union[np.ndarray, pd.Series],\n    y_values: Union[np.ndarray, pd.Series],\n    plot_type: Literal[\"success_rate\", \"prediction_rate\", \"roc\"] = \"success_rate\",\n) -&gt; matplotlib.figure.Figure:\n    \"\"\"Plot success rate, prediction rate or ROC curve.\n\n    Plot type depends on plot_type argument. Y-axis is always true positive rate, while x-axis can be either false\n    positive rate (roc) or proportion of area (success and prediction rate) depending on plot type.\n\n    Args:\n        x_values: False positive rate values or proportion of area values.\n        y_values: True positive rate values.\n        plot_type: Plot type. Can be either: \"success_rate\", \"prediction_rate\" or \"roc\".\n\n    Returns:\n        Success rate, prediction rate or ROC plot figure object.\n\n    Raises:\n        InvalidParameterValueException: Invalid plot type.\n        InvalidParameterValueException: x_values or y_values are out of bounds.\n    \"\"\"\n    if plot_type == \"success_rate\":\n        label = \"Success rate\"\n        xlab = \"Proportion of area\"\n    elif plot_type == \"prediction_rate\":\n        label = \"Prediction rate\"\n        xlab = \"Proportion of area\"\n    elif plot_type == \"roc\":\n        label = \"ROC\"\n        xlab = \"False positive rate\"\n    else:\n        raise InvalidParameterValueException(\"Invalid plot type\")\n\n    if x_values.max() &gt; 1 or x_values.min() &lt; 0:\n        raise InvalidParameterValueException(\"x_values should be within range 0-1\")\n\n    if y_values.max() &gt; 1 or y_values.min() &lt; 0:\n        raise InvalidParameterValueException(\"y_values should be within range 0-1\")\n\n    fig = _plot_rate_curve(x_values=x_values, y_values=y_values, label=label, xlab=xlab)\n\n    return fig\n</code></pre>"},{"location":"vector_processing/calculate_geometry/","title":"Calculate geometry","text":""},{"location":"vector_processing/calculate_geometry/#eis_toolkit.vector_processing.calculate_geometry.calculate_geometry","title":"<code>calculate_geometry(geodataframe)</code>","text":"<p>Calculate the length or area of the given geometries.</p> <p>Parameters:</p> Name Type Description Default <code>geodataframe</code> <code>GeoDataFrame</code> <p>Geometries to be calculated.</p> required <p>Returns:</p> Name Type Description <code>calculated_gdf</code> <code>GeoDataFrame</code> <p>Geometries and calculated values.</p> Source code in <code>eis_toolkit/vector_processing/calculate_geometry.py</code> <pre><code>@beartype\ndef calculate_geometry(geodataframe: gpd.GeoDataFrame) -&gt; gpd.GeoDataFrame:\n    \"\"\"Calculate the length or area of the given geometries.\n\n    Args:\n        geodataframe: Geometries to be calculated.\n\n    Returns:\n        calculated_gdf: Geometries and calculated values.\n\n    Raises:\n        EmptyDataFrameException if input geodataframe is empty.\n    \"\"\"\n    if geodataframe.shape[0] == 0:\n        raise EmptyDataFrameException(\"Expected geodataframe to contain geometries.\")\n\n    calculated_gdf = geodataframe.copy()\n    calculated_gdf[\"value\"] = calculated_gdf.apply(lambda row: _calculate_value(row), axis=1)\n\n    return calculated_gdf\n</code></pre>"},{"location":"vector_processing/cell_based_association/","title":"Cell-Based Association","text":""},{"location":"vector_processing/cell_based_association/#eis_toolkit.vector_processing.cell_based_association.cell_based_association","title":"<code>cell_based_association(cell_size, geodata, output_path, column=None, subset_target_attribute_values=None, add_name=None, add_buffer=None)</code>","text":"<p>Creation of CBA matrix.</p> <p>Initializes a CBA matrix from a vector file. The mesh is calculated according to the geometries contained in this file and the size of cells. Allows to add multiple vector data to the matrix, based on targeted shapes and/or attributes.</p> <p>Parameters:</p> Name Type Description Default <code>cell_size</code> <code>int</code> <p>Size of the cells.</p> required <code>geodata</code> <code>List[GeoDataFrame]</code> <p>GeoDataFrame to create the CBA matrix. Additional GeoDataFrame(s) can be imputed to add to the CBA matrix.</p> required <code>output_path</code> <code>str</code> <p>Name of the saved .tif file.</p> required <code>column</code> <code>Optional[List[str]]</code> <p>Name of the column of interest. If no attribute is specified, then an artificial attribute is created representing the presence or absence of the geometries of this file for each cell of the CBA grid. A categorical attribute will generate as many columns (binary) in the CBA matrix than values considered of interest (dummification). See parameter . Additional column(s) can be imputed for each added GeoDataFrame(s). <code>None</code> <code>subset_target_attribute_values</code> <code>Optional[List[Union[None, list, str]]]</code> <p>List of values of interest of the target attribute, in case a categorical target attribute has been specified. Allows to filter a subset of relevant values. Additional values can be imputed for each added GeoDataFrame(s).</p> <code>None</code> <code>add_name</code> <code>Optional[List[Union[str, None]]]</code> <p>Name of the column(s) to add to the matrix.</p> <code>None</code> <code>add_buffer</code> <code>Optional[List[Union[Number, bool]]]</code> <p>Allow the use of a buffer around shapes before the intersection with CBA cells for the added GeoDataFrame(s). Minimize border effects or allow increasing positive samples (i.e. cells with mineralization). The size of the buffer is computed using the CRS (if projected CRS in meters: value in meters).</p> <code>None</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>CBA matrix is created.</p> Source code in <code>eis_toolkit/vector_processing/cell_based_association.py</code> <pre><code>@beartype\ndef cell_based_association(\n    cell_size: int,\n    geodata: List[gpd.GeoDataFrame],\n    output_path: str,\n    column: Optional[List[str]] = None,\n    subset_target_attribute_values: Optional[List[Union[None, list, str]]] = None,\n    add_name: Optional[List[Union[str, None]]] = None,\n    add_buffer: Optional[List[Union[Number, bool]]] = None,\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"Creation of CBA matrix.\n\n    Initializes a CBA matrix from a vector file. The mesh is calculated\n    according to the geometries contained in this file and the size of cells.\n    Allows to add multiple vector data to the matrix, based on targeted shapes\n    and/or attributes.\n\n    Args:\n        cell_size: Size of the cells.\n        geodata: GeoDataFrame to create the CBA matrix. Additional\n            GeoDataFrame(s) can be imputed to add to the CBA matrix.\n        output_path: Name of the saved .tif file.\n        column: Name of the column of interest. If no attribute is specified,\n            then an artificial attribute is created representing the presence\n            or absence of the geometries of this file for each cell of the CBA\n            grid. A categorical attribute will generate as many columns (binary)\n            in the CBA matrix than values considered of interest (dummification).\n            See parameter &lt;subset_target_attribute_values&gt;. Additional\n            column(s) can be imputed for each added GeoDataFrame(s).\n        subset_target_attribute_values: List of values of interest of the\n            target attribute, in case a categorical target attribute has been\n            specified. Allows to filter a subset of relevant values. Additional\n            values can be imputed for each added GeoDataFrame(s).\n        add_name: Name of the column(s) to add to the matrix.\n        add_buffer: Allow the use of a buffer around shapes before the\n            intersection with CBA cells for the added GeoDataFrame(s). Minimize\n            border effects or allow increasing positive samples (i.e. cells\n            with mineralization). The size of the buffer is computed using the\n            CRS (if projected CRS in meters: value in meters).\n\n    Returns:\n        CBA matrix is created.\n    \"\"\"\n\n    # Swapping None to list values\n    if column is None:\n        column = [\"\"]\n    if add_buffer is None:\n        add_buffer = [False]\n\n    # Consistency checks on input data\n    for frame in geodata:\n        if frame.empty:\n            raise EmptyDataFrameException(\"The input GeoDataFrame is empty.\")\n\n    if cell_size &lt;= 0:\n        raise InvalidParameterValueException(\"Expected cell size to be positive and non-zero.\")\n\n    add_buffer = [False if x == 0 else x for x in add_buffer]\n    if any(num &lt; 0 for num in add_buffer):\n        raise InvalidParameterValueException(\"Expected buffer value to be positive, null or False.\")\n\n    for i, name in enumerate(column):\n        if column[i] == \"\":\n            if subset_target_attribute_values[i] is not None:\n                raise InvalidParameterValueException(\"Can't use subset of values if no column is targeted.\")\n        elif column[i] not in geodata[i]:\n            raise InvalidColumnException(\"Targeted column not found in the GeoDataFrame.\")\n\n    for i, subset in enumerate(subset_target_attribute_values):\n        if subset is not None:\n            for value in subset:\n                if value not in geodata[i][column[i]].unique():\n                    raise InvalidParameterValueException(\"Subset of value(s) not found in the targeted column.\")\n\n    # Computation\n    for i, data in enumerate(geodata):\n        if i == 0:\n            # Initialization of the CBA matrix\n            grid, cba = _init_from_vector_data(cell_size, geodata[0], column[0], subset_target_attribute_values[0])\n        else:\n            # If necessary, adding data to matrix\n            cba = _add_layer(\n                cba,\n                grid,\n                geodata[i],\n                column[i],\n                subset_target_attribute_values[i],\n                add_name[i - 1],\n                add_buffer[i - 1],\n            )\n\n    # Export\n    _to_raster(cba, output_path)\n\n    return cba\n</code></pre>"},{"location":"vector_processing/distance_computation/","title":"Distance computation","text":""},{"location":"vector_processing/distance_computation/#eis_toolkit.vector_processing.distance_computation.distance_computation","title":"<code>distance_computation(raster_profile, geometries)</code>","text":"<p>Calculate distance from raster cell to nearest geometry.</p> <p>Parameters:</p> Name Type Description Default <code>raster_profile</code> <code>Union[Profile, dict]</code> <p>The raster profile of the raster in which the distances to the nearest geometry are determined.</p> required <code>geometries</code> <code>GeoDataFrame</code> <p>The geometries to determine distance to.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>A 2D numpy array with the distances computed.</p> Source code in <code>eis_toolkit/vector_processing/distance_computation.py</code> <pre><code>@beartype\ndef distance_computation(raster_profile: Union[profiles.Profile, dict], geometries: gpd.GeoDataFrame) -&gt; np.ndarray:\n    \"\"\"Calculate distance from raster cell to nearest geometry.\n\n    Args:\n        raster_profile: The raster profile of the raster in which the distances\n            to the nearest geometry are determined.\n        geometries: The geometries to determine distance to.\n\n    Returns:\n        A 2D numpy array with the distances computed.\n\n    \"\"\"\n    if raster_profile.get(\"crs\") != geometries.crs:\n        raise NonMatchingCrsException(\"Expected coordinate systems to match between raster and geometries. \")\n    if geometries.shape[0] == 0:\n        raise EmptyDataFrameException(\"Expected GeoDataFrame to not be empty.\")\n\n    raster_width = raster_profile.get(\"width\")\n    raster_height = raster_profile.get(\"height\")\n\n    if not isinstance(raster_width, int) or not isinstance(raster_height, int):\n        raise InvalidParameterValueException(\n            f\"Expected raster_profile to contain integer width and height. {raster_profile}\"\n        )\n\n    raster_transform = raster_profile.get(\"transform\")\n\n    if not isinstance(raster_transform, transform.Affine):\n        raise InvalidParameterValueException(\n            f\"Expected raster_profile to contain an affine transformation. {raster_profile}\"\n        )\n\n    return _distance_computation(\n        raster_width=raster_width, raster_height=raster_height, raster_transform=raster_transform, geometries=geometries\n    )\n</code></pre>"},{"location":"vector_processing/extract_shared_lines/","title":"Extract shared lines","text":""},{"location":"vector_processing/extract_shared_lines/#eis_toolkit.vector_processing.extract_shared_lines.extract_shared_lines","title":"<code>extract_shared_lines(polygons)</code>","text":"<p>Extract shared lines/borders/edges between polygons.</p> <p>Parameters:</p> Name Type Description Default <code>polygons</code> <code>GeoDataFrame</code> <p>The geodataframe that contains the polygon geometries to be examined for shared lines.</p> required <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>Geodataframe containing the shared lines that were found between the polygons.</p> Source code in <code>eis_toolkit/vector_processing/extract_shared_lines.py</code> <pre><code>@beartype\ndef extract_shared_lines(polygons: gpd.GeoDataFrame) -&gt; gpd.GeoDataFrame:\n    \"\"\"Extract shared lines/borders/edges between polygons.\n\n    Args:\n        polygons: The geodataframe that contains the polygon geometries to be examined\n            for shared lines.\n\n    Returns:\n        Geodataframe containing the shared lines that were found between the polygons.\n\n    Raises:\n        EmptyDataFrameException if input geodataframe is empty.\n        InvalidParameterValueException if input geodataframe doesn't contain at least 2 polygons.\n    \"\"\"\n    if polygons.shape[0] == 0:\n        raise EmptyDataFrameException(\"Geodataframe is empty.\")\n\n    if polygons.shape[0] &lt; 2:\n        raise InvalidParameterValueException(\"Expected GeoDataFrame to have at least 2 polygons.\")\n\n    shared_lines = _extract_shared_lines(polygons)\n\n    return shared_lines\n</code></pre>"},{"location":"vector_processing/idw_interpolation/","title":"IDW","text":""},{"location":"vector_processing/idw_interpolation/#eis_toolkit.vector_processing.idw_interpolation.idw","title":"<code>idw(geodataframe, target_column, resolution, extent=None, power=2)</code>","text":"<p>Calculate inverse distance weighted (IDW) interpolation.</p> <p>Parameters:</p> Name Type Description Default <code>geodataframe</code> <code>GeoDataFrame</code> <p>The vector dataframe to be interpolated.</p> required <code>target_column</code> <code>str</code> <p>The column name with values for each geometry.</p> required <code>resolution</code> <code>Tuple[Number, Number]</code> <p>The resolution i.e. cell size of the output raster as (pixel_size_x, pixel_size_y).</p> required <code>extent</code> <code>Optional[Tuple[Number, Number, Number, Number]]</code> <p>The extent of the output raster as (x_min, x_max, y_min, y_max). If None, calculate extent from the input vector data.</p> <code>None</code> <code>power</code> <code>Number</code> <p>The value for determining the rate at which the weights decrease. As power increases, the weights for distant points decrease rapidly. Defaults to 2.</p> <code>2</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, dict]</code> <p>Rasterized vector data and metadata.</p> <p>Raises:</p> Type Description <code>EmptyDataFrameException</code> <p>The input GeoDataFrame is empty.</p> <code>InvalidParameterValueException</code> <p>Invalid resolution or target_column.</p> Source code in <code>eis_toolkit/vector_processing/idw_interpolation.py</code> <pre><code>@beartype\ndef idw(\n    geodataframe: gpd.GeoDataFrame,\n    target_column: str,\n    resolution: Tuple[Number, Number],\n    extent: Optional[Tuple[Number, Number, Number, Number]] = None,\n    power: Number = 2,\n) -&gt; Tuple[np.ndarray, dict]:\n    \"\"\"Calculate inverse distance weighted (IDW) interpolation.\n\n    Args:\n        geodataframe: The vector dataframe to be interpolated.\n        target_column: The column name with values for each geometry.\n        resolution: The resolution i.e. cell size of the output raster as (pixel_size_x, pixel_size_y).\n        extent: The extent of the output raster as (x_min, x_max, y_min, y_max).\n            If None, calculate extent from the input vector data.\n        power: The value for determining the rate at which the weights decrease.\n            As power increases, the weights for distant points decrease rapidly.\n            Defaults to 2.\n\n    Returns:\n        Rasterized vector data and metadata.\n\n    Raises:\n        EmptyDataFrameException: The input GeoDataFrame is empty.\n        InvalidParameterValueException: Invalid resolution or target_column.\n    \"\"\"\n\n    if geodataframe.shape[0] == 0:\n        raise EmptyDataFrameException(\"Expected geodataframe to contain geometries.\")\n\n    if target_column not in geodataframe.columns:\n        raise InvalidParameterValueException(\n            f\"Expected target_column ({target_column}) to be contained in geodataframe columns.\"\n        )\n\n    if resolution[0] &lt;= 0 or resolution[1] &lt;= 0:\n        raise InvalidParameterValueException(\"Expected height and width greater than zero.\")\n\n    interpolated_values, out_meta = _idw_interpolation(geodataframe, target_column, resolution, power, extent)\n\n    return interpolated_values, out_meta\n</code></pre>"},{"location":"vector_processing/kriging_interpolation/","title":"Kriging interpolation","text":""},{"location":"vector_processing/kriging_interpolation/#eis_toolkit.vector_processing.kriging_interpolation.kriging","title":"<code>kriging(data, target_column, resolution, extent=None, variogram_model='linear', coordinates_type='geographic', method='ordinary')</code>","text":"<p>Perform Kriging interpolation on the input data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>GeoDataFrame</code> <p>GeoDataFrame containing the input data.</p> required <code>target_column</code> <code>str</code> <p>The column name with values for each geometry.</p> required <code>resolution</code> <code>Tuple[Number, Number]</code> <p>The resolution i.e. cell size of the output raster as (pixel_size_x, pixel_size_y).</p> required <code>extent</code> <code>Optional[Tuple[Number, Number, Number, Number]]</code> <p>The extent of the output raster as (x_min, x_max, y_min, y_max). If None, calculate extent from the input vector data.</p> <code>None</code> <code>variogram_model</code> <code>Literal[linear, power, gaussian, spherical, exponential]</code> <p>Variogram model to be used. Either 'linear', 'power', 'gaussian', 'spherical' or 'exponential'. Defaults to 'linear'.</p> <code>'linear'</code> <code>coordinates_type</code> <code>Literal[euclidean, geographic]</code> <p>Determines are coordinates on a plane ('euclidean') or a sphere ('geographic'). Used only in ordinary kriging. Defaults to 'geographic'.</p> <code>'geographic'</code> <code>method</code> <code>Literal[ordinary, universal]</code> <p>Ordinary or universal kriging. Defaults to 'ordinary'.</p> <code>'ordinary'</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, dict]</code> <p>Grid containing the interpolated values and metadata.</p> <p>Raises:</p> Type Description <code>EmptyDataFrameException</code> <p>The input GeoDataFrame is empty.</p> <code>InvalidParameterValueException</code> <p>Target column name is invalid or resolution is not greater than zero.</p> Source code in <code>eis_toolkit/vector_processing/kriging_interpolation.py</code> <pre><code>@beartype\ndef kriging(\n    data: gpd.GeoDataFrame,\n    target_column: str,\n    resolution: Tuple[Number, Number],\n    extent: Optional[Tuple[Number, Number, Number, Number]] = None,\n    variogram_model: Literal[\"linear\", \"power\", \"gaussian\", \"spherical\", \"exponential\"] = \"linear\",\n    coordinates_type: Literal[\"euclidean\", \"geographic\"] = \"geographic\",\n    method: Literal[\"ordinary\", \"universal\"] = \"ordinary\",\n) -&gt; Tuple[np.ndarray, dict]:\n    \"\"\"\n    Perform Kriging interpolation on the input data.\n\n    Args:\n        data: GeoDataFrame containing the input data.\n        target_column: The column name with values for each geometry.\n        resolution: The resolution i.e. cell size of the output raster as (pixel_size_x, pixel_size_y).\n        extent: The extent of the output raster as (x_min, x_max, y_min, y_max).\n            If None, calculate extent from the input vector data.\n        variogram_model: Variogram model to be used.\n            Either 'linear', 'power', 'gaussian', 'spherical' or 'exponential'. Defaults to 'linear'.\n        coordinates_type: Determines are coordinates on a plane ('euclidean') or a sphere ('geographic').\n            Used only in ordinary kriging. Defaults to 'geographic'.\n        method: Ordinary or universal kriging. Defaults to 'ordinary'.\n\n    Returns:\n        Grid containing the interpolated values and metadata.\n\n    Raises:\n        EmptyDataFrameException: The input GeoDataFrame is empty.\n        InvalidParameterValueException: Target column name is invalid or resolution is not greater than zero.\n    \"\"\"\n\n    if data.empty:\n        raise EmptyDataFrameException(\"The input GeoDataFrame is empty.\")\n\n    if target_column not in data.columns:\n        raise InvalidParameterValueException(\n            f\"Expected target_column ({target_column}) to be contained in geodataframe columns.\"\n        )\n\n    if resolution[0] &lt;= 0 or resolution[1] &lt;= 0:\n        raise InvalidParameterValueException(\"The resolution must be greater than zero.\")\n\n    data_interpolated, out_meta = _kriging(\n        data, target_column, resolution, extent, variogram_model, coordinates_type, method\n    )\n\n    return data_interpolated, out_meta\n</code></pre>"},{"location":"vector_processing/rasterize_vector/","title":"Rasterize vector","text":""},{"location":"vector_processing/rasterize_vector/#eis_toolkit.vector_processing.rasterize_vector.rasterize_vector","title":"<code>rasterize_vector(geodataframe, resolution=None, value_column=None, default_value=1.0, fill_value=0.0, base_raster_profile=None, buffer_value=None, merge_strategy='replace')</code>","text":"<p>Transform vector data into raster data.</p> <p>Parameters:</p> Name Type Description Default <code>geodataframe</code> <code>GeoDataFrame</code> <p>The vector dataframe to be rasterized.</p> required <code>resolution</code> <code>Optional[float]</code> <p>The resolution i.e. cell size of the output raster. Optional if base_raster_profile is given.</p> <code>None</code> <code>value_column</code> <code>Optional[str]</code> <p>The column name with values for each geometry. If None, then default_value is used for all geometries.</p> <code>None</code> <code>default_value</code> <code>float</code> <p>Default value burned into raster cells based on geometries.</p> <code>1.0</code> <code>base_raster_profile</code> <code>Optional[Union[Profile, dict]]</code> <p>Base raster profile to be used for determining the grid on which vectors are burned in. If None, the geometries and provided resolution value are used to compute grid.</p> <code>None</code> <code>fill_value</code> <code>float</code> <p>Value used outside the burned/rasterized geometry cells.</p> <code>0.0</code> <code>buffer_value</code> <code>Optional[float]</code> <p>For adding a buffer around passed geometries before rasterization.</p> <code>None</code> <code>merge_strategy</code> <code>Literal[replace, add]</code> <p>How to handle overlapping geometries. \"add\" causes overlapping geometries to add together the values while \"replace\" does not. Adding them together is the basis for density computations where the density can be calculated by using a default value of 1.0 and the sum in each cell is the count of intersecting geometries.</p> <code>'replace'</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, dict]</code> <p>Rasterized vector data and metadata.</p> <p>Raises:</p> Type Description <code>EmptyDataFrameException</code> <p>The geodataframe does not contain geometries.</p> <code>InvalidColumnException</code> <p>Given value_column is not in the input geodataframe.</p> <code>InvalidParameterValueException</code> <p>No resolution or base_raster_profile is given, or base_raster_profile has the wrong type.</p> <code>NumericValueSignException</code> <p>Input resolution value is zero or negative, or input buffer_value is negative.</p> Source code in <code>eis_toolkit/vector_processing/rasterize_vector.py</code> <pre><code>@beartype\ndef rasterize_vector(\n    geodataframe: gpd.GeoDataFrame,\n    resolution: Optional[float] = None,\n    value_column: Optional[str] = None,\n    default_value: float = 1.0,\n    fill_value: float = 0.0,\n    base_raster_profile: Optional[Union[profiles.Profile, dict]] = None,\n    buffer_value: Optional[float] = None,\n    merge_strategy: Literal[\"replace\", \"add\"] = \"replace\",\n) -&gt; Tuple[np.ndarray, dict]:\n    \"\"\"Transform vector data into raster data.\n\n    Args:\n        geodataframe: The vector dataframe to be rasterized.\n        resolution: The resolution i.e. cell size of the output raster.\n            Optional if base_raster_profile is given.\n        value_column: The column name with values for each geometry.\n            If None, then default_value is used for all geometries.\n        default_value: Default value burned into raster cells based on geometries.\n        base_raster_profile: Base raster profile\n            to be used for determining the grid on which vectors are\n            burned in. If None, the geometries and provided resolution\n            value are used to compute grid.\n        fill_value: Value used outside the burned/rasterized geometry cells.\n        buffer_value: For adding a buffer around passed\n            geometries before rasterization.\n        merge_strategy: How to handle overlapping geometries.\n            \"add\" causes overlapping geometries to add together the\n            values while \"replace\" does not. Adding them together is the\n            basis for density computations where the density can be\n            calculated by using a default value of 1.0 and the sum in\n            each cell is the count of intersecting geometries.\n\n    Returns:\n        Rasterized vector data and metadata.\n\n    Raises:\n        EmptyDataFrameException: The geodataframe does not contain geometries.\n        InvalidColumnException: Given value_column is not in the input geodataframe.\n        InvalidParameterValueException: No resolution or base_raster_profile is given,\n            or base_raster_profile has the wrong type.\n        NumericValueSignException: Input resolution value is zero or negative, or input\n            buffer_value is negative.\n    \"\"\"\n\n    if geodataframe.shape[0] == 0:\n        raise EmptyDataFrameException(\"Expected geodataframe to contain geometries.\")\n\n    if resolution is None and base_raster_profile is None:\n        raise InvalidParameterValueException(\"Expected either resolution or base_raster_profile to be given.\")\n\n    if resolution is not None and resolution &lt;= 0:\n        raise NumericValueSignException(f\"Expected a positive resolution value ({dict(resolution=resolution)})\")\n\n    if value_column is not None and value_column not in geodataframe.columns:\n        raise InvalidColumnException(f\"Expected value_column ({value_column}) to be contained in geodataframe columns.\")\n\n    if buffer_value is not None and buffer_value &lt; 0:\n        raise NumericValueSignException(f\"Expected a positive buffer_value ({dict(buffer_value=buffer_value)})\")\n\n    if base_raster_profile is not None and not isinstance(base_raster_profile, (profiles.Profile, dict)):\n        raise InvalidParameterValueException(\n            f\"Expected base_raster_profile ({type(base_raster_profile)}) to be dict or rasterio.profiles.Profile.\"\n        )\n\n    if buffer_value is not None:\n        geodataframe = geodataframe.copy()\n        geodataframe[\"geometry\"] = geodataframe[\"geometry\"].apply(lambda geom: geom.buffer(buffer_value))\n\n    return _rasterize_vector(\n        geodataframe=geodataframe,\n        value_column=value_column,\n        default_value=default_value,\n        fill_value=fill_value,\n        base_raster_profile=base_raster_profile,\n        resolution=resolution,\n        merge_alg=getattr(MergeAlg, merge_strategy),\n    )\n</code></pre>"},{"location":"vector_processing/reproject_vector/","title":"Reproject vector","text":""},{"location":"vector_processing/reproject_vector/#eis_toolkit.vector_processing.reproject_vector.reproject_vector","title":"<code>reproject_vector(geodataframe, target_crs)</code>","text":"<p>Reprojects vector data to match given coordinate reference system (EPSG).</p> <p>Parameters:</p> Name Type Description Default <code>geodataframe</code> <code>GeoDataFrame</code> <p>The vector dataframe to be reprojected.</p> required <code>target_crs</code> <code>int</code> <p>Target CRS as an EPSG code.</p> required <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>Reprojected vector data.</p> Source code in <code>eis_toolkit/vector_processing/reproject_vector.py</code> <pre><code>@beartype\ndef reproject_vector(geodataframe: geopandas.GeoDataFrame, target_crs: int) -&gt; geopandas.GeoDataFrame:\n    \"\"\"Reprojects vector data to match given coordinate reference system (EPSG).\n\n    Args:\n        geodataframe: The vector dataframe to be reprojected.\n        target_crs: Target CRS as an EPSG code.\n\n    Returns:\n        Reprojected vector data.\n    \"\"\"\n\n    if geodataframe.crs.to_epsg() == target_crs:\n        raise MatchingCrsException(\"Vector data is already in the target CRS.\")\n\n    reprojected_gdf = geodataframe.to_crs(\"epsg:\" + str(target_crs))\n    return reprojected_gdf\n</code></pre>"},{"location":"vector_processing/vector_density/","title":"Vector density","text":""},{"location":"vector_processing/vector_density/#eis_toolkit.vector_processing.vector_density.vector_density","title":"<code>vector_density(geodataframe, resolution=None, base_raster_profile=None, buffer_value=None, statistic='density')</code>","text":"<p>Compute density of geometries within raster.</p> <p>Parameters:</p> Name Type Description Default <code>geodataframe</code> <code>GeoDataFrame</code> <p>The dataframe with vectors of which density is computed.</p> required <code>resolution</code> <code>Optional[float]</code> <p>The resolution i.e. cell size of the output raster. Optional if base_raster_profile is given.</p> <code>None</code> <code>base_raster_profile</code> <code>Optional[Union[Profile, dict]]</code> <p>Base raster profile to be used for determining the grid on which vectors are burned in. If None, the geometries and provided resolution value are used to compute grid.</p> <code>None</code> <code>buffer_value</code> <code>Optional[float]</code> <p>For adding a buffer around passed geometries before computing density.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, dict]</code> <p>Computed density of vector data and metadata.</p> Source code in <code>eis_toolkit/vector_processing/vector_density.py</code> <pre><code>@beartype\ndef vector_density(\n    geodataframe: gpd.GeoDataFrame,\n    resolution: Optional[float] = None,\n    base_raster_profile: Optional[Union[profiles.Profile, dict]] = None,\n    buffer_value: Optional[float] = None,\n    statistic: Literal[\"density\", \"count\"] = \"density\",\n) -&gt; Tuple[np.ndarray, dict]:\n    \"\"\"Compute density of geometries within raster.\n\n    Args:\n        geodataframe: The dataframe with vectors\n            of which density is computed.\n        resolution: The resolution i.e. cell size of the output raster.\n            Optional if base_raster_profile is given.\n        base_raster_profile: Base raster profile\n            to be used for determining the grid on which vectors are\n            burned in. If None, the geometries and provided resolution\n            value are used to compute grid.\n        buffer_value: For adding a buffer around passed\n            geometries before computing density.\n\n    Returns:\n        Computed density of vector data and metadata.\n    \"\"\"\n    out_raster_array, out_metadata = rasterize_vector(\n        geodataframe=geodataframe,\n        resolution=resolution,\n        base_raster_profile=base_raster_profile,\n        buffer_value=buffer_value,\n        value_column=None,\n        default_value=1.0,\n        fill_value=0.0,\n        merge_strategy=\"add\",\n    )\n    max_count = np.max(out_raster_array)\n    if statistic == \"count\" or np.isclose(max_count, 0.0):\n        return out_raster_array, out_metadata\n    else:\n        return (out_raster_array / max_count), out_metadata\n</code></pre>"}]}