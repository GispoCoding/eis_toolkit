### This document is taken from the official EIS TOOLKIT Docs

In traditional deep neural networks (DNN), the trained weights of a DNN are a point estimate for each parameter, thereby 
producing deterministic network outputs for a given input. On the other hand, Bayesian neural networks (BNN) establish 
a probabilistic distribution over the weight parameters. The process of estimating the posterior weight distribution 
enables Bayesian neural networks (BNNs) to capture and quantify uncertainty in their predictions effectively 
(Olivier et al.,2021). Measuring uncertainty can help identify when a test input's predictions are noisy because 
it deviates from the training distribution or because the model cannot account for unknown causes. The point 
predictions generated by Deep Neural Networks (DNNs) do not provide information regarding the inherent features 
of the input.

The total uncertainty in the prediction, referred to as the predictive uncertainty, is comprised of both epistemic 
and aleatoric uncertainty (Joachims, 2021). Epistemic uncertainty refers to uncertainty within the model parameters. 
It is reducible uncertainty, which means that it can be reduced by collecting more data or improving the model. This 
can be viewed as the posterior weight distribution, in which a peaked posterior distribution reflects lower epistemic 
uncertainty, while a broader posterior distribution shows higher epistemic uncertainty. For the given input point and 
fixed weight parameters, higher aleatoric uncertainty denotes a noisy and uncertain estimate of the model's output. 
In these cases, the model shows a lack of confidence in its forecasts, which is indicative of significant uncertainty 
or variation in the evaluated results.

The implementation of a Bayesian Neural Network (BNN) involves the utilization of probabilistic (e.g., DenseVariational) 
layers to define the posterior and prior distributions over the weights of the model, hence enabling the estimation of 
uncertainty (Chang et al., 2021). This methodology enables the neural network to effectively include the inherent 
uncertainty present in both the data and the model, resulting in the generation of a probability distribution 
including a range of potential outputs, as opposed to a singular deterministic prediction. By utilizing probabilistic 
layers, the neural network is capable of preserving and enhancing distribution over every weight. This approach 
allows for a more comprehensive representation that encompasses the uncertainty and variability in both the model 
parameters and predictions. Consequently, it leads to the development of a more resilient and informative model, 
particularly in situations where there is uncertainty or limited data availability.

