{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "\n",
    "# TensorFlow and TensorFlow Probability shortcuts\n",
    "tfd = tfp.distributions\n",
    "tfpl = tfp.layers\n",
    "tfk = tf.keras\n",
    "tfkl = tf.keras.layers\n",
    "\n",
    "# Functions to define prior and posterior for Bayesian layers\n",
    "def posterior_mean_field(kernel_size, bias_size=0, dtype=None):\n",
    "    n = kernel_size + bias_size\n",
    "    c = np.log(np.expm1(1.))\n",
    "    return tf.keras.Sequential([\n",
    "        tfp.layers.VariableLayer(2 * n, dtype=dtype),\n",
    "        tfp.layers.DistributionLambda(lambda t: tfd.Independent(\n",
    "            tfd.Normal(loc=t[..., :n],\n",
    "                       scale=1e-5 + tf.nn.softplus(c + t[..., n:])),\n",
    "            reinterpreted_batch_ndims=1)),\n",
    "    ])\n",
    "\n",
    "def prior_trainable(kernel_size, bias_size=0, dtype=None):\n",
    "    n = kernel_size + bias_size\n",
    "    return tf.keras.Sequential([\n",
    "        tfp.layers.VariableLayer(n, dtype=dtype),\n",
    "        tfp.layers.DistributionLambda(lambda t: tfd.Independent(\n",
    "            tfd.Normal(loc=t, scale=1),\n",
    "            reinterpreted_batch_ndims=1)),\n",
    "    ])\n",
    "\n",
    "# Example for a single input model. Adjust as needed.\n",
    "#def create_model_inputs():\n",
    "    #return {'input': tf.keras.Input(shape=(X_scaled.shape[1],), name='input')}\n",
    "\n",
    "# List of integers indicating the size of each hidden layer\n",
    "hidden_units = [16,8]\n",
    "learning_rate = 0.001\n",
    "\n",
    "def create_probabilistic_bnn_model(train_size):\n",
    "    inputs = create_model_inputs()\n",
    "    features = tfkl.Concatenate(axis=-1)(list(inputs.values()))\n",
    "    features = tfkl.BatchNormalization()(features)\n",
    "\n",
    "    # Replace `prior` and `posterior` with `prior_trainable` and `posterior_mean_field`\n",
    "    for units in hidden_units:\n",
    "        features = tfpl.DenseVariational(\n",
    "            units=units,\n",
    "            make_prior_fn=prior_trainable,\n",
    "            make_posterior_fn=posterior_mean_field,\n",
    "            kl_weight=1/train_size,\n",
    "            activation='softmax')(features)\n",
    "\n",
    "    # Create a probabilistic output (Normal distribution),\n",
    "    # and use the `Dense` layer to produce the parameters of the distribution.\n",
    "    distribution_params = tfkl.Dense(units=2)(features)\n",
    "    outputs = tfpl.IndependentNormal(1)(distribution_params)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_the_data(path):\n",
    "   \n",
    "    data = pd.read_csv(path)\n",
    "    \n",
    "    # Extract the label\n",
    "    label = data['class']\n",
    "    \n",
    "    # Drop specified columns and standardize the data\n",
    "    data = data.drop(['E', 'N', 'class'], axis=1)\n",
    "    data_columns= data.columns\n",
    "    data = StandardScaler().fit_transform(data)\n",
    "    \n",
    "    # Convert label to NumPy array (if not already)\n",
    "    label = label.to_numpy()\n",
    "    \n",
    "    return data, label, data_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the data\n",
    "data_class_1, label_class_1, data_columns_1 = load_the_data(path=f\"/home/dipak/Desktop/codes/MLP_all_codes/MLP_Jukka_07_03_23/17_annoted_points.csv\")\n",
    "data_class_0, label_class_0, data_columns_0 = load_the_data(path=f\"/home/dipak/Desktop/codes/MLP_all_codes/MLP_Jukka_07_03_23/2M_raster_points.csv\")\n",
    "\n",
    "print(f'[CLASS 1] {data_class_1.shape}')\n",
    "print(f'[CLASS 0] {data_class_0.shape}')\n",
    "\n",
    "print(len(data_class_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = 34\n",
    "print(num_rows)\n",
    "FEATURE_NAMES = data_columns_1\n",
    "print(FEATURE_NAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_inputs():\n",
    "    inputs = {}\n",
    "    for feature_name in FEATURE_NAMES:\n",
    "        inputs[feature_name] = layers.Input(\n",
    "            name=feature_name, shape=(1,), dtype=tf.float64\n",
    "        )\n",
    "    return inputs\n",
    "\n",
    "inputs = create_model_inputs()\n",
    "\n",
    "dataset_size = num_rows\n",
    "batch_size = 2\n",
    "train_size = int(dataset_size)\n",
    "\n",
    "\n",
    "def dataframe_to_dataset(data_class, label_class, batch_size):\n",
    "    dataframe = data_class.copy()\n",
    "    labels = label_class\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "    ds = ds.batch(batch_size)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_probability as tfp\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_epochs = 1000\n",
    "\n",
    "def run_experiment(model, loss, train_dataset, test_dataset):\n",
    "\n",
    "     model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.01),\n",
    "                   loss=loss,\n",
    "                   metrics=[keras.metrics.RootMeanSquaredError()]\n",
    "                   )\n",
    "    \n",
    "     print(\"Start training the model...\")\n",
    "    # Train model\n",
    "     model.fit(train_dataset, epochs= num_epochs, validation_data=test_dataset, verbose=1)\n",
    "\n",
    "def negative_loglikelihood(targets, estimated_distribution):\n",
    "    return -estimated_distribution.log_prob(targets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def generate_and_save_predictions(prob_bnn_model, test_dataset, round_number):\n",
    "    \"\"\"\n",
    "    Generate predictions using the Bayesian Neural Network model and save them as a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - prob_bnn_model: The probabilistic BNN model.\n",
    "    - test_dataset: The test dataset.\n",
    "    - round_number: The current round number, used for naming the output file.\n",
    "\n",
    "    Returns:\n",
    "    - targets: The actual target values.\n",
    "    - prediction_mean: The predicted mean values.\n",
    "    - prediction_stdv: The standard deviation of the predicted values.\n",
    "    \"\"\"\n",
    "    sample = test_dataset.cardinality().numpy() * batch_size  # Replace with actual batch size if this doesn't work\n",
    "    examples, targets = list(test_dataset.unbatch().shuffle(batch_size * 10).batch(sample))[0]\n",
    "    prediction_distribution = prob_bnn_model(examples)\n",
    "    \n",
    "    prediction_mean = prediction_distribution.mean().numpy().tolist()\n",
    "    prediction_stdv = prediction_distribution.stddev().numpy()\n",
    "    \n",
    "    # The 95% CI is computed as mean Â± (1.96 * stdv)\n",
    "    upper = (prediction_mean + (1.96 * prediction_stdv)).tolist()\n",
    "    lower = (prediction_mean - (1.96 * prediction_stdv)).tolist()\n",
    "    prediction_stdv = prediction_stdv.tolist()\n",
    "    \n",
    "    results = []\n",
    "    for idx in range(sample):\n",
    "        results.append({\n",
    "            \"Prediction mean\": round(prediction_mean[idx][0], 2),\n",
    "            \"stddev\": round(prediction_stdv[idx][0], 2),\n",
    "            \"95% CI lower\": round(lower[idx][0], 2),\n",
    "            \"95% CI upper\": round(upper[idx][0], 2),\n",
    "            \"Actual\": targets[idx].numpy()  # Adjust if the tensor is not an eager tensor\n",
    "        })\n",
    "        \n",
    "        print(\n",
    "            f\"Prediction mean: {round(prediction_mean[idx][0], 2)}, \"\n",
    "            f\"stddev: {round(prediction_stdv[idx][0], 2)}, \"\n",
    "            f\"95% CI: [{round(upper[idx][0], 2)} - {round(lower[idx][0], 2)}]\"\n",
    "            f\" - Actual: {targets[idx]}\"\n",
    "        )\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(f'predictions_round_{round_number}.csv', index=False)\n",
    "    \n",
    "    return targets, prediction_mean, prediction_stdv  # Added return statement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_and_save_CI(targets, prediction_mean, prediction_stdv, round_number):\n",
    "    \"\"\"\n",
    "    Plot the confidence interval and save it as an image file.\n",
    "\n",
    "    Parameters:\n",
    "    - targets: Actual target values.\n",
    "    - prediction_mean: Predicted mean values.\n",
    "    - prediction_stdv: Standard deviation of the predicted values.\n",
    "    - round_number: The current round number, used for naming the output file.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Assuming prediction_mean and prediction_stdv are nested lists like [[a], [b], [c], ...]\n",
    "    flat_mean = [x[0] for x in prediction_mean]\n",
    "    flat_std = [x[0] for x in prediction_stdv]\n",
    "\n",
    "    # Create a new figure and set the size of the figure\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Plot actual vs. predicted values\n",
    "    plt.scatter(range(len(targets)), targets, label='Actual', marker='o', color='red')\n",
    "    plt.plot(flat_mean, label='Predicted', marker='x')\n",
    "\n",
    "    # Fill between for 95% CI\n",
    "    plt.fill_between(\n",
    "        range(len(flat_mean)),\n",
    "        [pred - 1.96*std for pred, std in zip(flat_mean, flat_std)],\n",
    "        [pred + 1.96*std for pred, std in zip(flat_mean, flat_std)],\n",
    "        alpha=0.2,\n",
    "        label='95% CI'\n",
    "    )\n",
    "\n",
    "    # Labeling the axes and providing a title\n",
    "    plt.xlabel('Example')\n",
    "    plt.ylabel('Value')\n",
    "    plt.title('Predicted vs Actual Values with 95% Confidence Interval')\n",
    "    plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "    # Save the plot as a .png file\n",
    "    plt.savefig(f'confidence_interval_plot_round_{round_number}.png', bbox_inches='tight')\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_to_dataset1(data, labels, batch_size, data_columns_1):\n",
    "    \"\"\"\n",
    "    Convert the data and labels into a tf.data.Dataset.\n",
    "    \n",
    "    Parameters:\n",
    "        - data: DataFrame or array-like object of shape (n_samples, n_features)\n",
    "        - labels: Array-like object of shape (n_samples,)\n",
    "        - batch_size: Integer, size of batches to divide data into.\n",
    "    \n",
    "    Returns:\n",
    "        - ds: tf.data.Dataset object\n",
    "    \"\"\"\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        data_dict = {feature: data[feature].values for feature in data.columns}\n",
    "    else:  # assuming it's an array-like object\n",
    "        # Ensure you have a list of feature names when data is not a DataFrame\n",
    "        feature_names = data_columns_1  # Replace with actual feature names\n",
    "        data_dict = {feature: data[:, idx] for idx, feature in enumerate(feature_names)}\n",
    "\n",
    "    ds = tf.data.Dataset.from_tensor_slices((data_dict, labels))\n",
    "    ds = ds.batch(batch_size)\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def scatter_plot_predictions_and_save(targets, prediction_mean, prediction_stdv, round_number):\n",
    "    \"\"\"\n",
    "    Generates a scatter plot of actual vs. predicted values with confidence as color,\n",
    "    and saves the plot as an image file.\n",
    "\n",
    "    Parameters:\n",
    "    - targets: Actual target values.\n",
    "    - prediction_mean: Predicted mean values from the model.\n",
    "    - prediction_stdv: Predicted standard deviation values from the model.\n",
    "    - round_number: The current round number, used for naming the output image file.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Assuming that prediction_mean and prediction_stdv are nested lists like [[a], [b], [c], ...]\n",
    "    flat_mean = [x[0] for x in prediction_mean]\n",
    "    flat_std = [x[0] for x in prediction_stdv]\n",
    "\n",
    "    # Creating a scatter plot\n",
    "    plt.scatter(x=targets, y=flat_mean, c=flat_std, cmap='viridis', s=50, alpha=0.6)\n",
    "    plt.plot([min(targets), max(targets)], [min(targets), max(targets)], 'k--')\n",
    "    plt.title('Predicted vs. Actual with Confidence as Color')\n",
    "    plt.xlabel('Actual Value')\n",
    "    plt.ylabel('Predicted Value')\n",
    "    \n",
    "    # Saving the plot as an image file\n",
    "    plt.savefig(f'predicted_vs_actual_round_{round_number}.png')\n",
    "    \n",
    "    # Displaying the plot\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# plot_predictions_and_save(targets, prediction_mean, prediction_stdv, round_number)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_and_save_confusion_matrix(prediction_mean, targets, round_number):\n",
    "    \"\"\"\n",
    "    Plot the confusion matrix using predictions and actual targets and save the plot.\n",
    "\n",
    "    Parameters:\n",
    "    - prediction_mean: Mean of the predictions.\n",
    "    - targets: Actual target values.\n",
    "    - round_number: Current round number, used for naming the output file.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Discretizing the predictions\n",
    "    prediction_discrete = np.round(prediction_mean)\n",
    "\n",
    "    # Ensure your targets are also in a comparable shape/format\n",
    "    true_labels = [label.numpy() for label in targets]  # Adapt as per your use case\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(true_labels, prediction_discrete)\n",
    "    \n",
    "    plt.figure(figsize=(10, 7))\n",
    "\n",
    "    # Use seaborn to make the heatmap\n",
    "    sns.heatmap(cm, annot=True, fmt='g', cmap='Blues')\n",
    "    \n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix')\n",
    "    \n",
    "    # Save the plot as a .png file\n",
    "    plt.savefig(f'confusion_matrix_round_{round_number}.png', bbox_inches='tight')\n",
    "    \n",
    "    # Display the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_ard(prob_bnn_model, df, round_number):\n",
    "    \"\"\"\n",
    "    Plot Automatic Relevance Determination (ARD) and save it according to the round number.\n",
    "\n",
    "    Parameters:\n",
    "    - prob_bnn_model: The probabilistic BNN model.\n",
    "    - df: DataFrame used for extracting feature names.\n",
    "    - round_number: Current round number, used for naming the output file.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Extract trainable variables\n",
    "    trainable_variables = prob_bnn_model.layers[-3].trainable_variables\n",
    "\n",
    "    # Calculate input_dim\n",
    "    input_dim = df.shape[1] - 1  # Assuming the last column is the target variable\n",
    "\n",
    "    # Extract means and raw scales from trainable variables\n",
    "    posterior_means = trainable_variables[0][:input_dim].numpy()\n",
    "    raw_scale = trainable_variables[0][input_dim:2 * input_dim].numpy()\n",
    "\n",
    "    # Convert raw scales to standard deviations\n",
    "    posterior_stddevs = tf.nn.softplus(raw_scale).numpy()\n",
    "\n",
    "    # Compute importance\n",
    "    importance = np.abs(posterior_means) * posterior_stddevs\n",
    "\n",
    "    # Sort indices of importance\n",
    "    sorted_idx = np.argsort(importance)[::-1]\n",
    "\n",
    "    # Display the sorted feature importances\n",
    "    print(\"Features sorted by their importance:\")\n",
    "    for idx in sorted_idx:\n",
    "        print(f\"{df.columns[idx]}: {importance[idx]}\")\n",
    "\n",
    "    # Prepare data for plotting\n",
    "    features = [df.columns[idx] for idx in sorted_idx]\n",
    "    importances = [importance[idx] for idx in sorted_idx]\n",
    "\n",
    "    # Create horizontal bar plot\n",
    "    plt.figure(figsize=(10,8))\n",
    "    bars = plt.barh(features, importances, color='skyblue')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title('Feature Importance')\n",
    "\n",
    "    # Add the data value at the end of each bar\n",
    "    for bar, value in zip(bars, importances):\n",
    "        plt.text(bar.get_width(), bar.get_y() + bar.get_height()/2, \n",
    "                 f'{value:.4f}', \n",
    "                 va='center', ha='left', color='blue', fontsize=10)\n",
    "\n",
    "    # Invert y-axis to have the most important feature at the top\n",
    "    plt.gca().invert_yaxis()\n",
    "    \n",
    "    # Save and show the plot\n",
    "    plt.savefig(f'ARD_plot_round_{round_number}.png', bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_OF_SAMPLE_TO_TAKE = len(data_class_1)\n",
    "\n",
    "# number of total combination\n",
    "number_of_total_combination = math.floor(data_class_0.shape[0] / NUMBER_OF_SAMPLE_TO_TAKE)\n",
    "print(number_of_total_combination)\n",
    "# I need this to safeget stuff from the memory \n",
    "list_of_already_taken_id = list()\n",
    "\n",
    "for round_number in range(number_of_total_combination + 1):\n",
    "    # get the random number\n",
    "    class_0_we_need, label_0_we_need = list(), list()\n",
    "\n",
    "    list_of_index = list()\n",
    "\n",
    "    if len(list_of_already_taken_id) + NUMBER_OF_SAMPLE_TO_TAKE > len(data_class_0):\n",
    "        print(f\"[NO MORE SAMPLE]\")\n",
    "\n",
    "        for ii in range(len(list_of_already_taken_id), len(data_class_0)):\n",
    "            list_of_index.append(ii)\n",
    "        print(f'{len(list_of_index)} --- {list_of_index}')\n",
    "    else:\n",
    "\n",
    "        while len(list_of_index) < NUMBER_OF_SAMPLE_TO_TAKE:\n",
    "            random_number = random.randint(0, len(data_class_0) - 1)\n",
    "\n",
    "            # if the number is not in the list add to the random numbers\n",
    "            if random_number not in list_of_already_taken_id:\n",
    "                list_of_already_taken_id.append(random_number)\n",
    "                #print(f\"[AVAILLABLE INDEX] {random_number}\")\n",
    "                list_of_index.append(random_number)\n",
    "            else:\n",
    "                #print(f\"[NOT AVAILLABLE]\")\n",
    "                continue\n",
    "\n",
    "    for index in list_of_index:\n",
    "        class_0_we_need.append(data_class_0[index, :])\n",
    "        label_0_we_need.append(label_class_0[index])\n",
    "\n",
    "    class_0_we_need = np.array(class_0_we_need)\n",
    "    label_0_we_need = np.array(label_0_we_need)\n",
    "    \n",
    "    data = np.concatenate((data_class_1, class_0_we_need), axis=0)\n",
    "    labels = np.concatenate((label_class_1, label_0_we_need), axis=0)\n",
    "\n",
    "    FEATURE_NAMES = data_columns_1  # Ensure this list has the names of your features\n",
    "    df = pd.DataFrame(data, columns=FEATURE_NAMES)  # Convert data to a DataFrame\n",
    "\n",
    "    # Add labels to the DataFrame if they are not included in FEATURE_NAMES\n",
    "    df['target'] = labels\n",
    "\n",
    "\n",
    "\n",
    "    data = tf.cast(data, dtype=tf.float32)\n",
    "    labels = tf.cast(labels, dtype=tf.float32)\n",
    "\n",
    "    print(f\"Data shape: {data.shape}, Labels shape: {labels.shape}\")\n",
    "    print(f\"Data type: {type(data)}, Labels type: {type(labels)}\")\n",
    "\n",
    "    \n",
    "    print(f\"Concatenenated data is {data.shape} concateaned label is {labels.shape}\")\n",
    "\n",
    "    train_dataset = dataframe_to_dataset1(data, labels, batch_size=batch_size, data_columns_1=FEATURE_NAMES)\n",
    "    test_dataset = dataframe_to_dataset1(data,labels, batch_size=batch_size, data_columns_1=FEATURE_NAMES)\n",
    "    \n",
    "\n",
    "    print(f\"Concatenenated data is {data.shape} concateaned label is {labels.shape}\")\n",
    "\n",
    "    prob_bnn_model = create_probabilistic_bnn_model(train_size)\n",
    "    run_experiment(prob_bnn_model, negative_loglikelihood, train_dataset, test_dataset)\n",
    "    # Generate predictions and save them as a CSV file\n",
    "    targets, prediction_mean, prediction_stdv = generate_and_save_predictions( prob_bnn_model, test_dataset, round_number)\n",
    "    plot_and_save_CI(targets, prediction_mean, prediction_stdv, round_number)\n",
    "    scatter_plot_predictions_and_save(targets, prediction_mean, prediction_stdv, round_number)\n",
    "    plot_and_save_confusion_matrix(prediction_mean, targets, round_number)\n",
    "    plot_ard(prob_bnn_model, df, round_number)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayesian",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
